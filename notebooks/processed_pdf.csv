text,topic,relevant_to_model fit linear likelihood value,similarity_to_model fit linear likelihood value,relevant_to_model plot data response residuals,similarity_to_model plot data response residuals,relevant_to_data effects model random analysis,similarity_to_data effects model random analysis,relevant_to_data model tree linear regression,similarity_to_data model tree linear regression,relevant_to_model data test deviance log,similarity_to_model data test deviance log
"Computer-Aided Multivariate Analysis, Fourth Edition  
A.A.Afifi and V.A.Clark 
A Course in Categorical Data Analysis  
T.Leonard 
A Course in Large Sample Theory  
T.S.Ferguson 
Data Driven Statistical Methods  
P.Sprent 
Decision Analysis—A Bayesian Approach  
J.Q.Smith 
Elementary Applications of Prob ability Theory, Second Edition  
H.C.Tuckwell 
Elements of Simulation  
B.J.T.Morgan 
Epidemiology—Study Design and Data Analysis, Second Edition  
M.Woodward 
Essential Statistics, Fourth Edition  
D.A.G.Rees 
Extending the Linear Model with R: Generalized Linear, Mixed Effects and 
Nonparametric Regression Models  
Julian J.Faraway 
A First Course in Linear Model Theory  
Nalini Ravishanker and Dipak K.Dey 
Interpreting Data—A First Course in Statistics  
A.J.B.Anderson 
An Introduction to Generalized Linear Models, Second Edition  
A.J.Dobson 
Introduction to Multivariate Analysis  
C.Chatfield and A.J.Collins 
Introduction to Optimization Methods and Their Applications in Statistics  
B.S.Everitt",3,False,0.20422989,False,0.18036671,True,0.42609677,False,0.28988725,False,0.21121499
"Large Sample Methods in Statistics  
P.K.Sen and J.da Motta Singer 
Linear Models with R  
Julian J.Faraway 
Markov Chain Monte Carl o—Stochastic Simulation  for Bayesian Inference  
D.Gamerman 
Mathematical Statistics  
K.Knight 
Modeling and Analysis of Stochastic Systems  
V.Kulkarni 
Modelling Binary Data, Second Edition  
D.Collett 
Modelling Survival Data in Medical Research, Second Edition  
D.Collett 
Multivariate Analysis of Variance and Repeated Measures—A Practical Approach 
for Behavioural Scientists  
D.J.Hand and C.C.Taylor 
Multivariate Statistics—A Practical Approach  
B.Flury and H.Riedwyl 
Practical Data Analysis for Designed Experiments  
B.S.Yandell 
Practical Longitudinal Data Analysis  
D.J.Hand and M.Crowder 
Practical Statistics for Medical Research  
D.G.Altman 
Probability—Methods and Measurement  
A.O’Hagan 
Problem Solving—A Statistician’s Guide, Second Edition  
C.Chatfield 
Randomization, Bootstrap and Monte Carlo Methods in Biology, Second Edition  
B.F.J.Manly",3,False,0.14661212,False,0.11145212,True,0.3909265,False,0.1620723,False,0.1461057
"The Theory of Linear Models  
B.Jørgensen 
Texts in Statistical Science",4,False,0.25243723,False,0.18292195,True,0.34494457,True,0.31112993,False,0.1319501
"Extending the Linear Model with 
R 
Generalized Linear, Mixed Effect s and Nonparametric Regression 
Models  
Julian J.Faraway 
 
 
 
 
 
 
 
 
 
Boca Raton London New York",2,False,0.26046214,True,0.34852564,True,0.46362743,True,0.34693158,False,0.16342553
"Preface  
Linear models are central to th e practice of statistics. They ar e part of the core knowledge 
expected of any applied statistician. Linear models are the foundation of a broad range of 
statistical methodologies; this book is a survey of techniques that grow from a linear model. Our starting point is the regression model with response y and predictors x
1,…xp. 
The model takes the form: 
y=β0+β1x1+…+βpxp+ε   
where ε is normally distributed. This book presents three extensions to this framework. 
The first generalizes the y part; the second, the ε part; and the third, the x part of the linear 
model. 
Generalized Linear Models:  The standard linear model cannot handle nonnormal 
responses, y, such as counts or proportions. This motivates the development of 
generalized linear models that can represent cat egorical, binary and other response types. 
Mixed Effect Models:  Some data has a grouped, nested  or hierarchical structure. 
Repeated measures, longitudinal and multilevel data consist of several observations taken on the same individual or group. This indu ces a correlation stru cture in the error, ε. 
Mixed effect models allow the modeling of such data. 
Nonparametric Regression Models:  In the linear model, the predictors, x, are 
combined in a linear way to model the effect on the response. Sometimes this linearity is 
insufficient to capture the structure of the data and more flexibility is required. Methods 
such as additive models, trees and neural networks allow a more flexible regression modeling of the response that combine th e predictors in a nonparametric manner. 
This book aims to provide the reader with a well-stocked toolbox of statistical 
methodologies. A practicing statistician needs to  be aware of and familiar with the basic 
use of a broad range of ideas and techniques. This book will be a succe ss if the reader is 
able to recognize and get started on a wide range of problems. However, the breadth 
comes at the expense of some depth. Fortunately, there are book-length treatments of topics discussed in every chapter of this book, so the reader will know where to go next if 
needed. 
R is a free software environment for statisti cal computing and grap hics. It runs on a 
wide variety of platforms including the Windows, Linux and Macintosh operating 
systems. Although there are se veral excellent statistical packages, only R is both free and 
possesses the power to perform the analyses demonstrated in this book. While it is 
possible in principle to learn statistical me thods from purely theoretical expositions, I 
believe most readers learn best from the dem onstrated interplay of theory and practice. 
The data analysis of real examples is woven into this book and all the R commands 
necessary to reproduce th e analyses are provided. 
Prerequisites:  Readers should possess some knowledge of linear models. The first 
chapter provides a review of these models. This book can be viewed as a sequel to Linear",3,False,0.25434062,True,0.3383819,True,0.55314124,True,0.4012224,False,0.22839594
"Contents  
  
   Preface    ix 
  
1   Introduction    1 
2   Binomial Data    28 
3   Count Regression    61 
4   Contingency Tables    76 
5   Multinomial Data    106 
6   Generalized Linear Models    126 
7   Other GLMs    149 
8   Random Effects    169 
9   Repeated Measures and Longitudinal Data    203 
10   Mixed Effect Models for Nonnormal Responses    221 
11   Nonparametric Regression    232 
12   Additive Models    254 
13   Trees    278 
14   Neural Networks    296 
  
A   Likelihood Theory    307 
B   R Information    316 
   Bibliography    318 
   Index    324",3,False,0.1707263,False,0.1935222,True,0.42441615,False,0.29137468,False,0.22262836
"CHAPTER 1  
Introduction  
This book is about extending the linear model methodology using R statistical software. 
Before setting off on this journey, it is worth reviewing both linear models and R. We 
shall not attempt a detailed description of linear  models; the reader is advised to consult 
texts such as Faraway (2004), Draper and Smith (1998) or Weisberg (2005). However, 
we will review the main points. Also, we do not intend this as a self-contained 
introduction to R as this may be found in books such as Dalgaard (2002) or Maindonald 
and Braun (2003) or from guides obtainable from the R website. Even so, a reader unfamiliar with R should be able to follow the analysis to follow and learn a little R in the 
process without further preparation. 
Let’s consider an example. The 2000 Unit ed States Presidential election generated 
much controversy, particularly in the state of Florida where there were some difficulties 
with the voting machinery. In Meyer (2002), data on voting in the state of Georgia is 
presented and analyzed. Let’s take a look at th is data using R. Please refer to Appendix B 
for details on obtaining and installing R along with the necessary addon packages and data for running the examples in this te xt. R commands are typed at the command 
prompt: >. We start by loading the package of datasets that are used in this book: 
> library(faraway) 
Please remember that every time you want to  access a dataset specific to this book, you 
will need to type the library (faraway) command. Since you might start a new session at any point in this book, in future we will simply assume that you type this first. If you 
forget, you will receive an er ror message notifying you that the data could not be found. 
Next we load the dataset with the Georgia voting information: 
> data(gavote) 
The data command loads the particular dataset into R. In R, the object containing the data 
is called a dataframe . In most installations of R, this data step will be unnecessary as the 
datasets will be silently accessed using a process called lazy loading . However, we will 
retain this command throughout this book as a marker to indicate that we intend to use a particular dataset in R. Ra ther than typing the comman d, you might regard it as a 
reminder to consult the help page for the dataset. We can obtain definitions of the 
variables and more information about the dataset using the help command: 
> help(gavote)",4,False,0.11825979,False,0.21606863,False,0.29335767,True,0.3065406,False,0.14967892
"and what is observed. ε may include measurement error although it is often due to the 
effect of unincluded or unmeasured variables. 
The regression equation is more conveniently written as:  
y=Xβ+ε   
where, in terms of then data points, y=(y1,…, yn)T, ε=(ε1,…, εn)T, β=(β0,…, βp−1)T and: 
   
The column of ones incorpor ates the intercept term. The least squares  estimate of β, 
called 
 minimizes: 
   
Differentiating with respect to β and setting to zero, we find that 
 satisfies: 
   
These are called the normal equations . 
Fitting a linear model:  Linear models in R are fit using the 1m command. For 
example, suppose we model the undercount as the response and the proportions of Gore 
voters and African Americans as predictors: 
> lmod <- 1m(undercount ~ pergore+perAA, gavote) 
This corresponds to the linear model formula: 
undercount= β0+β1pergore+β2perAA+ε   
R uses the Wilkinson-Rogers  notation of Wilkinson and Rogers (1973). For a straight-
forward linear model, such as here, we see that it corresponds to just dropping the 
parameters from the mathematical form. The intercept is included by default. 
We can obtain the least squares estimates of β, called the regression coefficients, 
 by: 
> coef(lmod) 
(Intercept)  pergore    perAA 
   0.032376 0.010979 0.028533 
The construction of the least squares estimates do not require any assumptions about ε. If 
we are prepared to assume that the errors  are at least independent and have equal 
variance, then the Gauss-Markov  theorem tells us that the least squares estimates are the 
best linear unbiased estimates. Although it is not necessary, we might further assume that 
the errors are normally distributed, we might compute the maximum likelihood estimate Extending the linear model with R     8",1,True,0.32270294,False,0.23107073,False,0.23406379,False,0.25668782,False,0.12111543
"(MLE) of β (see Appendix A for more MLEs). For the linear models, these MLEs are 
identical with the least squares estimates. However, we shall find that, in some of the 
extension of linear models considered later in this book, an equivalent notion to least 
squares is not suitable and that likelihood methods must be used. This issue does not arise with the standard linear model. 
The predicted or fitted values are 
while the residuals are 
We can compute these as: 
> predict (lmod) 
  APPLING ATKINSON    BACON    BAKER  BALDWIN    BANKS 
0.041337 0.043291 0.039618 0.052412 0.047955 0.036016 
...  
> residuals(lmod) 
    APPLING   ATKINSON     BACON     BAKER   BALDWIN 
  0.0369466 -0.0069949 0.0655506 0.0023484 0.0035899 
... 
where the ellipsis indicates that (much of) the output has been omitted. 
It is useful to have some notion of how well the model fits the data. The residual sum 
of squares (RSS) is 
 This can be computed as: 
> deviance(lmod) 
[1] 0.09325 
The term deviance  is a more general measure of fit than RSS, which we will meet again 
in chapters to follow. For linear models, the deviance is the RSS. 
The degrees of freedom  for a linear model is the number of cases minus the number of 
coefficients or: 
> df.residual(lmod) 
[1] 156 
> nrow(gavote) - length(coef(lmod)) 
[1] 156 
Let the variance of the error be σ2, then σ is estimated by the residual standard error 
computed from 
 For our example, this works out to be: 
> sqrt (deviance (lmod) / df.residual (lmod)) 
[1] 0.024449 
Although several useful regression quantities ar e stored in the 1m model object (which 
we called lmod in this instance), we can compute several more using the summary 
command on the model object. For example: 
> lmodsum <- summary(lmod) 
> lmodsum$sigma Introduction     9",4,True,0.47712913,True,0.43247408,False,0.16390258,True,0.38334343,False,0.28254977
"[1] 0.024449 
R is an object-oriented language. One important feature of such a language is that generic  
functions, such as summary, recognize the type of object being passed to it and behave appropriately. We used summary for dataframes above and now for linear models, 
residuals is another generic function and we shall see how it can be applied to many 
model types and return appropriately defined residuals. 
The deviance measures how well the model fits in an absolute sense, but it does not 
tell us how well the model fits in a re lative sense. The popular choice is R
2, called the 
coefficient of determination  or percentage of variance explained : 
   
where 
 and stands for total sum of squares. This can be most 
conveniently extracted as: 
> lmodsum$r.squared 
[1] 0.053089 
We see that R2 is only about 5% which indicates that this particular model does not fit so 
well. An appreciation of what constitutes a good value of R2 varies according to the 
application. Another way to think of R2 is the (squared) correla tion between the predicted 
values and the response: 
> cor (predict(lmod), gavote$undercount) ^2 
[1] 0.053089 
R2 suffers as a criterion for choosing models among those available because it can never 
decrease when you add a new predictor to the model. This means that it will favor the 
largest models. The adjusted R2 makes allowance for the fact  a larger model also uses 
more parameters. It is defined as: 
   
Adding a predictor will only increase 
 if it has some predictive value. Furthermore, 
minimizing 
 means maximizing 
 over a set of possible linear models. The value can 
be extracted as: 
> lmodsum$adj.r.squared 
[1] 0.040949 
One advantage of R over many statistical p ackages is that we can extract all these 
quantities individually for subsequent calculati ons in a convenient way. However, if we 
simply want to see the regression output prin ted in a readable way, we use the summary: Extending the linear model with R     10",0,False,0.24975851,True,0.42210582,False,0.227741,False,0.25319666,True,0.31548938
"Thus we would reject the null hypothesis that the smaller model is correct if 
 
For example, we might compar e the two linear models we have fit so far. The smaller 
model has just pergore and perAA while the larger model adds rural and equip along with 
an interaction. We  may compute the F-test as: 
> anova(lmod,lmodi) 
Analysis of Variance Table 
Model 1: undercount ~ pergore + perAA 
Model 2: undercount ~ cperAA + cpergore * rural + eguip 
  Res.Df    RSS  Df Sum of Sg    F Pr(>F) 
1 156 0.0932 
2 150 0.0818      6    0.0115 3.51 0.0028 
It does not matter that the variables have been  centered in the larger model but not in the 
smaller model, because the centering makes no difference to the RSS. The p-value here is 
small indicating the null hypothesis of preferring the smaller model should be rejected. 
One common F-test is the comparison of the current model to the null model, which is 
the model with no predictors and just an inte rcept term. This corresponds to the question 
of whether any of the variables have predictive value. For the larger model above, we can 
see that this F-statistic is 3.83 on 8 and 1 50 degrees of freedom with a p-value of 0.0004. 
We can see clearly that at least some of  the predictors have some significance. 
Another common need is to test specific predictors in the model. It is possible to use 
the general F-testing method: fit a model with the predictor and without the predictor and 
compute the F-statistic. It is important to know what other predictors are also included in 
the models and the results may differ if these ar e also changed. An a lternative approach is 
to use a t-statistic for testing the hypothesis: 
   
and check for significance using a t-distribution with n−p degrees of freedom. This 
approach will produce exactly the same p-value as the F-testing method. For example, in 
the larger model above, the test for th e significance of the proportion of African 
Americans gives a p-value of 0.3648. This indicates that this predictor is not statistically 
significant after adjusting for the effect of the other predictors on the response. 
We would usually avoid using the t-tests for the levels of qualitative predictors with 
more than two levels. For example, if we were interested in testing the effects of the 
various voting equipment, we would need to fit a model without this predictor and compute the corresponding F-test. A comparison of all m odels with one predictor less 
than the larger model may be obtained conveniently as: 
> dropl(lmodi, test=""F"") 
Single term deletions 
Model: 
undercount ~ cperAA + cpergore * rural + equip 
               Df Sum of Sq      RSS   AIC F value Pr 
(F) 
<none>                      0.081775 -1186 Extending the linear model with R     14",4,False,0.24090913,False,0.21821366,False,0.2018806,False,0.19930178,True,0.31301996
"them correctly. Another set of assumptions concerns the random part of the model: ε. We 
require that the errors have equal variance, be uncorrelated and have a normal 
distribution. We are also interested in detecting points, called outliers,  that are unusual in 
that they do not fit the model that seems adequate for the rest of the data. Ideally, we would like each case to have an equal contribution to the fitted model; yet sometimes a 
few points have a much larger effect  than others. Such points are called influential . 
Diagnostic methods can be graphical or numerical. We generally prefer graphical 
methods because they tend to be more versatile and informative. It is virtually impossible 
to verify that a given model is exactly correct. The purpose of the diagnostics is more to 
check whether the model is not grossly wrong. Indeed, a successful data analyst should pay more attention to avoiding big mistakes than optimizing the fit. 
A collection of four useful diagnostics can be simply obtained with: 
> plot (lmodi) 
as can be seen in Figure 1.4. The plot in the upper-left panel shows the residuals plotted against the fitted values. The plot can be used to detect lack of fit. If the residuals show 
some curvilinear trend, this is a sign that some change to the model is required, often a transformation of one of the variables. In this instance, there is no sign of such a problem. 
The plot is also used to check the constant variance assumption on the errors. In this case, 
it seems the variance is roughly constant as the fitted values vary. Assuming symmetry of the errors, we can effectively double the resolution by plotting the absolute value of the 
residuals against the fitted values. As it happens 
tends to be rather skewed and is better 
to use 
 Such a plot is shown in the lower-left panel, confirming what we have already 
observed about the constancy of the variance. Notice that a few larger residuals have 
been labeled. 
The residuals can be assessed for normality using a QQ plot . This compares the residuals 
to “ideal” normal observations. We plot the sorted residuals against 
 for i=1,…, 
n. This can be seen in the upper-right panel of Figure 1.4. In this plot, the points follow a 
linear trend (except for one or two cases), indicating that normality is a reasonable 
assumption. If we observe a curve, this  indicates skewness, suggesting a possible 
transformation of the response, while two ta ils of points diverging from linearity would 
indicate a long-tailed error, suggesting that we should consider robust fitting methods. 
Particularly for larger datasets, the normality assumption is not crucial, as the inference 
will be approximately correct in spite of the nonnormality. Only a clear deviation from 
normality should necessarily spur some action to change the model. 
The Cook statistics are a popular influe nce diagnostic because they reduce the 
information to a single value for each case. They are defined as: 
   
 Extending the linear model with R     16",1,False,0.21694171,True,0.37679392,False,0.23243155,False,0.1786863,False,0.2781896
"> termplot(lmodi,partial=TRUE,terms=1) 
The line is the least squares fit to the data on this plot as well as having the same  
 
Figure 1.5 Half-normal plot of the 
leverages is shown on the left and a 
partial residual plot for the proportion of African Americans is shown on the right.  
slope as the cperAA term in the current model. This plot gives us a snapshot of the 
marginal relationship between this predictor and the response. In this case, we see a linear 
relationship indicating that it is not worthwhile seeking transformations. Furthermore, 
there is no sign that a few points are having undue influence on the relationship. 
Robust regression:  Least squares works well when there are normal errors, but 
performs poorly for long-tailed errors. We ha ve identified a few potential outliers in the 
current model. One approach is to simply eliminate the outliers from the dataset and then 
proceed with least squares. This approach is  satisfactory when we are convinced that the 
outliers represent truly incorrect observations, but even then, detecting such cases is not 
always easy as multiple outliers can mask each  other. However, in other cases, outliers 
are real observations. Sometimes, removing these cases simply creates other outliers. A generally better approach is to use a robust alternative to least squares that downweights 
the effect of larger errors. The Huber method is the default choice of the rlm function, 
which is part of the MASS package of Venables and Ripley (2002): 
> library(MASS) 
> rlmodi <- rlm(undercount ~ 
cperAA+cpergore*rural+equip, gavote) 
> summary(rlmodi) 
Coefficients: 
                    Value   Std. Error t value 
(Intercept)          0.041   0.002     17.866 
cperAA               0.033   0.025      1.290 Introduction     19",1,False,0.13720684,True,0.40589625,False,0.167355,True,0.30331117,False,0.12961821
"Transformation:  Models can sometimes be improved by transforming the variables. 
Ideas for transformations can come from se veral sources. One method is to search 
through a family of possible transformations looking for the best fit. An example of this 
approach is the Box-Cox method of selecting a transformation on the response variable. Alternatively, the diagnostic plots for the cu rrent model can suggest transformations that 
might improve the fit or ameliorate an apparent violations of the assumptions. In other 
situations, transformations may be motivated by theories concerning the relationship between the variables or to aid the interpretation of the model. 
For this dataset, transformation of the response is problematic for both technical and 
interpretational reasons. The minimum undercount is exactly zero which precludes directly applying some popular transformations such as the log or inverse. An arbitrary 
fix for this problem is to add a small amount (say 0.005 here) to the response which 
would enable the use of all power transformations. The application of the Box-Cox 
method, using the boxcox function from the MASS package, suggests a square root 
transformation of the response. However, it is  difficult to give an interpre-tation to the 
regression coefficients with this transformation on the response. Other than no 
transformation at all, a logged response does allow a simple interpretation. For an 
untransformed response, the coefficients repr esent addition to the undercount whereas for 
a logged response, the coefficients can be interpreted as multiplying the response. So we 
see that, although transformations of the response might sometimes improve the fit, they 
can lead to difficulties with interpretation an d so should be applied with care. Another 
point to consider is that if the untransformed response was normally distributed, it will 
not be so after transformation. This suggests considering nonnormal, continuous 
responses as seen in Section 7.1, for example. 
Transformations of the predictors are le ss problematic. Let’s first consider the 
proportion of African Americans predictor in the current model. Polynomials provide a 
commonly used family of transformations. The use of orthogonal polynomials is recommended as these a more numerically stab le and make it easier to select the correct 
degree: 
> plmodi <- 1m(undercount ~ 
poly(cperAA,4)+cpergore*rural+equip, gavote) 
> summary(plmodi) 
Coefficients: 
                    Estimate Std. Error t value 
Pr(>|t|) 
(Intercept)          0.04346    0.00288   15.12  < 2e-
16 
poly (cperAA, 
4)1    0.05226    0.06939    0.75   0.4526 
poly (cperAA, 4)2   -0.00299    0.02613   -
0.11   0.9091 
poly (cperAA, 4)3   -0.00536    0.02427   -
0.22   0.8254 
poly (cperAA, 4)4   -0.01651    0.02420   -
0.68   0.4961 
cpergore             0.01315    0.05693    0.23   0.817
6 Introduction     21",1,False,0.16157576,True,0.30143565,False,0.15861668,False,0.23312181,False,0.23735276
"ruralurban          -0.01913    0.00474   -4.03 
0.000088 
equipOS-
CC           0.00644    0.00472    1.36   0.1746 
equipOS-
PC           0.01559    0.00588    2.65   0.0089 
equipPAPER          -0.01027    0.01720   -
0.60   0.5514 
equipPUNCH           0.01405    0.00687    2.05   0.042
5 
cpergore:ruralurban -0.01054    0.04136   -
0.25   0.7993 
Residual standard error: 0.0235 on 147 degrees of 
freedom 
Multiple R-Squared: 0.173, Adjusted R-squared: 0.111 
F-statistic: 2.79 on 11 and 147 DF,   p-value: 0.00254 
The hierarchy principle requires that we avoi d eliminating lower-order terms of a variable 
when high-order terms are still in the model. From the output, we see that the fourth-order term is not significant and can be eliminated. With standard polynomials, the 
elimination of one term would cause a change in the values of the remaining coefficients. 
The advantage of the orthogonal polynomials is that the coefficients for the lower-order terms do not change as we change the maximu m degree of the model. Here we see that 
all the terms of cperAA are not significant and all can be removed. Some insight into the 
relationship may be gained by plotting the fit on top of the partial residuals: 
> termplot(plmodi,partial=TRUE,terms=1) 
The plot, seen in the first panel of Figure 1. 6, shows that the quartic polynomial is not so 
different from a constant fit, explaining the lack of significance. 
Polynomial fits become less attractive with higher-order terms. The fit is not local in 
the sense that a point in one part of the ra nge of the variable affects the fit across the 
whole range. Furthermore, polynomials tend to ha ve rather oscillatory fits and extrapolate 
poorly. A more stable fit can be had using splines, which are piecewise polynomials. 
Various types of splines are available and th ey typically have the local fit and stable 
extrapolation properties. We demonstr ate the use of cubic B-splines here: 
> library (splines) 
> blmodi <- lm(undercount ~ cperAA+bs (cpergore, 
4)+rural+equip, gavote) 
Because the spline fit for cperAA was very  similar to orthogonal polynomials, we 
consider cpergore here for some variety. No tice that we have eliminated the interaction 
with rural for simplicity. The complexity of the B-spline fit may be controlled by 
specifying the degrees of freedom. We have used four here. The nature of the fit can be 
seen in the second panel of Figure 1.6: 
> termplot(blmodi,partial=TRUE,terms=2) Extending the linear model with R     22",0,False,0.1988245,False,0.23063514,False,0.12544692,True,0.309403,False,0.16795924
"Figure 1.6 Partial fits using orthogonal 
polynomials for cperAA (shown on the left) and cubic B-splines for cpergore 
(shown on the right).  
We see that the curved fit is not much different from a constant. 
Variable selection:  One theoretical view of the problem  of variable selection is that 
one subset of the available variables represents  the correct model for the data and that any 
method should be judged by its success in id entifying this correct model. While this may 
be a tempting world in which to test competing variable selection methods, it seems unlikely to match with reality. Even if we believe that a correct model even exists, it is 
more than likely that we will not have recorded all the relevant variables or not have 
chosen the correct transformations or functional form for the model amongst the set we choose to consider. We might then retreat from the initial goal and hope to identify the 
best model from the available set. Even then, we would need to define what is meant by 
best. 
Linear modeling serves two broad goals. Some build linear models for the purposes of 
prediction—they expect to observe new X and wish to predict y, along with measures of 
uncertainty in the prediction. Prediction perf ormance is improved by removing variables 
that contribute little or nothing to the model. We can define a criterion for prediction 
performance and search for the model that optim izes that criterion. One such criterion is 
the adjusted R
2 previously mentioned. The regsubsets function in the leaps package 
implements this search. For problems involv ing a moderate number of variables, it is 
possible to exhaustively search all possible models for the best. As the number of 
variables increases, exhaustive search become prohibitive and various stepwise methods 
must be used to search the model space. Th e implementation also has the disadvantage 
that it can only be applied to quantitative predictors. 
Another popular criterion is the Akaike Information Criterion or AIC defined as: 
AIC=−2 maximum log likelihood+2 p   
where p is the number of parameters. This criterion has the advantage of generality and 
can be applied far beyond normal linear models. The step command implements a Introduction     23",3,False,0.25206247,False,0.11459215,False,0.11524123,True,0.32871163,False,0.08454478
"stepwise search strategy through the space of possible models. It does allow qualitative 
variables and respects the hierarchy principle. We start by defining a rather large model: 
> biglm <- lm(undercount ~ (equip+econ+rural+atlanta) 
^2+ 
   (equip+econ+rural+atlanta)* (perAA+pergore), gavote) 
This model includes up to all two-way interactions between the qualitative variables 
along with all two-way interaction between a qualitative and a quantitative variable. All 
main effects are included. The step command sequentially eliminates terms to minimize 
the AIC: 
> smallm <- step (biglm, trace=F) 
The resulting model includes interactions betw een equip and econ, econ and perAA, and 
rural and perAA, together with the associated  main effects. The trace=F arguments blocks 
the large amount of intermediate model information that we would otherwise see. 
Linear modeling is also used to try to understand the relationship between the 
variables—we want to develop an explanation for the data. For this dataset, we are much 
more interested in explanation than prediction. However, the two goals are not mutually 
exclusive and often the same methods are used for variable selection in both cases. Even 
so, when explanation is the goal, it may be unwise to rely on completely automated variable selection methods. For example, the proportion of voters for Gore was 
eliminated from the model by the AIC-based step method and yet we know this variable 
to be strongly correlated with  the proportion of African Americans which is in the model. 
It would be rash to conclude that the latter variable is important and the former is not—
the two are intertwined. Researchers interested  in explaining the relationship may prefer a 
more manual variable selection approach th at takes into account background information 
and is geared toward the substantive questions of interest. 
The other major class of variable selection methods is based on testing. We can use F-
tests to compare larger models with smaller nested models. A stepwise testing approach can then be applied to select a model. The consensus view among statisticians is that this 
is an inferior method to variable selection compared to the criterion-based methods. 
Nevertheless, testing-based methods are still useful, particularly when under manual 
control. They have the advantage of applicability across a wide class of models where 
tests have been developed. They allow the user to respect restrictions of hierarchy and 
situations where certain variables must be  included for explanatory purposes. Let’s 
compare the AlC-selected models abov e to models with one fewer term: 
> dropl(smallm,test=""F"") 
Single term deletions 
Model: 
undercount ~ equip + econ + rural + perAA + equip:econ 
+ equip:perAA + 
    rural:perAA 
            Df Sum of Sq    RSS   AIC F value  Pr(F) 
<none>                   0.0536 -1231 Extending the linear model with R     24",3,False,0.2752874,False,0.22408788,False,0.28368273,True,0.35822713,False,0.15845725
"Because there are only two pape r-using counties, there is insu fficient data to estimate the 
interaction terms involving paper. This model output is difficult to interpret because of 
the interaction terms. 
Conclusion:  Let’s attempt an interpretation of this final model. Certainly we should 
explore more models and check more diagnostics, so our conclusions can only be 
tentative. The reader is invited to investigate other possibilities. 
To interpret interactions, it is often helpful to  construct predictions for all the levels of 
the variables involved. Here we generate all combinations of equip and econ for a median 
proportion of perAA: 
> pdf <- data.frame(econ=rep(levels(gavote$econ),5) , 
  equip=rep(levels(gavote$equip),rep(3,5)),perAA=0.233) 
We now compute the predicted undercount for all 15 combinations and display the result 
in a table: 
> pp <- predict (finalm, new=pdf) 
> xtabs(round(pp,3) ~ econ + equip, pdf) 
         equip 
econ      LEVER  OS-CC  OS-PC  PAPER  PUNCH 
  middle  0.032  0.046  0.039  0.004  0.037 
  poor    0.052  0.055  0.108  0.024  0.053 
  rich    0.015  0.031  0.009 -0.013  0.040 
We can see that the undercount is lower in ri cher counties and higher in poorer counties. 
The amount of difference depends on the voting system. Of the three most commonly used voting methods, the LEVER method seems best. It is hard to separate the two 
optical scan methods, but ther e is clearly a problem with the precinct count in poorer 
counties, which is partly due to the two outliers we observed earlier. We notice one 
impossible prediction—a negative undercount in rich paper-using counties, but given the 
absence of such data (there were no such counties), we are not too disturbed. 
We use the same approach to investigate the relationship between the proportion of 
African Americans and the voting equipment. We set the proportion of African 
Americans at three levels—the first quartile, the median and the third quartile and then 
compute the predicted undercount for all types of voting equipment. We set the econ variable to middle: 
> pdf <- 
data.frame(econ=rep(""middle"",15),equip=rep(levels(gavot
e$equip) , 
  rep(3,5)),perAA=rep(c(.11,0.23,0.35),5)) 
> pp <- predict(finalm, new=pdf) 
We create a three-level factor for the three le vels of perAA to aid the construction of the 
table: 
> propAA <- gl(3,1,15,labels=c(""low"",""medium"", ""high"")) 
> xtabs(round(pp,3) ~ propAA + equip, pdf) Extending the linear model with R     26",3,False,0.15290877,False,0.21906926,True,0.3194657,False,0.18386137,False,0.16706327
"equip 
propAA    LEVER  OS-CC  OS-PC  PAPER   PUNCH 
  low     0.037  0.038  0.045 -0.007   0.031 
  medium  0.032  0.046  0.039  0.003   0.036 
  high    0.027  0.053  0.034  0.014   0.042 
We see that the effect of the proportion of  African Americans on  the undercount is 
mixed. High proportions are associated wi th higher undercounts for OS-CC and PUNCH 
and associated with lower unde rcounts for LEVER and OS-PC. 
In summary, we have found that  the economic status of a co unty is the clearest factor 
determining the proportion of undercounted vo tes, with richer counties having lower 
undercounts. The type of voting equipment and the proportion of African Americans do 
have some impact on the response, but the di rection of the effects are not simply stated. 
We would like to emphasize again that this da taset deserves further analysis before any 
definitive conclusions are drawn. 
Exercises  
Since this is a review chapter, it is best to consult the recommended background texts for 
specific questions on linear models. However, it is worthwhile gaining some practice 
using R on some real data. Your data analysis should consist of: 
• An initial data analysis that explores the numerical and graphical characteristics of the 
data. 
• Variable selection to choose the best model. 
• An exploration of transformations to improve the fit of the model. 
• Diagnostics to check the assumptions of your model.  
• Some predictions of future observations for interesting values of the predictors. 
• An interpretation of the meaning of the mode l with respect to the particular area of 
application. 
There is always some freedom in deciding which methods to use, in what order to apply 
them, and how to interpret the results. So th ere may not be one clear right answer and 
good analysts may come up with different models. 
Here are some datasets which should provide some good practice at building linear 
models: 
1. The swiss data—use Fertility as the response. 
2. The rock data—use perm as the response. 
3. The mtcars data—use mpg as the response. 
4. The attitude data—use rating as the response. 
5. The prostate data—use lpsa as the response. 
6. The teengamb data—use gamble as the response. Introduction     27",3,False,0.07396208,False,0.1755626,True,0.30752555,False,0.17095476,False,0.11370424
"We might consider the number of damage incidents to be binomially distributed. For a 
linear model, we require the errors to be approximately normally di stributed for accurate 
inference. However, for a binomial with only six trials, the normal approx- 
 
Figure 2.1 Damage to O-rings in 23 
space shuttle missions as a function of 
launch temperature. Least squares fit 
line is shown.  
imation is too much of a stretch. Furthermore, the variance of a binomial variable is not 
constant which violates another crucial assumption of the linear model. 
The standard linear model is clearly not dire ctly suitable here. Although, we could use 
transformation and weighting to  correct some of these problem s, it is better to develop a 
model that is directly suited for binomial data. 
2.2 Binomial Regression Model  
Suppose the response variable Yi for i=1,…, ni is binomially distributed B(n i, pi) so that: 
   
We further assume that the Yi are independent. The individu al trials that compose the 
response Yi are all subject to the same q predictors ( xi1,…, xiq). The group of trials is 
known as a covariate class . We need a model that describes the relationship of x1,…, xq 
to p. Following the linear model approach, we construct a linear predictor:  
ηi=β0+β1xi1+…+βqxiq   Binomial data     29",2,False,0.08584046,False,0.13276607,True,0.34425488,False,0.16195962,False,0.17551908
"> summary(probitmod) 
Coefficients: 
            Estimate Std. Error z value Pr(>|z|) 
(Intercept)   5.5915     1.7105    3.27   0.0011 
temp         -0.1058     0.0266   -3.98  6.8e-05 
(Dispersion parameter for binomial family taken to be 
1)  
    Null deviance: 38.898 on 22 degrees of freedom 
Residual deviance: 18.131 on 21 degrees of freedom 
AIC: 34.89 
Although the coeffici ents seem quite different, the fits are similar, particularly in the 
range of the data, as seen in Figure 2.2: 
> lines(x, pnorm(5.5915-0.1058*x), lty=2) 
We can predict the response at 31°F for both models: 
> ilogit (11.6630-0.2162*31) 
[1] 0.99304 
> pnorm(5.5915-0.1058*31) 
[1] 0.9896 
We see a very high probability of damage with either model although we still need to 
develop some inferential techniques before we leap to conclusions. 
2.3 Inference  
Consider two models, a larger model with l parameters and likelihood LL and a smaller 
model with s parameters and likelihood LS where the smaller model represents a linear 
subspace (a linear restriction on the paramete rs) of the larger model. Likelihood methods 
suggest the likelihood ratio statistic: 
 (2.1) 
as an appropriate test statistic for comparing the two models. Now suppose we choose a 
saturated larger model—such a model typically has as many parameters as cases and has 
fitted values 
 In such a case, the test statistic becomes: 
   
where ŷi are the fitted values from the smaller model. Now since the saturated model fits 
as well as any model can fit, the deviance D  measures how close the (smaller) model 
comes to perfection. Thus deviance is a measure of goodness of fit. In the output for the Extending the linear model with R     32",0,False,0.26057065,True,0.3086982,False,0.25446367,False,0.16662614,True,0.37851995
"models above, the Residual deviance is the deviance for the current model while the Null 
deviance is the deviance for a model with no predictors and just an intercept term. 
Provided that Y is truly binomial and that the ni are relatively large, the deviance is 
approximately χ2 distributed with n–l degrees of freedom if the model is correct. Thus we 
can use the deviance to test whether the model is an adequate fit. For the logit model of 
the Challenger data, we may compute: 
> pchisq(deviance(logitmod) , 
df.residual(logitmod),lower=FALSE) 
[1] 0.71641 
Since this p-value is well in excess of 0.05, we may conclude that this model fits 
sufficiently well. Of course, this does not mean that this model is correct or that a simpler 
model might not also fit adequately. Even so, for the null model:  
> pchisq(38.9,22,lower=FALSE) 
[1] 0.014489 
we see that the fit is inadequate, so we cannot ascribe the response to simple variation not 
dependent on any predictor. Note that a 
 variable has mean d and standard deviation 
so that it is often possible to quickly judge whether a deviance is large or small 
without explicitly computing the p-value. If the deviance is far in excess of the degrees of 
freedom, the null hypothesis can be rejected. 
The χ2 distribution is only an  approximation that becomes more accurate as the ni 
increase. For the case, ni=1, when yi=0 or 1, in other words, a binary response, the 
deviance reduces to: 
   
For a deviance to measure fit, it has to compare the fitted values 
 to the data yi, but here 
we have only a function of 
 Thus this deviance does not assess goodness of fit and 
furthermore, it is not even approximately χ2 distributed. Other methods must be used to 
judge goodness of fit for binary data—for example, the Hosmer-Lemeshow test described 
in Hosmer and Lemeshow (2000). 
The approximation is very poor for small ni. Although it is not possible to say exactly 
how large ni should be for an adequate approximation, ni≥5 has often been suggested. 
Permutation or bootstrap methods might be considered as an alternative. 
We can also use the deviance to compare two nested models. The test statistic in (2.1) 
becomes DS—DL. This test statistic is asymptotically distributed 
 assuming that the 
smaller model is correct and the distributional assumptions hold. We can use this to test 
the significance of temperature by computing the difference in the deviances between the 
model with and without temperature. The model without temperature is just the null model and the difference in degrees of freedom or parameters is one: Binomial data     33",0,False,0.26683235,False,0.24107167,False,0.1639862,False,0.17322636,True,0.5182659
"> pchisq(38.9–16.9,1,lower=FALSE) 
[1] 2.7265e-06 
Since the p-value is so small, we conclude that  the effect of launch temperature is 
statistically significant. An alternative to this test is the z-value, which is 
 here –
4.07 with a p-value of 4.8e-05. In contrast to the normal (Gaussian) linear model, these 
two statistics are not identical. In this particul ar example, there is no practical difference, 
but in some cases, especially with sparse data, the standard  errors can be overestimated 
and so the z-value is too small and the significance of an effect could be missed. This is 
known as the Hauck-Donner effect—see Hauc k and Donner (1977).  So the deviance-
based test is preferred. 
Again, there are concerns with the accu racy of the approximation, but the test 
involving differences of deviances is generall y more accurate than the goodness of fit test 
involving a single deviance. 
Confidence intervals for the regression parameters may be constructed using normal 
approximations for the parameter estimates. A 100(1— α)% confidence interval for βi 
would be: 
   
where zα/2 is a quantile from the normal distribution. Thus a 95% confidence interval for 
β1 in our model would be: 
> c(-0.2162–1.96*0.0532,-0.2162+1.96*0.0532) 
[1] -0.32047 -0.11193 
It is also possible to construct a profile likelihood-based confidence interval: 
> library(MASS) 
> confint(logitmod) 
Waiting for profiling to be done... 
               2.5 %   97.5 % 
(Intercept)  5.57543 18.73812 
temp        -0.33267 -0.12018 
It is important to load the MASS package or the default confint method for ordinary 
linear models will be used (whi ch will not be quite right). The profile likelihood method 
is generally preferable for the same Hauck-Donner reasons discussed above although it is more work to compute. 
Although we have only computed results for the logit link, the same methods would 
apply for the probit or any other link. 
 
 Extending the linear model with R     34",0,False,0.2604345,False,0.1698714,False,0.22860092,False,0.14419876,True,0.33676863
"correspond to p=1/2. The dotted line is 
equivalent from brlr.  
androgen       100.9    92755.6  0.00109       1 
(Dispersion parameter for binomial family taken to be 
1) 
    Null deviance: 3.5426e+01  on 25  degrees of 
freedom 
Residual deviance: 2.3229e-09  on 23  degrees of 
freedom 
AIC: 6 
Number of Fisher Scoring iterations: 25 
Notice that the residual deviance is extremel y small indicating a very good fit and yet 
none of the predictors are si gnificant due to the high standa rd errors. We see that the 
maximum default number of iterations (25) ha s been reached. A look at the data reveals 
the reason for this. We s ee that the two groups are linearly separable  so that a perfect fit 
is possible. We can compute the line separating the groups by finding the line that corresponds to p=1/2 which is when the logit is zero: 
> abline(-84.5/90.2,100.9/90.2) 
We suffer from an embarrassment of riches  in this example—we can fit the data 
perfectly. Unfortunately, this results in unstable estimates of the parameters and their 
standard errors and would (probably falsely) suggest that perfect predictions can be 
made. An alternative fitting approach might be considered in such cases called exact 
logistic regression . See Cox (1970) and the work of Cyrus Mehta, for example: Mehta 
and Patel (1995). Currently, th ere are no comprehensive packag es for such exact methods 
in R, although it is available in products such as LogExact©. 
An alternative to exact methods is the bias reduction method of Firth (1993). For the 
and indeed a sensible unbiased estimator would be difficult to ob- 
tain. Firth’s method removes the O(1/n) term from the asymptotic bias of estimated 
coefficients. These estimates have the advantage of always being finite: 
> library(brlr) 
> modb <- brlr(orientation ~ estrogen + androgen, 
hormone, 
  family=binomial) 
> summary(modb) 
Coefficients: 
            Value   Std. Error t value 
(Intercept) -3.650   2.910     -1.254 
estrogen    -3.586   1.499     -2.393 
androgen     4.074   1.621      2.513 
Deviance: 3.70 
Penalized deviance:  4.184 
Residual df: 23 Extending the linear model with R     44",2,False,0.27942383,False,0.2131876,False,0.16384834,False,0.26084638,True,0.3416202
"We can see that this results in significant predictors which we expect given Figure 2.4. 
Although the fit appears, judging from the coefficients, to be different from the glm 
result, it is effectively very close as we can see by plotting the line corresponding to 
p=1/2: 
> abline(-3.65/3.586,4.074/3.586,lty=2) 
Instability in parameter estimation will also occur in datasets that approach linear 
separability. Care will be needed in such cases. 
2.9 Goodness of Fit  
The deviance is one measure of how well the model fits the data, but there are 
alternatives. The Pearson’s X2 statistic takes the general form: 
   
where Oi is the observed counts and Ei are the expected counts for case i. For a binomial 
response, we count the number of successes for which 0i=yi while 
 and failures 
for which Oi=ni–yi and 
 which results in: 
   
If we define Pearson residuals  as: 
   
which can be viewed as a type of standardized residual, then 
 So the 
Pearson’s X2 is analogous to the residual sum of squares used in normal linear models. 
The Pearson X2 will typically be close in size to the deviance and can be used in the 
same manner. Alternative versions of the hypothesis tests described above might use the 
X2 in place of the deviance with the sa me approximate null distributions.  
However, some care is necessary because th e model is fit to minimize the deviance 
and not the Pearson’s X2. This means that it is possible, although unlikely, that the X2 
could increase as a predictor is added to the model. X2 can be computed like this: 
> modl <- glm(cbind(dead,alive) ~ conc, 
family=binomial, data=bliss) 
> sum(residuals(modi,type=""pearson"")^2) 
[1] 0.36727 
> deviance(modl) 
[1] 0.37875 
As can be seen, there is little difference here between X2 and the deviance. Binomial data     45",1,True,0.33899668,False,0.25770444,False,0.20357823,False,0.116382234,True,0.3520943
"2.11 Overdispersion  
If the binomial GLM model specification is corr ect, we expect that the residual deviance 
will be approximately distributed χ2 with the appropriate degr ees of freedom. Sometimes, 
we observe a deviance that is much larger than would be expected if the model were 
correct. We must then determine which asp ect of the model specification is incorrect. 
The most common explanation is that we have the wrong structural form for the 
model. We have not included the right predictors or we have not transformed or combined them in the correct way. We have a number of ways of determining the 
importance of potential additional predictors and diagnostics for determining better 
transformations—see Section 6.4. Suppose, howe ver, that we are able to exclude this 
explanation. This is difficult to achieve, but wh en we have only one or two predictors, it 
is feasible to explore the model space quite t horoughly and be sure that there is not a 
plausible superior model formula. 
Another common explanation for a large devi ance is the presence of a small number 
of outliers. Fortunately, these are easily checked using diagnostic methods explained more fully in Section 6.4. When larger numb ers of points are identified as outliers, they 
become unexceptional, and we might more reasonably conclude that there is something 
amiss with the error distribution. 
Sparse data can also lead to large deviances . In the extreme case of a binary response, 
the deviance is not even approximately χ
2. In situations where th e group sizes are simply 
small, the approximation is poor. Because we cannot judge the fit using the deviance, we shall exclude this case from further consideration in this section. 
Having excluded these other possibilities, we might explain a large deviance by 
deficiencies in the random part of th e model. A binomial distribution for Y arises when 
the probability of success p is independent and identical fo r each trial within  the group. If 
the group size is m, then var Y=mp(1–p) if the binomial assumptions are correct. 
However, the assumptions are broken, th e variance may be greater. This is 
overdispersion.  In rarer cases, the variance is less and underdispersion  results. 
There are two main ways that overdispersio n can arise—the indepe ndent or identical 
assumptions can be violated. We look at the constant p assumption first. It is easy to see 
how there may be some unexplained heterogeneity within a group that might lead to 
some variation in p. For example, in the shuttle disaster case study of Section 2.1, the 
position of the O-ring on the booster rocket may have some effect on the failure 
probability. Yet this variable was not recorded and so we cannot include it as a predictor. 
Heterogeneity can also result from clustering. Suppose a population is divided into clusters, so that when you take a sample, you actually get a sample of clusters. This 
would be common in epidemiological applications. 
Let the sample size be m, the cluster size be k and the number of clusters be l=m/k. Let 
the number of successes in cluster i be Z
i~B(k, p i). Now suppose that pi is a random 
variable such that Epi=p and var pi=τ2p(1–p). Let the total number of successes be 
Y=Z1+…+Z l. Then: 
   
as in the standard case, but: Binomial data     49",4,False,0.17803282,False,0.2566385,False,0.20460945,False,0.12559554,True,0.3518811
"The half-normal plot is shown in the left panel of Figure 2.5. No single outlier is 
apparent. Perhaps one can discern a larger number of residuals which seem to follow a 
more dispersed distribution than the rest.  
We can also check whether the predictors are correctly expressed by plotting the 
empirical logits . These are defined as: 
   
The halves are added to prevent infinite va lues for groups consisting of all successes or 
failures. We now construct an interaction plot of the empirical logits: 
 
 
Figure 2.5 Diagnostic plots for the 
trout egg model. A half-normal plot of 
the residuals is shown on the left and an interaction plot of the empirical 
logits is shown on the right.  
> elogits <- 
log((troutegg$survive+0.5)/(troutegg$total- 
  troutegg$survive+0.5)) 
> 
with(troutegg,interaction.plot(period,location,elogits)
) 
Interaction plots are always difficult to interpret conclusively, but there is no obvious sign 
of large interactions. So there is no evidence that the linear model is inadequate. We do 
not have any outliers and the functional form of the model appears to be suitable, but the 
deviance is still larger than should be expe cted. Having eliminated  these more obvious 
causes as the source of the problem, we may now put the blame on overdispersion. Extending the linear model with R     52",1,False,0.18483636,True,0.42461288,False,0.23794156,False,0.19685358,False,0.25210992
"response while that on the right is for 
the square-root transformed response.  
We see clear evidence of nonconstant varian ce in left panel of Figure 3.1. Some 
experimentation (or the use of the Box-Cox method) reveals that a square-root 
transformation is best:  
> modt <- 1m(sqrt(Species) ~ . , gala) 
> 
plot(predict(modt),residuals(modt),xlab=""Fitted"",ylab=""
Residuals"") 
We now see in the right panel of Figure 3.1 that the nonconstant variance problem has been cleared up. Let’s take a look at the fit: 
> summary(modt) 
Coefficients: 
             Estimate  Std. Error t value Pr(>|t|) 
(Intercept)  3.391924    0.871268    3.89  0.00069 
Area        -0.001972    0.001020   -1.93  0.06508 
Elevation    0.016478    0.002441    6.75  5.5e-07 
Nearest      0.024933    0.047950    0.52  0.60784 
Scruz       -0.013483    0.009798   -1.38  0.18151 
Adjacent    -0.003367    0.000805   -4.18  0.00033 
Residual standard error:  2.77 on 24 degrees of freedom 
Multiple R-Squared: 0.783,       Adjusted R-squared: 
0.737 
F-statistic: 17.3 on 5 and 24  degrees of freedom,  p-
value: 2.87e-07 
We see a fairly good fit ( R2=0.78) considering the nature of the variables. However, we 
achieved this fit at the cost of transforming the response. This makes interpretation more 
difficult. Furthermore, some of the response values are quite small (single digits) which makes us question the validity of the normal approximation. This model may be 
adequate, but perhaps we can do better. We develop a Poisson regression model: 
Suppose we have count responses Y
i that we wish to model in terms of a vector of 
predictors xi. Now if Yi~Pois(µi), we need some way to link the µi to the xi. We use a 
linear combination of the xi to form the linear predictor 
 Since we require that 
µi≥0, we can ensure this by using a log link function, that is: 
   
So, as with the binomial regression models of the previous chapter, this models also has a 
linear predictor and a link function. 
Now, the log-likelihood is: 
   Count regression     63",0,False,0.23353666,True,0.37096483,False,0.15935585,False,0.2698277,False,0.1193235
"confidence intervals for β using the standard errors,
  although, as before, it is better 
to use profile likelihood methods. 
An alternative and perhaps better-known go odness of fit measur e is the Pearson’s X2 
statistic: 
   
In this example, we see that the residual deviance is 717 on 24 degrees of freedom which 
indicates an ill-fitting model if the Poisson is the correct model for the response. We 
check the residuals to see if the large deviance can be explained by an outlier:  
> halfnorm(residuals(modp)) 
 
Figure 3.2 Half-normal plot of the 
residuals of the Poisson model is 
shown on the left and the relationship between the mean and variance is shown on the right. A line representing 
mean equal to variance is also shown.  
The half-normal plot of the residuals shown in Figure 3.2 shows no outliers. It could be 
that the structural form of the model needs some improvement, but some experimentation 
with different forms for the predictors will reveal that there is little scope for 
improvement. Furthermore, the proportion of deviance explained by this model, 1-
717/3510=0.796, is about the same as in the linear model above. 
For a Poisson distribution, the mean is equal to the variance. Let’s investigate this 
relationship for this model. It is difficult to estimate the variance for a given value of the 
mean, but 
 does serve as a crude approximation. We plot this estimated variance 
against the mean, as seen in the second panel of Figure 3.2: Count regression     65",1,True,0.36656952,True,0.38661635,False,0.28705424,False,0.2584257,True,0.3549354
"> plot(log(fitted(modp)),log((gala$Species-
fitted(modp))^2), 
  xlab=expression(hat(mu)),ylab=expression((y-
hat(mu))^2)) 
> abline(0,1) 
We see that the variance is proportional to, but larger than, the mean. When the variance 
assumption of the Poisson regression model is broken but the link function and choice of 
predictors is correct , the estimates of β are consistent, but the standard errors will be 
wrong. We cannot determine which predictors are statistically significant in the above 
model using the output we have. 
The Poisson distribution has only one parameter and so is not very flexible for 
empirical fitting purposes. We can generalize by allowing ourselves a dispersion 
parameter. Over- or underdispersion can occur in Poisson models. For example, suppose 
the Poisson response Y has rate λ which is itself a random variable. The tendency to fail 
for a machine may vary from unit to unit even  though they are the same model. We can 
model this by letting λ be gamma distributed with Eλ=µ and var 
 Now Y is 
negative binomial with mean EY=µ. The mean is the same as the Poisson, but the 
variance var 
 which is not equal to µ. In this case, overdispersion would 
occur. 
If we know the specific mechanism, as in  the above example, we could model the 
response as a negative binomial or other more flexible distribution. If the mechanism is 
not known, we can introduce a dispersion parameter 
 such that var 
is the regular Poisson regression case, while 
 is overdispersion and 
 is 
underdispersion. 
The dispersion parameter may be estimated using: 
   
We estimate the dispersion parameter in our example by: 
> (dp <- 
sum(residuals(modp,type=""pearson"")^2)/modp$df.res) 
[1] 31.749 
We can then adjust the standard errors and so forth in the summary as follows: 
> summary (modp, dispersion=dp) 
Coefficients: 
             Estimate Std.   Error z value Pr(>|z|) 
(Intercept)  3.154808     0.291590   10.82  < 2e-16 
Area        −0.000580     0.000148   -3.92  8.9e-05 
Elevation    0.003541     0.000493    7.19  6.5e-13 
Nearest      0.008826     0.010262    0.86     0.39 
Scruz       -0.005709     0.003525   -1.62     0.11 
Adjacent    -0.000663     0.000165   -4.01  6.0e-05 Extending the linear model with R     66",1,True,0.31843746,True,0.3251868,True,0.30026788,False,0.23980726,True,0.30298448
"(Dispersion parameter   for poisson family taken to be 
31.749) 
    Null deviance: 3510.73    on 29  degrees of freedom 
Residual deviance:  716.85    on 24  degrees of freedom 
AIC: 889.7 
Notice that the estimation of the dispersion and the regression parameters is independent, 
so choosing a dispersion other than one has no effect on the regression parameter 
estimates. Notice also that ther e is some similarity in whic h variables are picked out as 
significant and which not when compared with the linear regression model. 
When comparing Poisson models with overdispersion, an F-test rather than a χ2 test 
should be used. As in normal linear models, the variance, or dispersion parameter in this 
case, needs to be estimated. Th is requires the use of the F-test. So to test the significance 
of each of the predictors relative to the full model, use: 
> dropl(modp,test=""F"") 
Single term deletions 
Model: 
Species ~ Area + Elevation + Nearest + Scruz + Adjacent 
          Df Deviance  AIC F value   Pr(F) 
<none>            717  890  
Area       1     1204 1375   16.32 0.00048 
Elevation  1     2390 2560   56.00   1e-07 
Nearest    1      739  910    0.76 0.39336 
Scruz      1      814  984    3.24 0.08444 
Adjacent   1     1341 1512   20.91 0.00012 
Warning message: 
F test assumes quasipoisson family in: dropl.glm(modp, 
test=""F"") 
The z-statistics from the summary () are less reliable and so the F-test is preferred. In this 
example, there is little practical difference between the two. 
3.2 Rate Models  
The number of events observed may depend on a size variable that determines the 
number of opportunities for the events to occur. For example, if we record the number of 
burglaries reported in different cities, the ob served number will depend on the number of 
households in these cities. In other cases, the size variable may be time. For example, if we record the number of customers served by a sales worker, we must take account of the 
differing amounts of time worked. 
Sometimes, it is possible to analyze such data using a binomial response model. For 
the burglary example above, we might model the number of burglaries out of the number 
of households. However, if the proportional is small, the Poisson approximation to the 
binomial is effective. Furtherm ore, in some examples, the total number of potential cases 
may not be known exactly. The modeling of rare diseases illustrates this issue as we may 
know the number of cases but not have precise population data. Sometimes, the binomial Count regression     67",2,False,0.25114805,False,0.24705888,False,0.16018791,False,0.1504261,True,0.36611125
"model simply cannot be used. In the burglary  example, some households may be affected 
more than once. In the customer service exam ple, the size variable is not a count. An 
alternative approach is to model the ratio . However, there are often difficulties with 
normality and unequal variance when taking this approach, particularly if the counts are small. 
In Purott and Reeder (1976), some data is presented from an experiment conducted to 
determine the effect of gamma radiation on the numbers of chromosomal abnormalities (ca) observed. The number (cells ), in hundreds of cells expo sed in each run, differs. The 
dose amount (doseamt) and the rate (doserate) at which the dose is applied are the 
predictors of interest. We may format the data for observation like this: 
> data(dicentric) 
> round(xtabs(ca/cells ~ doseamt+doserate, 
dicentric),2) 
       doserate 
doseamt  0.1 0.25  0.5    1  1.5    2  2.5    3    4 
    1   0.05 0.05 0.07 0.07 0.06 0.07 0.07 0.07 0.07 
    2.5 0.16 0.28 0.29 0.32 0.38 0.41 0.41 0.37 0.44 
    5   0.48 0.82 0.90 0.88 1.23 1.32 1.34 1.24 1.43 
We can plot the data as seen in the first panel of Figure 3.3: 
> 
with(dicentric,interaction.plot(doseamt,doserate,ca/cel
ls)) 
We might try modeling the rate directly. We see that the effect of the dose rate may be multiplicative, so we log this variable in the following model:  
 
Figure 3.3 Chromosomal 
abnormalities rate response is shown 
on the left and a residuals νs. fitted 
plot of a linear model fit to these data 
is shown on the right.  Extending the linear model with R     68",0,False,0.19655398,False,0.16001804,True,0.40284032,False,0.17931086,False,0.22669819
"> lmod <- lm(ca/cells ~ log(doserate)*factor(doseamt), 
dicentric) 
> summary(lmod)$adj 
[1] 0.98444 
As can be seen from the adjusted R2, this model fits well. However, a look at the 
diagnostics reveals a problem, as seen in the second panel of Figure 3.3: 
> plot(residuals(lmod) ~ 
fitted(lmod),xlab=""Fitted"",ylab=""Residuals"") 
> abline(h=0) 
We might prefer an approach that directly models the count response. We need to use the log of the number of cells because we expect this to have a multiplicative effect on the 
response: 
> dicentric$dosef <- factor(dicentric$doseamt) 
> pmod <- glm(ca ~ log(cells)+log(doserate)*dosef, 
  family=poisson,dicentric) 
> summary(pmod) 
Coefficients: 
                       Estimate Std. Error z value 
Pr(>|z|) 
(Intercept)             -2.7653     0.3812   -
7.25    4e-13 
log(cells)               1.0025     0.0514   19.52   <2
e-16 
log(doserate)            0.0720     0.0355    2.03  0.0
4240 
dosef2.5                 1.6298     0.1027   15.87   <2
e-16 
dosef5                   2.7667     0.1229   22.52   <2
e-16 
log(doserate):dosef2.5   0.1611     0.0484    3.33  0.0
0087 
log(doserate):dosef5     0.1932     0.0430    4.49    7
e-06 
(Dispersion parameter for poisson family taken to be 1) 
    Null deviance: 916.127 on 26 degrees of freedom 
Residual deviance:  21.748 on 20 degrees of freedom 
AIC: 211.2 
We can relate this Poisson model with a log link back to a linear model for the ratio 
response: 
log(ca/cells)= Xβ   
This can be rearranged as 
log ca=log cells+ Xβ   Count regression     69",0,True,0.34052554,True,0.33747637,False,0.2831248,False,0.18320596,False,0.26106396
"We are using log cells as a predictor. Checki ng above, we can see that the coefficient of 
1.0025 is very close to one. This suggests fitting a model with the coefficient fixed as 
one. In this manner, we are modeling the ra te of chromosomal a bnormalities while still 
maintaining the count response for the Poisson model. This is known as a rate model . We 
fix the coefficient as one by using an offset . Such a term on the predictor side of the 
model equation has no parameter attached: 
> rmod <- glm(ca ~ offset 
(log(cells))+log(doserate)*dosef, 
  family=poisson,dicentric) 
> summary(rmod) 
Coefficients: 
                      Estimate Std.   Error  z 
value  Pr(>|z|) 
(Intercept)            -2.7467       0.0343   -
80.16   < 2e-16 
log(doserate)           0.0718       0.0352     2.04   
0.04130 
dosef2.5                1.6254       0.0495    32.86   
< 2e-16 
dosef5                  2.7611       0.0435    63.49   
< 2e-16 
log(doserate):dosef2.5  0.1612       0.0483     3.34   
0.00084 
log(doserate):dosef5    0.1935       0.0424     4.56 
0.0000051 
(Dispersion parameter for poisson family taken to be 1) 
Null deviance:     4753.00  on 26 degrees of freedom 
Residual deviance:   21.75  on 21 degrees of freedom 
AIC: 209.2 
Not surprisingly, the coefficients  are only slightly different from the previous model. We 
see from the residual deviance that the model fits well. Previous analyses have posited a 
quadratic effect in dose; indeed, the observed  coefficients speak against a purely linear 
effect. However, given that we have only three dose levels, we can hardly check whether quadratic is appropriate. Given the significant interaction effect, we can see that the effect 
of the dose rate is different depending on the overall dose. We can see that the 
combination of a high dose, delivered quickly,  has a greater combined effect than the 
main effect estimates would suggest. More on the analysis of this data may be found in 
Frome and DuFrain (1986). 
3.3 Negative Binomial  
Given a series of independent tria ls, each with probability of success p, let Z be the 
number of trials until the k
th success. Then: 
   Extending the linear model with R     70",4,True,0.34016,False,0.2654351,False,0.21065694,False,0.20386925,True,0.30096656
"[1] 1.1307e-10 
The fit is improved but not enough to conclude that the model fits. We could try adding 
more interactions but that would make inte rpretation increasingly difficult. A check for 
outliers reveals no problem. 
An alternative model for counts is the negative binomial. The functions for fitting 
come from the MASS package—see Venables a nd Ripley (2002) for more details. We 
can specify the link parameter k. Here we choose k=1 to demonstrate the method, 
although there is no substantive motivation from this application to use this value:  
> library(MASS) 
> modn <- glm(skips ~ . , negative.binomial(1),solder) 
> modn 
Coefficients: 
(Intercept)     OpeningM       OpeningS   SolderThin   
    MaskA3 
    -
1.6993       0.5085         1.9997       1.0489       0
.6571 
     MaskA6       MaskB3         MaskB6    PadTypeD6   
 PadTypeD7 
     2.5265       1.2726         2.0803      -
0.4612       0.0161 
  PadTypeL4    PadTypeL6      PadTypeL7    PadTypeL8   
 PadTypeL9 
     0.4688      -0.4711        -0.2949      -
0.0849      -0.5213 
  PadTypeW4    PadTypeW9          Panel 
    -0.1425      -1.4836         0.1693 
Degrees of  Freedom: 899 Total  (i.e. Null); 882 
Residual 
Null Deviance:      1740 
Residual Deviance: 559   AIC: 3880 
We could experiment with different values of k, but there is a more direct way of 
achieving this by allowing the parameter k to vary and be estimated using maximum 
likelihood in: 
> modn <- glm.nb(skips ~ .,solder) 
> summary (modn) 
Coefficients: 
            Estimate Std. Error z value Pr(>|z|) 
(Intercept)  -1.4225     0.1427   -9.97  < 2e-16 
OpeningM      0.5029     0.0798    6.31  2.9e-10 
Openings      1.9132     0.0715   26.75  < 2e-16 
SolderThin    0.9393     0.0536   17.52  < 2e-16 
MaskA3        0.5898     0.0965    6.11  9.9e-10 
MaskA6        2.2673     0.1018   22.27  < 2e-16 
MaskB3        1.2110     0.0964   12.57  < 2e-16 
MaskB6        1.9904     0.0922   21.58  < 2e-16 Extending the linear model with R     72",1,True,0.3715918,False,0.20927979,False,0.17503703,False,0.18269041,False,0.19142918
"chcond2 as possible predictor variables. Considering the deviance of this model, 
does this model fit the data? 
(b) Plot the residuals and the fitted values—why are there lines of observations on the 
plot? 
(c) Use backward elimination with a critical  p-value of 5% to reduce the model as 
much as possible. Report your model. 
(d) What sort of person would be predicted to visit the doctor the most under your 
selected model? 
(e) For the last person in the dataset, compute the predicted probability distribution for 
their visits to the doctor, i.e., give the probability they visit 0,1,2, etc. times.  
(f) Fit a comparable (Gaussian) linear m odel and graphically compare the fits. 
Describe how they differ. 
6. Components are attached to an electronic circuit card assembly by a wave-soldering 
process. The soldering process involves baking and preheating the circuit card and 
then passing it through a solder wave by conveyor. Defects aris e during the process. 
The design is 27−3 with three replicates. The data is presented in the dataset 
wavesolder. Assuming that the replicates are independent, analyze the data. Write a 
report on the analysis that summarizes the substantive conclusions and includes the 
highlights of your analysis. 
7. The dataset esdcomp was recorded on 44 doctors working in an emergency service at a 
hospital to study the factor s affecting the number of co mplaints received. Build a 
model for the number of complaints receive d and write a report on your conclusions. Extending the linear model with R     74",1,False,0.21984811,False,0.25941807,True,0.35031486,False,0.22043633,False,0.2417753
"qualitybad   -1.0576     0.1078   -9.81   <2e-16 
(Dispersion parameter for poisson family taken to be 1)  
    Null deviance: 474.10 on 3 degrees of freedom 
Residual deviance:  54.03 on 1 degrees of freedom 
The null model, which suggests all four outcomes occur at the same rate, does not fit 
because the deviance of 474.1 is very larg e for three degrees of freedom. The additive 
model, with a deviance of 54.03 is clearly an improvement over this. We might also want 
to test the significance of the individual predictors. We could use the z-values, but it is 
better to use the likelihood ratio test based on the differences in the deviance (not that it 
matters much for this particular dataset): 
> dropl(modl,test=""Chi"") 
Single term deletions 
Model: 
y ~ particle + quality 
         Df Deviance AIC LRT Pr(Chi) 
<none>            54  84 
particle  1      364 392 310  <2e-16 
quality   1      164 192 110  <2e-16 
We see that both predictors are significant relative to the full model. By examining the 
coefficients, we see that wafers with partic les occur at a significantly higher rate than 
wafers without particles. Similarly, we see th at bad-quality wafers occur at a significantly 
higher rate than good-quality wafers. 
The model coefficients are closely related to the marginal totals in the table. The 
maximum likelihood estimates satisfy: 
   
where the XTy is, in this example: 
> (t(model.matrix(modl)) %*%y)[,] 
(Intercept) particleyes qualitybad 
        450          50        116 
So we see that the fitted values, 
 are a function of marginal totals. This fact is exploited 
in an alternative fitting method known as iterative proportional fitting . The glm function 
in R, however, uses Fisher scoring, described in Section 6.2. In any case, the log-likelihood (ignoring any terms not involving µ) is: 
   
which is maximized to obtain the fit. 
The analysis so far has told us nothing about the relationship between the presence of 
particles and the quality of the wafer. The additive model posits: 
log µ=γ+αi+βj   Extending the linear model with R     78",4,True,0.3030511,False,0.21232644,False,0.25665322,False,0.14909938,True,0.44460267
"The fitted values are then 
 or: 
> (fv <- outer(qp,pp)*450) 
       particle 
quality     no    yes 
good    296.89 37.111  
bad 103.11 12.889  
To test the fit, we compare this model against the saturated model, for which 
 So 
the deviance is: 
   
which computes to: 
> 2*sum(ov*log(ov/fv)) 
[1] 54.03 
which is the same deviance we observed in the Poisson model. So we see that the test for 
independence in the multinomial model coincides with the test for no interaction in the Poisson model. The latter test is easier to execute in R, so we shall usually take that 
approach. 
This connection between the Poisson and multinomial is no surprise due to the 
following result. Let Y
1,…,Yk be independent Poisson random variables with means λ1,…, 
λk, then the joint distribution of Y1,…, Yk|ΣiYi=n is multinomial with probabilities 
pj=λj/Σiλi. 
One alternative to the deviance is the Pearson X2 statistic: 
   
which takes the value: 
> sum( (ov-fv)^2/fv) 
[1] 62.812 
Yates’ continuity correction subtracts 0.5 from 
 when this value is positive and 
adds 0.5 when it is negative. This gives superior results for small samples. This 
correction is implemented in: 
> prop.test(ov) 
        2-sample test for equality of proportions with 
        continuity correction 
data: ov 
X-squared = 60.124, df = 1, p-value = 8.907e-15 Extending the linear model with R     80",4,False,0.27043664,False,0.09386411,False,0.13200384,False,0.10017938,True,0.39041084
"The deviance-based test is preferred to the Pearson’s X2. 
Binomial  
It would also be natural to view the presence of the particle as affecting the quality of 
wafer. We would view the quality as the respon se and the particle status as a predictor. 
We might fix the number of wafers with no pa rticles at 400 and the number with particles 
as 50 and then observe the outcome. We could then use a binomial model for the 
response for both groups. Let’s see what happens: 
> (m <- matrix(y,nrow=2)) 
     [,1] [,2] 
[1,]  320   80 
[2,]   14   36 
> modb <- glm(m ~ 1, family=binomial)  
> deviance(modb) 
[1] 54.03  
We fit the null model which suggests that the probability of the response is the same in both the particle and no particle group. This hypothesis of homogeneity  corresponds 
exactly to the test of independence and the deviance is exactly the same. 
For larger contingency tables, where there are more than two rows (or columns), we 
can use a multinomial model for each row. This model is more accurately called a 
product  multinomial model to distinguish it from the unrestricted multinomial model 
introduced above. 
Hypergeometric  
The remaining case is where both marginal totals  are fixed. This situation is rather less 
common in practice, but does suggest a mo re accurate test for independence. This 
sampling scheme can arise when classifying obj ects into one of two types when the true 
proportions of each type are known in advance. For example, suppose you are given 10 
true or false statements and told that 5 are tr ue and 5 are false. You are asked to sort the 
statements into true and false. We can ge nerate a two-by-two table of the correct 
classification against the observed classification generated. Under the hypergeometric distribution and the assumption of independence, the probability of the observed table is: 
   
If we fix any number in the table, say y11, the remaining three numbers are completely 
determined because the row and column tota ls are known. There is a limited number of 
values which y11 can possibly take and we can compute the probability of all these 
outcomes. Specifically, we can compute the total probability of all outcomes more extreme than the one observ ed. This method is called Fisher’s exact test . We may 
execute it as follows: 
> fisher.test(ov) 
        Fisher's Exact Test for Count Data 
data:  ov Contingency tables     81",4,False,0.14610073,False,0.116393015,False,0.18196306,False,0.045089386,True,0.35566306
"> c(deviance(modi),df.residual(modi)) 
[1] 735 19 
Although the statistics for the two tests are so mewhat different, in either case, we see a 
very large value for the degrees of freedom. We conclude that this model does not fit the data. 
We can show that the coefficients of this model correspond to the marginal 
proportions. For example, co nsider the smoker factor: 
> (coefsmoke <- exp(c(0,coef(modi)[2]))) 
         smokerno 
1.0000     1.2577 
> coefsmoke/sum(coefsmoke) 
         smokerno 
0.44292   0.55708 
We see that these are just the marginal pr oportions for the smokers and nonsmokers in 
the data: 
> prop.table(xtabs(y ~ smoker, femsmoke)) 
smoker  
    yes      no 
0.44292 0.55708 
This just serves to emphasize the point that the main effects of the model just convey information that we already know and is not the main interest of the study. 
Joint Independence:  Let p
ij be the (marginal) probability that the observation falls 
into a (i, j, ·)  cell where any value of the third variable is acceptable. Now suppose that 
the first and second variable are dependent, but jointly independent of the third. Then: 
Pijk=PijPk   
We can represent this as: 
logEY ijk=log n+log pij+logpk   
Using the hierarchy principle, we would also include the ma in effects corresponding to 
the interaction term log  pij. So the log-linear model with just one interaction term 
corresponds to joint independence. The specific interaction term tells us which pair of 
variables is dependent. For example, we fit a model that says age is jointly independent 
smoking and life status: 
> modj <- glm(y ~ smoker*dead + age, femsmoke, 
family=poisson) 
> c(deviance(modj), df.residual(modj)) 
[1] 725.8  18.0 
Although this represents a small improvement over the mutual independence model, the deviance is still very high for the degrees of freedom and it is clear that this model does Contingency tables     93",4,False,0.20482457,False,0.27633476,True,0.30354136,False,0.17094703,True,0.3702297
"not fit the data. There are two other joint independence models that have the two other 
interaction terms. These models also fit badly. 
Conditional Independence:  Let pij|k be the probability that an observation falls in cell 
(i,j,.) given that we know the third variable takes the value k. Now suppose we assert that 
the first and second variables are independent given the value of the third variable, then: 
Pij|k=Pi|kPj|k   
which leads to: 
Pijk=PikPjk|Pk   
This results in the model: 
logEYijk=log n+log pik+ log  pjk–logpk   
Again, using the hierarchy principle, we  would also include the main effects 
corresponding to the interaction terms and we would have model with main effects and 
two interaction terms. The minus for the log pk term is irrelevant. The nature of the 
conditional independence can be determined by observing which of one of the three 
possible two-way interactions does not appear in the model. 
The most plausible conditional independence model for our data is: 
> modc <- glm(y ~ smoker*age + age*dead, femsmoke, 
family=poisson) 
> c(deviance(mode),df.residual(mode)) 
[1] 8.327 7.000 
We see that the deviance is only slightly la rger than the degrees of freedom indicating a 
fairly good fit. This indicates that smoking is independent of life status given age. 
However, bear in mind that we do have some zeroes and other small numbers in the table 
and so there is some doubt as to the accuracy of the χ2 approximation here. It is generally 
better to compare models rather than assess the goodness of fit. 
Uniform Association:  We might consider a model with all three-way interactions: 
log EYijk=log n+log pi+log pj+log pk+log pij+log pik+log Pjk   
The model has no three-way interaction and so it is not saturated. There is no simple 
interpretation in terms of independence. Consider our example: 
> modu <- glm(y ~ (smoker+age+dead)^2, femsmoke, 
family=poisson) 
Now we compute the fitted values and determ ine the odds ratios for each age group based 
on these fitted values: 
> ctf <- xtabs(fitted(modu) ~ smoker+dead+age, 
femsmoke) 
> apply(ctf, 3, function(x) 
(x[1,1]*x[2,2])/(x[1,2]*x[2,1]) ) Extending the linear model with R     94",4,False,0.2562273,False,0.20711315,True,0.3107112,False,0.23547846,False,0.2703144
"18-24  25-34  35-44  45-54  55-64  65-74    75+ 
1.5333 1.5333 1.5333 1.5333 1.5333 1.5333 1.5333 
We see that the odds ratio is the same for every age group. Thus the uniform association 
model asserts that for every level of one vari able, we have the same association for the 
other two variables. 
The information may also be extracted from the coefficients of the fit. Consider the 
log-odds ratio for smoking and life status for a given age group: 
log(EY11kEY22k)/(EY12kEY21k)   
This will be precisely the coefficient for the smoking and life-status term. We extract 
this: 
> exp(coef(modu)['smokernordeadno']) 
smokerno:deadno 
         1.5333 
We see that this is exactly the log-odds ratio we found above. The other interaction terms may be interpreted similarly. 
Model Selection:  Log-linear models are hierarchical, so it makes sense to start with 
the most complex model and see how far it can be reduced. We can use analysis of 
deviance to compare models. We start with the saturated model: 
> modsat <- glm(y ~ smoker*age*dead, femsmoke, 
family=poisson) 
> dropl(modsat,test=""Chi"") 
Single term deletions 
Model: 
y ~ smoker * age * dead 
                Df Deviance   AIC  LRT Pr(Chi) 
<none>              3.0e-10 190.2 
smoker:age:dead  6      2.4 180.6  2.4    0.88 
We see that the three-way interaction term may be dropped. Now we consider dropping the two-way terms:  
> dropl(modu,test=""Chi"") 
Single term deletions 
Model: 
y ~ (smoker + age + dead)^2 
            Df Deviance AIC LRT Pr (Chi) 
<none>                2 181 
smoker:age   6       93 259  90   <2e −16 
smoker:dead  1        8 185   6    0.015 
age:dead     6      632 798 630   <2e −16 
Two of the interaction terms are strongly significant, but the smoker: dead term is only 
just statistically significant. This term corresponds to the test for conditional Contingency tables     95",4,True,0.30105138,False,0.19090396,False,0.2662359,False,0.28107065,True,0.366349
"> pchisq(deviance(nomod),df.residual(nomod),lower=F) 
[1] 0.26961 
However, we can take advantage of the ordinal structure of both variables and define 
some scores. As there seems to be no strong  reason to the contrary, we assign evenly 
spaced scores: one to seven for both PID and educ: 
> partyed$oPID <- unclass(partyed$PID) 
> partyed$oeduc <- unclass(partyed$educ) 
Now fit the linear-by-linear association model and compare to the independence model: 
> ormod <- glm(Freq ~ PID + educ + I (oPID*oeduc), 
partyed, 
  family= poisson) 
> anova(nomod,ormod,test=""Chi"") 
Analysis of Deviance Table 
Model 1: Freq ~ PID + educ 
Model 2: Freq ~ PID + educ + I (oPID * oeduc) 
  Resid. Df Resid. Dev Df Deviance  P(>|Chi|) 
1        36       40.7 
2        35       30.6  1     10.2    0.0014 
We see that there is some evidence of an a ssociation. So we see that using the ordinal 
information gives us more power to detect an association. We can examine 
  
> summary(ormod)$coef[ ′I(oPID * oeduc)',] 
  Estimate Std. Error    z value   Pr(>|z|) 
0.0287446  0.0090617  3.1720850  0.0015135 
We see that 
 is 0.0287. The p-value here can also be  used to test the significance of the 
association although, as a Wald test, it is less reliable than the likelihood ratio test we 
used first. We see that 
 is positive, which, given the way that we have assigned the 
scores, mean that a higher level of educatio n is associated with a greater probability of 
tending to the Republican end of the spectrum. 
Just to check the robustness of the assignment of the scores, it is worth trying some 
different choices. For example, suppose we ch oose scores so that there is more of a 
distinction between Democrats and Indepe ndents as well as Independents and 
Republicans. Our assignment of scores for apid below achieves this. Another idea might be that people who complete high school or less are not different; that those who go to 
college, but do not get a BA degree are not different and that those who get a BA or 
higher are not different. My assignment of scores in aedu achieves this: 
> apid <- c(1,2,5,6,7,10,11) 
> aedu <- c(1,1,1,2,2,3,3) 
> ormoda <- glm(Freq ~ PID + educ + 
I(apid[oPID]*aedu[oeduc]), Contingency tables     99",4,False,0.22359839,False,0.20950177,False,0.27691728,False,0.15848917,True,0.3631445
"partyed, family= poisson) 
> anova(nomod,ormoda,test=""Chi"") 
Analysis of Deviance Table 
Model 1: Freq ~ PID + educ 
Model 2: Freq ~ PID + educ + I(apid[oPID] * 
aedu[oeduc]) 
  Resid. Df Resid. Dev Df Deviance P(>|Chi|) 
1        36       40.7 
2        35       30.9  1      9.8    0.0017 
The numerical outcome is slightly different, but the result is still significant. Some 
experimentation with other plausible choices indicates that we can be fairly confident 
about the association here. 
The association parameter may be interpreted in terms of log-odds. For example, 
consider the log-odds ratio for adjacent entries in both rows and columns: 
   
For evenly spaced scores, these log-odds ratios will all be equal. For our example, where 
the scores are spaced one ap art, the log-odds ratio is γ. To illustrate this point, consider 
the fitted values under the linear-by-linear association model: 
> round(xtabs(predict(ormod,type=""response"") ~ PID + 
educ, partyed),2) 
         educ 
PID       MS    HSdrop HS    Coll  CCdeg BAdeg MAdeg 
  strDem   3.58 13.36  59.22 41.34 18.34 42.46 21.71 
  weakDem  2.92 11.22  51.20 36.78 16.80 40.02 21.06 
  indDem   1.59  6.27  29.45 21.78 10.23 25.09 13.59 
  indind   0.49  2.00   9.65  7.34  3.55  8.96  5.00 
  indRep   1.12  4.71  23.41 18.33  9.13 23.70 13.60 
  weakRep  1.61  6.95  35.59 28.68 14.69 39.28 23.19 
  strRep   1.69  7.49  39.48 32.74 17.26 47.49 28.85 
Now compute log-odds ratio for, say, the lower two-by-two table: 
> log(39.28*28.85/(47.49*23.19)) 
[1] 0.028585 
We see this is, but for rounding, equal to 
  
It is always worth examining the residuals to  check if there is more structure than the 
model suggests. We use the raw response residuals (the unscaled difference between 
observed and expected) because we would lik e to see effects which are large in an 
absolute sense. 
> round(xtabs(residuals(ormod,type=""response"") ~ PID + 
educ, partyed),2) 
         educ Extending the linear model with R     100",4,False,0.21331334,False,0.17774075,False,0.24977481,False,0.12875974,True,0.3998407
"educColl:oPID    0.0044605   0.050603  0.088147 
0.929760 
educCCdeg:oPID  -0.0086994   0.060667 -0.143395 
0.885978 
educBAdeg:oPID   0.0345539   0.048782  0.708330 
0.478740 
The last coefficient, educMAdeg: oPID, is not identifiable and so this may be taken as 
zero. If there was really a monotone trend in the effect of educational level on party 
affiliation, we would expect these coefficients  to be monotone. However, we can see that 
they are not. However, if we compare th is to the linear-by-linear association model: 
> anova(ormod,cmod,test=""Chi"") 
Analysis of Deviance Table 
Model 1: Freq ~ PID + educ + I(oPID * oeduc) 
Model 2: Freq ~ PID + educ + educ:oPID 
  Resid. Df Resid. Dev Df Deviance P(>|Chi|) 
1        35      30.57 
2        30      22.76  5     7.81      0.17 
We see that the simpler linear -by-linear association is preferred to the more complex 
column-effects model. Nevertheless, if the lin ear-by-linear associatio n were a good fit, 
we would expect the observed column-effect coefficients to be ro ughly evenly spaced. 
Looking at these coefficients, we observe that for high school and abov e, the coefficients 
are not significantly different from zero while for the lowest two categ ories, there is some 
difference. This suggests an alternate assignment of scores for education:  
> aedu <- 0(1,1,2,2,2,2,2) 
> ormodb <- glm(Freq ~ PID + educ + I 
(oPID*aedu[oeduc]), 
  partyed, family= poisson) 
> deviance(ormodb) 
[1] 28.451 
> deviance(ormod) 
[1] 30.568 
We see that the deviance of this model is even lower than our original model. This gives 
credence to the view that whether a person finishes high school or not is the determining factor in party affiliation. However, since we used the data itself to assign the scores and 
come up with this hypothesis, we would be tempting fate to then use the data again to test 
this hypothesis. 
The use of scores can be helpful in reducin g the complexity of models for categorical 
data with ordinal variables. It is especially useful in higher dimensional tables where a 
reduction in the number of parameters is par ticularly welcome. The use of scores can also 
sharpen our ability to detect associations. 
Further Reading:  See books by Agresti (2002), Bishop, Fienberg, and Holland 
(1975), Haberman (1977), Le (1998), Leonard (2000), Powers and Xie (2000), Santner and Duffy (1989) and Simonoff (2003). Extending the linear model with R     102",4,False,0.17702219,False,0.14205393,False,0.2920011,False,0.18380995,True,0.31568718
"6. The dataset suicide contains one year of suicide data from the United Kingdom cross-
classified by sex, age and method. 
(a) Determine the most appropriate de pendence model between the variables. 
(b) Collapse the sex and age of the subject in to a single six-level factor containing all 
combinations of sex and age. Conduct a correspondence analysis and give an 
interpretation of the plot. 
(c) Repeat the correspondence  analysis separately for ma les and females. Does this 
analysis reveal anything new compared to  the combined analys is in the previous 
question? 
7. A student newspaper conducted a survey of student opinions about the Vietnam War in 
May 1967. Responses were classified by sex, year in the program and one of four 
opinions. The survey was voluntary. The data may be found in the dataset uncviet. 
(a) Conduct an analysis of the patterns of dependence in the data assuming that all 
variables are nominal. 
(b) Assign scores to the year and opinion and fit an appropriate model. Interpret the 
trends in opinion over the years. Check the sensitivity of your conclusions to the 
assignment of the scores. 
8. The dataset HairEyeColor contains the same data analyzed in this chapter as haireye. 
Repeat the analysis in the text for each sex and make a comparison of the conclusions. 
9. A sample of psychiatry patients were cross-classified by their diagnosis and whether a 
drug treatment was prescribed. The data may be found in drugpsy. Is the chance that 
drugs will be prescribed constant across diagnoses? 
10. The UCBadmissions dataset presents data on applicants to graduate school at 
Berkeley for the six largest departments in  1973 classified by admission and sex. 
(a) Show that this provides an example of Simpson’s paradox. 
(b) Determine the most appropriate de pendence model between the variables. 
(c) Fit a binomial regression with admissions status as the response and show the 
relationship to your model in the previous question. Extending the linear model with R     104",1,False,0.18495303,False,0.27345416,True,0.3255033,False,0.25129217,True,0.3069291
"> predict(mmodi,data.frame(nincome=il)) 
[1] 
Democrat   Democrat   Democrat   Republican  Republican 
[6] Republican Republican 
We may also examine the coef ficients to gain an understanding of the relationship 
between the predictor and the response: 
> summary(mmodi) 
Coefficients: 
            (Intercept)   nincome 
Independent    -1.17493  0.016087 
Republican     -0.95036  0.017665 
Std. Errors: 
             (Intercept)   nincome 
Independent      0.15361 0.0028497 
Republican       0.14169 0.0026525 
Residual Deviance:  1985.4 
AIC: 1993.4 
The intercept terms model the probabilities of the party identification for an income of 
zero. We can see the relations hip from this calculation: 
> cc <- c(0,-1.17493,-0.95036) 
> exp(cc)/sum(exp(cc)) 
[1] 0.58982 0.18216 0.22802 
> predict(mmodi,data.frame(nincome=0),type=""probs"") 
   Democrat Independent Republican 
    0.58982     0.18216    0.22802 
The slope terms represent the log-odds of moving from the baseline category of 
Democrat to Independent and Republican, respectively, for a unit change of $1000 in 
income. We can see more explicitly what this means by predicting probabilities for 
incomes $1000 apart and then computing the log-odds: 
> (pp <- 
predict(mmodi,data.frame(nincome=c(0,1)),type=""probs"")) 
  Democrat Independent Republican 
1  0.58982     0.18216    0.22802 
2  0.58571     0.18382    0.23047 
> log(pp[1,1]*pp[2,2]/(pp[l,2]*pp[2,l])) 
[1] 0.016087 
> log(pp[1,1]*pp[2,3]/(pp[1,3]*pp[2,1])) 
[1] 0.017665 
Log-odds can be difficult to interpret particul arly with many predic tors and interactions. 
Sometimes, computing predicted probabilities for a selected range of predictors can 
provide better intuition. Extending the linear model with R     110",4,False,0.28970802,True,0.3303836,False,0.25240695,False,0.25862962,False,0.2783438
"The effect of income is modeled with an interaction with party affiliation: 
> glmod <- glm(y ~ resp.factor + cat.factor + 
cat.factor:rnincome, 
  family=poisson) 
We find that the deviance is the same as the multinomial model above: 
> deviance(glmod) 
[1] 1985.4 
> deviance(mmodi) 
[1] 1985.4 
The coefficients also correspond: 
> coef(glmod)[c(1,945:949)] 
         (Intercept)           cat.factorl          cat
.factorR 
          -0.5119613            -1.1749375           -
0.9503621 
cat.factorD:rnincome cat.factorI:rnincome 
cat.factorR:rnincome 
          -0.0176645            -
0.0015777                   NA 
> coef(mmodi) 
            (Intercept)  nincome 
Independent    -1.17493 0.016087 
Republican     -0.95036 0.017665 
The parameterization is slightly different  for the Poisson GLM. Because only two 
interaction parameters are identifiable, the la st one, being inestimable, is not estimated. 
This has the effect of making Republicans the reference level rather than Democrats as in 
the multinomial model. We see that the sign of the Republican-Democrat contrast is 
reversed and that we may obtain the Independent-Democrat contrast from the Poisson GLM by computing: 
> 0.016087-0.017665 
[1] -0.001578 
So we may obtain the same results using the Poisson GLM, but the multinom function is more transparent. However, the point is that  the multinomial logit can be view as a GLM-
type model, which allows us to apply all the common methodology developed for GLMs.  
 
 Extending the linear model with R     112",0,False,0.28543037,False,0.18601486,False,0.23601538,False,0.17386203,True,0.32863367
"Figure 5.2 Hierarchical response for 
birth types.  
We start with the binomial model. First we accumulate the number of CNS births and 
plot the data with the response on the logit scale as shown in the first panel of Figure 5.3: 
> cns$CNS <- cns$An+cns$Sp+cns$Other 
> plot(log(CNS/NoCNS) ~ Water, cns, 
pch=as.character(Work)) 
 
Figure 5.3 The first plot shows the 
empirical logits for the proportion of 
CNS births related to water hardness and profession (M=Manual, N=Nonmanual). The second is a half-normal plot of the residuals of the 
chosen model  Extending the linear model with R     114",2,False,0.20271607,True,0.4011095,False,0.24540004,True,0.32825318,False,0.27739877
"We observe that the proportion of CNS births falls with increasing water hardness and is 
higher for manual workers. We observe one observation (manual, Newport) that may be 
an outlier. Notice that the Area is confounded with the Water hardness, so we cannot put 
both these predictors in our model. We try them both and compare: 
> binmodw <- glm(cbind(CNS,NoCNS) ~ Water + Work, cns, 
family=binomial) 
> binmoda <- glm(cbind(CNS,NoCNS) ~ Area + Work, cns, 
family=binomial) 
> anova(binmodw,binmoda,test=""Chi"") 
Analysis of Deviance Table 
Model 1: cbind(CNS, NoCNS) ~ Water + Work 
Model 2: cbind(CNS, NoCNS) ~ Area + Work 
  Resid. Df Resid. Dev Df Deviance P(>|Chi|) 
1       13       12.36 
2        7        3.08  6     9.29     0.16 
One can view this test as a check for linear tr end in the effect of water hardness. We find 
that the simpler model using Water is accepta ble. A check for an interaction effect 
revealed nothing significant although a look at the residuals is worthwhile: 
> halfnorm(residuals(binmodw)) 
In the second plot of Figure 5.3, we s ee an outlier corresponding to Newport manual 
workers. This case deserves closer examinat ion. Finally, a look at the chosen model: 
> summary(binmodw) 
Coefficients: 
               Estimate Std. Error z value Pr(>|z|) 
(Intercept)   -4.432580   0.089789  -49.37   < 2e-16 
Water         -0.003264   0.000968   -3.37   0.00075 
WorkNonManual -0.339058   0.097094   -3.49   0.00048 
(Dispersion parameter for binomial family taken to be 
1) 
    Null deviance: 41.047  on 15 degrees of freedom 
Residual deviance: 12.363  on 13 degrees of freedom 
AIC: 102.5 
The residual deviance is close to the degrees of freedom indicating a reasonable fit to the 
data. We see that since: 
> exp(-0.339058) 
[1] 0.71244 
births to nonmanual workers have a 29% lower chance of CNS malformation. Water hardness ranges from about 40 to 160. So a difference of 120 would decrease the odds of CNS malformation by about 32%. Multinomial data     115",2,False,0.23471247,False,0.22999647,False,0.28148568,False,0.20860691,True,0.37611228
"response. We can then fit this using the polr function from the MASS library described in 
Venables and Ripley (2002): 
> library(MASS) 
> pomod <- polr(sPID ~ age + educ + nincome, nes96) 
The deviance and number of para meters for this model are: 
> c(deviance(pomod), pomod$edf) 
[1] 1984.2 10.0 
which can be compared to the corresponding multinomial logit model: 
> c(deviance (mmod), mmod$edf) 
[1] 1968.3    18.0 
The proportional odds model uses fewer parameters, but does not fit quite as well. 
Typically, the output from the proportional odds model is easier to interpret. We may use 
an AlC-based variable selection method: 
> pomodi <- step(pomod) 
Start:  AIC= 2004.2 
sPID ~ age + educ + nincome 
          Df  AIC 
- educ     6 2003 
<none>       2004 
- age      1 2004 
- nincome  1 2039 
Step:  AIC= 2002.8 
sPID ~ age + nincome 
         Df  AIC 
- age     1 2001 
<none>      2003 
- nincome 1 2047  
Step: AIC= 2001.4 
sPID ~ nincome 
          Df AIC 
<none>      2001 
- nincome 1 2045 
Thus we finish with a model including just income as we did with the earlier multinomial model. We could also use a likelihood ratio test to compare the models: 
> deviance(pomodi)-deviance(pomod) 
[1] 11.151 
> pchisq(11.151,pomod$edf-pomodi$edf,lower=F) 
[1] 0.13217 Multinomial data     119",0,False,0.2634385,False,0.15725908,False,0.17734456,False,0.18623225,True,0.36512995
"Figure 5.5 Solid lines represent an 
income of $0, dotted lines are for 
$50,000 and dashed lines are for $100,000. Probability of being a Democrat is given by the area lying to the left of the leftmost of each pair of lines, while the probability of being a Republican is given by the area to the right of the rightmost of the pair. Independents are represented by the 
area in between.  
The deviance is similar to the logit version of this model, but the coefficients appear to be 
different. However, if we compute the same predictions: 
> dems <- pnorm(0.128-i1*0.008182) 
> demind <- pnorm(0.798-i1*0.008182) 
> cbind(dems,demind-dems,1-demind) 
        dems 
[1,] 0.52494 0.24315 0.23192 
[2,] 0.46624 0.25458 0.27918 
[3,] 0.41463 0.26058 0.32479 
[4,] 0.36446 0.26236 0.37318 
[5,] 0.31651 0.25982 0.42366 
[6,] 0.27147 0.25310 0.47543 
[7,] 0.22739 0.24173 0.53088 
We see that the predicted values are very similar to those seen for the logit. If the 
coefficients are appropriately rescal ed, they are also very similar. 
Proportional Hazards Model:  A concept of hazard  was developed in insurance 
applications. When issuing a life insurance policy, the insurer is interested in the Extending the linear model with R     122",0,False,0.17919216,False,0.17545354,False,0.19703647,False,0.15197471,True,0.37705037
"(b) Use backward elimination to reduce th e model to one where all predictors are 
statistically significant. Give an interpretation of the resulting model. 
(c) For the student with id 99, compute the predicted probabilities of the three possible 
choices. 
2. Data were collected from 39 students in  a University of Chicago MBA class and 
may be found in the dataset happy. 
(a) Build a model for the level of happiness as a function of the other variables. 
(b) Interpret the parameters of your chosen model. 
(c) Predict the happiness distribution for subject whose parents earn $30,000 a year, 
who is lonely, not sexually active and has no job. 
3. A student newspaper conducted a survey of student opinions about the Vietnam War in 
May 1967. Responses were classified by sex, year in the program and one of four 
opinions. The survey was voluntary. The data may be found in the dataset uncviet. 
(a) Treat the opinion as the response and the sex and year as predictors. Build a 
proportional odds model, giving an interpretation to the estimates. 
(b) If you completed the analysis of this same dataset as a question in the previous 
chapter, compare and contrast the results of the two analyses. 
4. The pneumo data gives the number of coal miners classified by radiological 
examination into one of thr ee categories of pneumonoconiosis and by the number of 
years spent working at the coal f ace divided into eight categories. 
(a) Treating the pneumonoconiosis status as  response variable as nominal, build a 
model for predicting the frequency of the three outcomes in terms of length of 
service and use it to predict the outcome fo r a miner with 25 y ears of service.  
(b) Repeat the analysis with the pneumonoconiosis status being treated as ordinal.  
(c) Now treat the response variable as hier archical with top level indicating whether 
the miner has the disease and the second level indicating, given they have the 
disease, whether they have a moderate or severe case.  
(d) Compare the three analyses. 
5. The debt data arise from a large postal survey on the psychology of debt. The 
frequency of credit card use is a three-level factor ranging from never, through 
occasionally to regularly. Build a model for predicting credit card use as a function of 
the other variables. Write a report describing the nature of the effect of the predictors 
on the response. 
6. The National Youth Survey collected a sample of 11–17 year-olds with 117 boys and 
120 girls, asking questions about marijuana usage. The data may be found in pot use. 
This data is actually longitudinal—the same boys and girls are followed for five years. 
However, for the purposes of this question, imagine that the data is cross-sectional, 
that is, a different sample of boys and gi rls are sampled each year. Build a model for 
the different levels of marijuana usage, describing the trend over time and the 
difference between the sexes. Extending the linear model with R     124",2,False,0.22900364,False,0.24480751,True,0.34490493,False,0.2672155,False,0.27901113
"6.2 Fitting a GLM  
The parameters, β, of a GLM can be estimated using maximum likelihood. The log-
likelihood for single observation, where 
 is: 
   
So for independent observations, the log-likelihood will be 
 Sometimes 
we can maximize this analytically a nd find an exact solution for the MLE 
 but the 
Gaussian GLM is the only common case where this is possible. Typically, we must use 
numerical optimization. By applying the Newton-Raphson method with Fisher scoring, 
McCullagh and Nelder (1989) show that the optimization is equivalent to iteratively reweighted least squares (IRWLS).  
The procedure can be understood intuitively by analogy to the procedure for the 
Gaussian linear model Y=Xβ+ ε. Suppose var 
where 
 We would 
use weights wi where 
 Since the weights are a function of 
 an iterative 
fitting procedure would be needed. We might set the weights all equal to one, estimate 
use this to recompute the weights, reestimate 
 and so on until convergence. 
We can use a similar idea to fit a GLM. Roughly speaking, we want to regress g(y) on 
X with weights inversely proportional to var g(y). However, g(y) might not make sense in 
some cases—for example, in the binomial GLM. So we linearize g(y) as follows: Let 
η=g(µ) and µ=EY. Now do a one-step expansion: 
   
and 
   
So the IRWLS procedure would be: 
1. Set initial estimates 
 and 
  
2. Form the “adjusted dependent variable” 
  
3. Form the weights 
  Generalized linear models     129",0,True,0.36905155,False,0.1897198,False,0.11862905,False,0.20477843,False,0.11563593
"4. Reestimate β to get 
  
5. Iterate steps 2–3–4 until convergence. 
Notice that the fitting procedure uses only η=g(µ) and V(µ),  but requires no further 
knowledge of the distribution of y. This point will be important later in Section 7.4. 
Estimates of variance may be obtained from: 
   
which is comparable to the fo rm used in weighted least squares with the exception that 
the weights are now a function of the response for a GLM. 
Let’s implement the procedure explicitly to understand how the fitting algorithm 
works. We use the Bliss data from Section 2.7 to illustrate this. Here is the fit we are trying to match: 
> data(bliss) 
> modl <- glm(cbind(dead,alive) ~ conc, 
family=binomial, bliss) 
> summary(modl) $coef 
            Estimate Std. Error z value   Pr(>|z|) 
(Intercept)  -2.3238    0.41789 -5.5608 2.6854e-08 
conc          1.1619    0.18142  6.4046 1.5077e-10 
For a binomial response, we have:  
   
where the variance is computed with the understanding that y is the proportion not the 
count. We use y for our initial guess for 
 which works here because none of the 
observed proportions are zero or one: 
> y <- bliss$dead/30; mu <- y 
> eta <- logit(mu) 
> z <- eta + (y-mu)/(mu*(1-mu)) 
> w <- 30*mu*(1-mu) 
> lmod <- 1m(z ~ conc, weights=w, bliss) 
> coef(lmod) 
(Intercept)       conc 
    -2.3025     1.1536 
It is interesting how close these initial estimates are to the converged values given above. 
This is not uncommon. Even so, to get a more precise result, iteration is necessary. We do 
five iterations here: 
> for(i in 1:5){ 
+ eta <- lmod$fit 
+ mu <- ilogit(eta) 
+ z <- eta + (y-mu)/(mu*(1-mu)) Extending the linear model with R     130",1,True,0.4499671,False,0.27469748,False,0.20119444,False,0.2500184,False,0.20345324
"6.3 Hypothesis Tests  
When considering the choice of model for some data, we should define the range of 
possibilities. The null model is the smallest model we will entertain while the full or 
saturated  model is the most complex. 
The null model represents the situation where there is no relation between the 
predictors and the response. Usually this means we fit a common mean µ for all y, that is, 
one parameter only. For the Gaussian GLM, this is the model y=µ+ε. For some 
contingency table models, there will be additional parameters that represent row or 
column totals or other such constraints. In these cases, the null model will have more than 
one parameter. 
In the saturated model, the data is explained exactly. Typically, we need to use n 
parameters for n data points. This can often be achieved by fitting a sufficiently high-
order polynomial or by treating the numerical values of quantitative predictors as codes, 
thereby changing them into qualitative predictors. If enough interactions are included, the 
model will be saturated. This model tells us no more than the data itself and is usually 
uninformative. 
A statistical model describes how we partition the data into systematic structure and 
random variation. The null model represents one extreme where the data is represented 
entirely as random variation, while the satu rated or full model represents the data as 
being entirely systematic. 
The full model does give us a measure of how well any model could possibly fit and 
so we might consider the difference betw een the log-likelihood for the full model, 
and that for the model under consideration, 
 expressed as a likelihood 
ratio statistic: 
   
Provided that the observations are independent  and for an exponential family distribution, 
when 
 this simplifies to: 
   
where 
 are the estimates under the full (saturated) model and 
 are the estimates under 
the model of interest. The above can be written simply as 
 where 
 is 
called the deviance and 
 is called the scaled deviance. Deviances for the 
common GLMs are shown in Table 6.2.  
GLM Deviance 
Gaussian 
 
Poisson 
 
Binomial 
 
Gamma 
 Extending the linear model with R     132",4,False,0.26339006,True,0.30007228,False,0.26146805,False,0.19749802,False,0.26830944
"Inverse Gaussian 
 
Table 6.2 For the binomial yi ~ B(m, p i) and µi = 
mpi that is µ is the count  and not proportion in this 
formula. For the Poisson, the deviance is known as 
the G-statistic. The second term  
 is usually 
zero if an intercept term is used in the model . 
Pearson’s X2 statistic: 
   
where 
 is an alternative measure of discrepancy that is sometimes used in 
place of the deviance. 
There are two main types of hypothesis test we shall employ. The goodness of fit test 
simply asks whether the current model fits the data. The other type of test compares two 
nested models where the smaller model represents a linear restriction on the parameters 
of the larger model. The goodness of fit test can be viewed as model comparison test if 
we identify the smaller model with the model of interest and the larger model with the 
full or saturated model. 
For the goodness of fit test, we  use the fact that, under cer tain conditions, provided the 
model is correct, the scaled Deviance and the Pearson’s X2 statistic are both 
asymptotically χ2 with degrees of freedom equal to the number of identifiable parameters. 
For GLMs such as the Gaussian, we usually do not know the value of the dispersion 
parameter, 
 and so this test cannot be used. For the binomial and the Poisson, 
and so the test is practical. However, the accuracy of the asymptotic approximation is 
dubious for smaller datasets. For a binary, th at is a 0-1 response, the approximation is 
worthless. 
For comparing a larger model, Ω, to a smaller nested model, ω the difference in the 
scaled deviances, Dω–DΩ is asymptotically χ2 with degrees of freedom equal to the 
difference in the number of identifiable para meters in the two models. For the Gaussian 
model and other models where the dispersion 
 is usually not known, this test cannot be 
directly used. However, if we insert an estimate of 
 we may compute an F-statistic of the 
form: 
   
where 
 is a good estimate of the dispersion. For the Gaussian model, 
and the resulting F-statistic has an exact F distribution for the null. For 
other GLMs with free dispersion parameters, the statistic is only approximately F 
distributed. Generalized linear models     133",4,True,0.3607282,False,0.16246736,False,0.16423997,False,0.08671249,True,0.4011824
"For every GLM except the Gaussian, an approximate null distribution must be used 
whose accuracy may be in doubt particul arly for smaller samples. However, the 
approximation is better when comparing models than for the goodness of fit statistic. 
Let’s consider the possible tests on the Bliss insect data: 
> summary(modl) 
Coefficients: 
           Estimate Std. Error z value Pr(>|z|) 
(Intercept)  -2.324      0.418   -5.56  2.7e-08 
conc          1.162      0.181    6.40  1.5e-10 
(Dispersion parameter for binomial family taken to be 
1) 
    Null deviance: 64.76327 on 4 degrees of freedom 
Residual deviance:  0.37875 on 3 degrees of freedom 
We are able to make a goodness of fit test by examining the size of the residual deviance compared to its degrees of freedom: 
> 1-pchisq(deviance(modi),df.residual(modi)) 
[1] 0.9446 
where we see the p-value is large indicating no evidence of  a lack of fit. As with lack of 
fit tests for Gaussian linear models, this outco me does not mean that this model is correct 
or that no better models exist. We can also quickly see that the null model would be 
inadequate for the data since the null deviance of 64.7 is very large for four degrees of freedom. 
We can also test for the significance of th e linear concentration term by comparing the 
current model to the null model: 
> anova(modl,test=""Chi"") 
Analysis of Deviance Table 
Model: binomial, link: logit 
Terms added sequentially (first to last) 
     Df Deviance Resid. Df Resid. Dev P(>|Chi|) 
NULL                     4        64.8 
conc  1     64.4         3         0.4    le-15 
We see that the concentration term is clearly significant. We can also fit and test a more 
complex model: 
> mod12 <- glm(cbind(dead, alive) ~ conc+I(conc^2), 
family=binomial,bliss) 
> anova(mod1,mod12,test=""Chi"") 
  Resid. Df Resid. Dev Df Deviance P(>|Chi|) 
1         3      0.379 
2         2      0.195  1    0.183     0.669 Extending the linear model with R     134",4,True,0.36284235,False,0.2056549,False,0.15498362,False,0.12987381,True,0.36434367
"We can see that there is no need for a quadra tic term in the model. The same information 
could be extracted with: 
> anova(mod12,test=""Chi"") 
We may also take a Wald test approach. We may use the standard error of the parameter 
estimates to construct a z-statistic of the form 
 This has an asymptotically normal 
null distribution. For the Bliss data, for the concentration term, we have 
z=1.162/0.181=6.40. Thus the (approximate) p-value for the Wald test of the 
concentration parameter being equal to zero is 1.5e−10 and thus we clearly reject the null 
here. Remember that this is again only an ap proximate test except in the special case of 
the Gaussian GLM where the z-statistic is the t-statistic and has an exact t-distribution. 
The difference of deviances test is preferred to the Wald test due, in part, to the problem 
noted by Hauck and Donner (1977). 
6.4 GLM Diagnostics  
As with standard linear models, it is important to check the adequacy of the 
assumptions that support the GLM. The diagnostic methods for GLMs mirror those used 
for Gaussian linear models. However, some adaptations are necessary and, depending on the type of GLM, not all diagnostic methods will be applicable. 
Residuals:  Residuals represent the difference between the data and the model and are 
essential to explore the adequ acy of the model. In the Gaussian case, the residuals are 
These are called response residuals fo r GLMs, but since the variance of the 
response is not constant for most GLMs, some modification is necessary. We would like 
residuals for GLMs to be defined such that th ey can be used in a similar way as in the 
Gaussian linear model. 
The Pearson residual  is comparable to the standardized residuals used for linear 
models and is defined as: 
   
where V (µ)=b″(θ). These are just a rescaling of 
 Notice that 
 and hence 
the name. Pearson residuals can be skewed for nonnormal responses. 
The deviance residuals  are defined by analogy to P earson residuals. The Pearson 
residual was rP such that 
 so we set the deviance residual as rD such that 
Thus: 
   
For example, in the Poisson: 
   Generalized linear models     135",4,False,0.24658065,False,0.19420509,False,0.10999802,False,0.04889319,True,0.36155367
"Let’s examine the types of residuals available to us using the Bliss data. We can obtain 
the deviance residuals as: 
> residuals(modl) 
[1] -0.451015 0.359696 0.000000 0.064302 -0.204493 
These are the default choice of resi duals. The Pearson residuals are: 
> residuals(modl,""pearson"") 
        1        2        3        4         5 
-0.432523 0.364373 0.000000 0.064147 -0.208107 
which are just slightly different from the de viance residuals. The response residuals are: 
> residuals(modl,""response"")  
         1         2         3         4          5 
-0.0225051 0.0283435 0.0000000 0.0049898 -0.0108282  
which is just the response minus the fitted value: 
> bliss$dead/30 - fitted(modl) 
         1         2         3         4          5 
-0.0225051 0.0283435 0.0000000 0.0049898 -0.0108282 
Finally, the so-called working residuals are: 
> residuals(modl,""working"") 
        1        2        3        4         5 
-0.277088 0.156141 0.000000 0.027488 -0.133320 
> modl$residuals 
        1        2        3        4         5 
-0.277088 0.156141 0.000000 0.027488 -0.133320 
Note that it is important to use the residuals () function to get the deviance residuals 
which are most likely what is needed for diag nostic purposes. Using $residuals gives the 
working residuals which is not usually needed for diagnostics. We can now identify the working residuals as a by-product of the IRWLS fitting procedure: 
> residuals(lmod) 
          1          2           3          4          
 5 
-2.7709e-01 1.5614e-01 -3.8463e-16 2.7488e-02 -1.3332e-
01 
Leverage and influence:  For a linear model, 
 where H is the hat matrix  that 
projects the data onto the fitted values. The leverages hi are given by the diagonal of H 
and represent the potential of the point to infl uence the fit. They ar e solely a function of X Extending the linear model with R     136",1,False,0.20182955,True,0.4642309,False,0.06686599,False,0.17568146,True,0.32785964
"Figure 6.1 Residual νs. fitted plots for 
the Galápagos model. The first uses fitted values in the scale of the response while the second uses fitted values in the scale of the linear predictor. The third plot uses response residuals while the first two use 
deviance residuals.  
There are just a few islands with a large predicted number of species while most 
predicted response values are small. This makes it difficult to see the relationship 
between the residuals and the fitted values because most of the points are compressed on 
the left of the display. Now we try plotting 
  
> plot(residuals(modp) ~ predict(modp,type=""link""), 
  xlab=expression(hat(eta)),ylab=""Deviance residuals"") 
Now the points, shown in the second panel of Figure 6.1, are more evenly spaced in the 
horizontal direction. We are looking for two main features in such a plot. Is there any 
nonlinear relationship between the predicted values and the residuals? If so, this would be an indication of a lack of fit that might be re ctified by a change in the model. For a linear 
model, we might consider a transformation of the response, but this is usually impractical 
for a GLM since it would change the assumed distribution of the response. We might also consider a change to the link function, but often this is undesirable since there a few 
choices of link function that lead to easily interpretable models. It is best if a change in 
the choice of predictors or transformations on these predictors can be made since this involves the least disruption to the GLM. For this particular plot, there is no evidence of 
nonlinearity. 
The variance of the residuals with respect to the fitted values should also be inspected. 
The assumptions of the GLM would require constant variance in the plot and, in this 
case, this appears to be the case. A violation of this assumption would prompt a change in the model. We might consider a change in the variance function V (µ),  but this would Generalized linear models     139",1,False,0.26557973,True,0.5683576,False,0.12335526,False,0.27501264,True,0.32211822
"involve abandoning the Poisson GLM since th is specifies a particular form for the 
variance function. We would need to use a quasi-likelihood GLM described in Section 
7.4. Alternatively, we could employ a different GLM for a count response such as the 
negative binomial. Finally, we might use weights if we could identify some feature of the data that would suggest a suitable choice. 
For all GLMs but the Gaussian, we have a nonconstant variance function. However, 
by using deviance residuals, we have already scaled out the variance function and so, provided the variance function is correct, we do expect to see constant variance in the 
plot. If we use response residuals, that is 
as seen in the third panel of Figure 6.1: 
> plot (residuals (modp,type=""response"") ~ predict 
(modp, type=""link"") , 
  xlab=expression(hat(eta)),ylab=""Response residuals"") 
We see a pattern of increasing variation consistent with the Poisson. 
In some cases, plots of the residuals are not particularly helpful. For a binary response, 
the residual can only take two possible values for given predicted response. This is the 
most extreme situation, but similar discreteness can occur for binomial responses with small group sizes and Poisson responses that ar e small. Plots of residuals in these cases 
tend to show curved lines of points corresponding to the limited number of observed 
responses. Such artifacts can obscure the main  purpose of the plot. Difficulties arise for 
binomial data where the covariate classes have very different sizes. Points on plots may 
represent just a few or a large number of individuals. 
Investigating the nature of the relationship between the predictors and the response is 
another primary objective of diagnostic plots. Even before a model is fit to the data, we 
might simply plot the response against the pr edictors. For the Galápagos data, consider a 
plot of the number of species against the area of the island shown in the first panel of 
Figure 6.2: 
¾ plot (Species ~ Area, gala) 
We see that both variables have skewed dist ributions. We start with a log transformation 
on the predictor as seen in the second panel of Figure 6.2:  
> plot(Species ~ log(Area), gala) 
We see a curvilinear relationship between the predictor and the response. However, the 
default Poisson GLM uses a log link which we need to take into account. To allow for the 
choice of link function, we can plot the linearized response: 
   
as we see in the third panel of Figure 6.2: 
> mu <- predict (modp, type=""response"") 
> z <- predict (modp)+(gala$Species-mu)/mu Extending the linear model with R     140",1,False,0.27104527,True,0.45209187,False,0.21498737,False,0.09625205,True,0.3667545
"> plot(z ~ log(Area), gala,ylab=""Linearized Response"") 
 
 
Figure 6.2 Plots of the number of 
species against area for the Galápagos 
data. The first plot clearly shows a need for transformation, the second shows the advantage of using logged area, while the third shows the value of 
using the linearized response.  
We now see a linear relationship suggesting th at no further transformation of area is 
necessary. Notice that we used the cu rrent model in the computation of z. Some might 
prefer to use an initial guess here to avoi d presuming the choice of model. For this 
dataset, we find that a log transformation of all the predictors is helpful: 
> modpl <- glm(Species ~ log(Area) + log(Elevation) + 
log(Nearest) + 
  log(Scruz+0.1) + log(Adjacent), family=poisson, gala) 
> c(deviance(modp),deviance(modpl)) 
[1] 716.85 359.12 
We see that this results in a substantial reduction in the deviance. 
The disadvantage of simply examining the raw relationship between the response and 
the other predictors is that it fails to take into account the effect of  the other predictors. 
Partial residual plots are used for linear mode ls to make allowance for the effect of the 
other predictors while focusing on the relationship of interest. These can be adapted for 
use in GLMs by plotting 
 versus xj. The interpretation is the same as in the 
linear model case. We compute the partial resi dual plot for the (now logged) area, as 
shown in the first panel of Figure 6.3: Generalized linear models     141",1,False,0.15006599,False,0.28480828,False,0.23795372,False,0.25274104,True,0.30167925
"> mu <- predict (modpl,type=""response"") 
> u <- (gala$Species-mu)/mu + coef(modpl) 
[2]*log(gala$Area) 
> plot(u ~ log(Area), gala,ylab=""Partial Residual"") 
> abline(0,coef(modpl)[2]) 
In this plot, we see no reason for concern. There is no nonlinearity indicating a need to 
transform nor are there any ob vious outliers or influential points. Partial residuals can 
also be obtained from residuals (., type=“partial”) although an offset will be necessary if you want the regression line displayed correctly on the plot. 
One can search for good transformations of the predictors in nongraphical ways. 
Polynomials terms or spline functions of the predictors can be experimented with, but generalized additive models, described in Chapter 12, offer a more direct way to discover 
some good transformations. 
The link function is a fundamental assumption of the GLM. Quite often the choice of 
link function is set by the characteristics of the response, such as positivity, or by ease of 
interpretation, as with logit link for binomial GLMs. It is often difficult to contemplate alternatives. Nevertheless, it is worth checking to see whether the link assumption is not 
grossly wrong. Before doing this, it is important to eliminate other simpler violations of 
the assumptions that are more eas ily rectified such as outliers  
 
Figure 6.3 A partial residual plot for 
log(Area) is shown on the left while a 
diagnostic for the link function is 
shown on the right.  
or transformations of the predictors. After these concerns have been eliminated, one can 
check the link assumption by making a plot of the linearized response z against linear 
predictor 
 An example of this is shown in the second panel of Figure 6.3: 
> z <- predict(modpl)+(gala$Species-mu)/mu 
> plot(z ~ predict(modpl), xlab=""Linear predictor"", 
  ylab=""Linearized Response"") Extending the linear model with R     142",1,False,0.18891212,True,0.49457178,False,0.15056163,False,0.23809417,False,0.20909548
"In this case, we see no indication of a problem. 
An alternative approach to checking the link function is to propose a family of link 
functions of which the current choice is a member. A range of links can then be fit and 
compared to the current choice. The approach is analogous to the Box-Cox method used for linear models. Alternative choices are eas ier to explore within the quasi-likelihood 
framework described in Section 7.4. 
Unusual Points  
We have already described the raw material of  residuals, leverage and influence measures 
that can be used to check for points that do not fit the model or influence the fit unduly. 
Let’s now see how to use graphical methods to examine these quantities. 
The Q-Q plot of the residuals is the standard way to check the normality assumption 
on the errors typically made for a linear model. For a GLM, we do not expect the 
residuals to be normally distributed, but we are still interested in detecting outliers. For 
this purpose, it is better to use a half-normal plot that compares the sorted absolute 
residuals and the quantiles of the half-normal distribution: 
   
The residuals are not expected to be normally distributed, so we are not looking for an 
approximate straight line. We only seek outliers which may be identified as points off the 
trend. A half-normal plot is better for this purpose because in a sense the resolution of the plot is doubled by having all the points in one tail.  
Since we are more specifically interested in outliers, we should plot the jacknife 
residuals. An example for the Galápagos model is shown in the first panel of Figure 6.4: 
> halfnorm(rstudent(modpl)) 
 
Figure 6.4 Half-normal plots of the 
jacknife residuals on the left and the leverages on the right.  Generalized linear models     143",1,False,0.27999744,True,0.47536305,False,0.16313641,False,0.13093632,False,0.25497806
"We see no sign of outliers in the plot. The half-normal plot is also useful for positive-
valued diagnostics such as the leverages and th e Cook statistics. A look at the leverages is 
shown in the second panel of Figure 6.4: 
> gali <- influence(modpl) 
> halfnorm(gali$hat) 
There is some indication that case 25, Santa Cruz island, may have some leverage. The predictor Scruz is the distance from Santa Cruz island which is zero for this case. This 
posed a problem for making the log transformation and explains why we added 0.1 to this variable. However, there is some indication that this inelegant fix may be causing some 
difficulty. 
Moving on to influence, a half-normal plot of the Cook statistics is shown in the first 
panel of Figure 6.5: 
> halfnorm(cooks.distance(modpl)) 
Again we have some indication that Santa Cruz island is influential. We can examine the change in the fitted coefficients. For example, consider the change in the Scruz coefficient as shown in the se cond panel of Figure 6.5: 
> plot(gali$coef[, 5],ylab=""Change in Scruz 
coef"",xlab=""Case no."") 
We see a substantial change for case 25. If we compare the full fit to a model without this 
case, we find: 
> modplr <- glm(Species ~ log(Area) + log(Elevation) + 
log(Nearest) 
  + log(Scruz+0.1) + log(Adjacent), family=poisson, 
gala, subset=-25) 
¾ cbind(coef(modpl),coef(modplr)) 
(Intercept)       3.287941  3.050699 
log(Area)         0.348445  0.334530 
log(Elevation)    0.036421  0.059603 
log(Nearest)     -0.040644 -0.052548 
log(Scruz + 0.1) -0.030045  0.015919 
log(Adjacent)    -0.089014 -0.088516 
We see a sign change for the Scruz coefficient. This is interesting since in the full model, 
the coefficient is more than twice the st andard error way from zero indicating some 
significance. A simple solution is to add a larger amount, say 0.5, to Scruz. 
Other than this user-intro duced anomaly, we find no difficulty. Using our earlier 
discovery of the log transformation, some variable selection and allowing for remaining overdispersion, our final model is: 
 Extending the linear model with R     144",1,False,0.17818575,True,0.30413663,False,0.16894585,False,0.13044557,False,0.25056893
"Figure 6.5 Half-normal plot of the 
Cook statistics is shown on the left and 
an index plot of the change in the 
Scruz coefficient is shown on the right.  
                      [,1]      [,2] 
> modpla <- glm(Species ~ log(Area)+log(Adjacent), 
family=poisson, gala) 
> dp <- 
sum(residuals(modpla,type=""pearson"")^2)/modpla$df.res 
> summary(modpla,dispersion=dp) 
Coefficients: 
              Estimate Std. Error z value Pr(>|z|) 
(Intercept)     3.2767     0.1794   18.26  < 2e-16 
log(Area)       0.3750     0.0326   11.50  < 2e-16 
log(Adjacent)  -0.0957     0.0249   -3.85  0.00012 
(Dispersion parameter for poisson family taken to be 
16.527) 
    Null deviance: 3510.73 on 29 degrees of freedom 
Residual deviance:  395.54 on 27 degrees of freedom 
Notice that the deviance is much lower and the elevation variable is not used when 
compared with our model choice in Section 3.1.  
This example concerned a Poisson GLM. Diag nostics for binomial GLMs are similar, 
but see Pregibon (1981) and Collett (2003) for more details. 
Further Reading:  The canonical book on GLMs is McCullagh and Nelder (1989). 
Other books include Dobson (1990), Lindsey (1997), Myers, Montgomery, and Vining 
(2002), Gill (2001) and Fahrmeir  and Tutz (2001). For a Bayesian perspective, see Dey, 
Ghosh, and Mallick (2000). 
 Generalized linear models     145",2,False,0.19431007,True,0.33850867,False,0.23401308,False,0.14102444,True,0.30464432
"Exercises  
1. Consider the or ings data from Chapter 2.  Suppose that, in spite of all the drawbacks, 
we insist on fitting a model with an identity link, but with the binomial variance. Show 
how this may be done using a quasi family model using the glm function. (You will 
need need to consult the help pages for quasi and glm and in particular you will need 
to set good starting values for beta—if it doesn’t work at the first attempt, try different 
values.) Describe how the fitted model diffe rs from the standard logistic regression 
and give the predicted response at a temperature of 31°F. 
2. Fit the orings data with a binomial response and a logit link as in Chapter 2. 
(a) Construct the appropriate test statistic for testing the effect of the temperature. 
State the appropriate null distribution and give the p-value. 
(b) Generate data under the null distribution for the previous test. Use the rbinom 
function with the average proportion of damaged O-rings. Recompute the test 
statistic and compute the p-value. 
(c) Repeat the process of the previous question 1000 times, saving the test statistic 
each time. Compare the empirical  distribution of these simu lated test statistics with 
the nominal null distribution stated in the first part of this question. Compare the 
critical values for a 5% level test computed using these two methods. 
3. Fit the orings data with a binomial response and a logit link as in Chapter 2. 
(a) Construct the appropriate test statistic for testing the effect of the temperature. 
State the appropriate null distribution and give the p-value. 
(b) Generate a random permutation of the responses using sample and recompute the 
test statistic and compute the p-value. 
(c) Repeat the process of the previous question 1000 times, saving the test statistic 
each time. Compare the empirical  distribution of these permuted data test statistics 
with the nominal null distribution stated in the first part of this question. Compare 
the critical values for a 5% level test computed using these two methods. 
4. Data is generated from the exponential distribution with density f(y)=λexp(−λy) where 
λ, y>0. 
(a) Identify the specific form of 
 and c() for the exponential distribution.  
(b) What is the canonical link and varian ce function for a GLM with a response 
following the exponential distribution? 
(c) Identify a practical difficulty that may arise when using the canonical link in this 
instance. 
(d) When comparing nested models in this case, should an F or χ2 test be used? 
Explain. 
(e) Express the deviance in this case in terms of the responses yi and the fitted values 
 
5. The Conway-Maxwell-Poisson distribution has probability function: 
   Extending the linear model with R     146",1,False,0.24254437,False,0.17559072,False,0.2512285,False,0.2278554,True,0.34187067
"where 
   
Place this in exponential family form, identifying all the relevant components 
necessary for use in a GLM. Generalized linear models     147",0,True,0.33001894,False,0.2255455,False,0.27456975,False,0.25978118,False,0.13945225
"The maximum likelihood estimator and the usual estimator, D/(n–p), are both sensitive to 
unusually small values of the response and ar e not consistent estima tes of the coefficient 
of variation when the gamma distribution assumption does not hold. 
Myers and Montgomery (1997) present data from a step in the manufacturing process 
for semiconductors. Four factors are believed to influence the resistivity of the wafer and 
so a full factorial experiment with two levels of each factor was run. Previous experience 
led to the expectation that resistivity would have a skewed distribution and so the need for transformation was anticipated. We start with a look at the data: 
> data(wafer) 
> summary(wafer) 
x1    x2    x3    x4        resist 
-:8    -:8   -:8   -:8   Min.   :166  
+:8  +:8  +:8  +:8  1st Qu.:201 
                    Median :214 
                    Mean   :229 
                    3rd Qu.:259 
                    Max.   :340  
The application of the Box-Cox method or past experience suggests the use of a log 
transformation on the respons e. We fit the full model and then reduce it using AlC-based 
model selection: 
> llmdl <- lm(log(resist)~ .^2, wafer) 
> rlmdl <- step(llmdl) 
> summary(rlmdl) 
Coefficients: 
            Estimate Std. Error t value Pr(>|t|) 
(Intercept)   5.3111     0.0476  111.53  4.7e-14 
x1+           0.2009     0.0476    4.22  0.00292 
x2+          -0.2107     0.0476   -4.43  0.00221 
x3+           0.4372     0.0673    6.49  0.00019 
x4+           0.0354     0.0476    0.74  0.47892 
x1+:x3+      -0.1562     0.0673   -2.32  0.04896 
x2+:x3+      -0.1782     0.0673   -2.65  0.02941 
x3+:x4+      -0.1830     0.0673   -2.72  0.02635 
Residual standard error: 0.0673 on 8 degrees of freedom 
Multiple R-Squared: 0.947,Adjusted R-squared: 0.901 
F-statistic: 20.5 on 7 and 8 DF, p-value:  0.000165 
We find a model with three two-way interactions, all with x3. 
Now we fit the corresponding gamma GLM and again select the model using the AIC 
criterion. Note that the family must be specified as Gamma rather than gamma to avoid 
confusion with the Γ function. We use the log link to be consistent with the linear model. 
This must be specified as the default is the inverse link: Extending the linear model with R     152",0,False,0.29826325,False,0.2952114,False,0.24743913,False,0.20427045,True,0.32747245
"> gmdl <- glm(resist ~ . ^2, family=Gamma(link=log), 
wafer) 
> rgmdl <- step(gmdl) 
> summary(rgmdl) 
Coefficients: 
            Estimate Std. Error t value Pr(>|t|) 
(Intercept)   5.3120     0.0476  111.68  4.6e-14 
x1+           0.2003     0.0476    4.21  0.00295 
x2+          -0.2110     0.0476   -4.44  0.00218 
x3+           0.4367     0.0673    6.49  0.00019 
x4+           0.0354     0.0476    0.74  0.47836 
x1+:x3+      -0.1555     0.0673   -2.31  0.04957 
x2+:x3+      -0.1763     0.0673   -2.62  0.03064 
x3+:x4+      -0.1819     0.0673   -2.70  0.02687 
(Dispersion parameter for Gamma family taken to be 
0.0045249) 
    Null deviance: 0.697837 on 15 degrees of freedom 
Residual deviance: 0.036266 on  8 degrees of freedom  
AIC: 139.2  
In this case, we see that the coefficients ar e remarkably similar to the linear model with 
the logged response. Even the standard errors are almost identical an d the square root of 
the dispersion corr esponds to the residual standard error of the linear model: 
> sqrt(0.0045249) 
[1] 0.067267 
The maximum likelihood estimate of 
 may be computed using the MASS package: 
> library(MASS) 
> gamma.dispersion(rgmdl) 
[1] 0.0022657 
We see that this gives a substantially smaller estimate, which would suggest smaller 
standard errors. However, it is not consiste nt with our experience with the Gaussian 
linear model in this example. 
In this example, because the value of 
 is large (221), the gamma distribution 
is well approximated by a normal. Similarly, for the logged response linear model, a 
lognormal distribution with a small variance ( σ=0.0673) is also very well approximated 
by a normal. For this reason, there is not much to distinguish these two models. The 
gamma GLM has the advantage of modeling the response directly while the lognormal has the added convenience of working with a standard linear model. 
Let us examine another example where ther e is more distinction between the two 
approaches. In Hallin and Ingenbleek (1983) data on payments for insurance claims for various areas of Sweden in 1977 are presented. The data is further subdivided by mileage 
driven, the bonus from not having made previous claims and the type of car. We have 
information on the number of insured, meas ured in policy-years, within each of these 
groups. Since we expect that the total amount of the claims for a group will be Other GLMs     151",2,True,0.38244498,True,0.33040485,False,0.14958858,False,0.22665757,False,0.2892033
"proportionate to the number of insured, it makes sense to treat the log of the number 
insured as an offset for similar reasons to those in Section 3.2. Attention has been 
restricted to data from Zone 1. After some model selection, a gamma GLM of the 
following form was found: 
> data(motorins) 
> motori <- motorins[motorins$Zone == 1,] 
> gl <- glm(Payment ~ 
offset(log(Insured))+as.numeric(Kilometres)+ 
     Make+Bonus , family=Gamma(link=log), motori) 
> summary(gl) 
Coefficients : 
                       Estimate Std. Error t value 
Pr(>|t|) 
(Intercept)              6.5273     0.1777   36.72  < 
2e-16 
as. numeric (Kilometres) 
0.1201     0.0311    3.85  0.00014 
Make2                    0.4070     0.1782    2.28  0.0
2313 
Make3                    0.1553     0.1796    0.87  0.3
8767 
Make4                   -0.3439     0.1915   -
1.80  0.07355 
Make5                    0.1447     0.1810    0.80  0.4
2473 
Make6                   -0.3456     0.1782   -
1.94  0.05352 
Make7                    0.0614     0.1824    0.34  0.7
3689 
Make8                    0.7504     0.1873    4.01 
0.000079 
Make9                    0.0320     0.1782    0.18  0.8
5778  
Bonus                   -0.2007    0.0215    -9.33  < 
2e-16 
(Dispersion parameter for Gamma family taken to be 
0.55597) 
    Null deviance: 238.97 on 294 degrees of freedom 
Residual deviance: 155.06 on 284 degrees of freedom 
AIC: 7168 
In comparison, the lognormal model, where we have used the glm function for 
compatibility, looks like this: 
> llg <- glm(log(Payment) ~ 
offset(log(Insured))+as.numeric(Kilometres) + 
  Make+Bonus,family=gaussian , motori) 
> summary(11g) 
Coefficients: Extending the linear model with R     154",4,True,0.30394685,False,0.2265014,False,0.2588424,False,0.22069186,False,0.29649174
"Estimate Std. Error t value 
Pr(>|t|) 
(Intercept)              6.51403   0.18634   34.96  < 
2e-16 
as .numeric (Kilometres) 
0.05713   0.03265    1.75   0.0813 
Make2                    0.36387   0.18686    1.95   0.
0525 
Make3                    0.00692   0.18824    0.04   0.
9707 
Make4                   -0.54786   0.20076   -
2.73   0.0067 
Make5                   -0.02179   0.18972   -
0.11   0.9087 
Make6                   -0.45881   0.18686   -
2.46   0.0147 
Make7                   -0.32118   0.19126   -
1.68   0.0942 
Make8                    0.20958   0.19631    1.07   0.
2866 
Make9                    0.12545   0.18686    0.67   0.
5025 
Bonus                   -0.17806   0.02254   -
7.90  6.2e-14 
(Dispersion parameter for gaussian family taken to be 
0.61102) 
    Null deviance: 238.56 on 294 degrees of freedom 
Residual deviance: 173.53 on 284 degrees of freedom 
AIC: 704.6 
Notice that there are now important differen ces between the two models. We see that 
mileage class given by Kilometers is statistically significant in the gamma GLM, but not in the lognormal model. Some of the coeffici ents are quite different. For example, we see 
that for make 8, relative to the reference level of make 1, there are exp(0.7504)=2.1178 
times as much payment when using the ga mma GLM, while the co mparable figure for 
the lognormal model is exp(0.20958)=1.2332. 
These two models are not nested and have different distributions for the response, 
which makes direct comparison problematic. The AIC criterion, which is minus twice the 
maximized likelihood plus twice the number of parameters, has often been used as a way 
to choose between models. Smaller values are preferred. Howeve r, when computing a 
likelihood, it is common practice to discard parts that are not functions of the parameters. 
This has no consequence when models with same distribution for the response are 
compared since the parts discarded will be equal. For responses with different distributions, it is essential that all parts of the likelihood be retained. The large difference 
in AIC for these two models indicate that this precaution was not taken. Nevertheless, we 
note that the null deviance for both models is  almost the same while the residual deviance 
is smaller for the gamma GLM. This improvement relative to the null indicates that the 
gamma GLM should be preferred here. Note th at purely numerical comparisons such as 
this are risky and that some attention to residual diagnostics, scientific context and interpretation is necessary. Other GLMs     153",2,True,0.30570754,False,0.24443512,False,0.1606228,False,0.23491962,False,0.2540843
"We compare the shapes of the distributions for the response using the dispersion 
estimates from the two models, as seen in Figure 7.2: 
> x <- seq(0,5,by=0.05) 
> 
plot(x,dgamma(x,1/0.55597,scale=0.55597),type=""l"",ylab=
"""", 
  xlab="""",yaxs=""i"",ylim=c(0,1)) 
> plot(x,dlnorm(x,meanlog=-0.30551,sdlog=sqrt(0.55597)) 
,type=""l"", 
  ylab="""",xlab="""",yaxs=""i"",ylim=c(0,1)) 
 
Figure 7.2 Gamma density for 
observed shape of 1/0.55597 is shown 
on the left and lognormal density for an observed SD on the log scale  
The means have been set to 
one in both cases.  
We see the greater peakedness of the lognormal indicating more small payments which 
are balanced by more large payments. The lognormal thus has higher kurtosis. 
We may also make predictions from both models. Here is a plausible value of the 
predictors: 
> x0 <- 
data.frame(Make=""1"",Kilometres=1,Bonus=1,Insured=100) 
and here is predicted response for the gamma GLM: 
> predict(gl,new=x0,se=T,type=""response"") 
$fit 
[1] 63061 
$se.fit 
[1] 9711.5 Extending the linear model with R     156",1,False,0.23821926,True,0.33379343,False,0.24127474,False,0.14345998,False,0.2852242
"For the lognormal, we have: 
> predict(11g,new=x0,se=T,type=""response"") 
$fit 
[1] 10.998  
$se.fit 
[1] 0.16145 
so that the corresponding values on the original scale would be: 
> c(exp(10.998),exp(10.998)*0.16145) 
[1] 59754.5  9647.4 
where we have used the delta method to estimate the standard error on the original scale. 
7.2 Inverse Gaussian GLM  
The density of an inverse Gaussian random variable, Y~IG(µ, λ) is: 
f(y|µ,λ) = (λ/2πy3)1/2exp[−λ(y−µ)2/2µ2y] y,µ,λ>0   
The mean is µ and the variance is µ3/λ. The canonical link is η=1/µ2 and the variance 
function is V(µ)=µ3. The deviance is given by: 
   
Plots of the inverse Gaussian density for a range of values of the shape parameter, λ, are 
shown in Figure 7.3: 
> library(SuppDists) 
> x <- seq(0,8,by=0.1) 
> 
plot(x,dinvGauss(x,1,0.5),type=""l"",ylab="""",xlab="""",ylim
=c(0,1.5), 
  xaxs=""i"",yaxs=""i"") 
> 
plot(x,dinvGauss(x,1,1),type=""l"",ylab="""",xlab="""",ylim=c
(0,1.5), 
  xaxs=""i"",yaxs=""i"") 
> 
plot(x,dinvGauss(x,1,5),type=""l"",ylab="""",xlab="""",ylim=c
(0,1.5), Other GLMs     155",1,True,0.3699237,True,0.31738794,False,0.15006433,False,0.17083111,True,0.3251971
"projected   1.1036     0.0614    18.0  2.2e-13 
(Dispersion parameter for inverse.gaussian family taken 
to be 0.00017012) 
    Null deviance:       Inf on 20 degrees of freedom 
Residual deviance: 0.0030616 on 19 degrees of freedom 
> abline(igmod,lty=2) 
We see that there is a clear difference in the estimates of the slope. The fits are shown in 
the first panel of Figure 7.4. We should check the diagnostics on the inverse Gaussian 
GLM: 
> plot(residuals(igmod) ~ 
log(fitted(igmod)),ylab=""Deviance residuals"", 
  xlab=expression(log(hat(mu)))) 
> abline(h=0) 
We see in the second panel of Figure 7.4 that the variance of the residuals is decreasing with error indicating that the inverse Gaussian variance function is too strong for this 
data. We have used 
so that the points are more evenly spread horizontally making 
it easier, in this case, to see the variance relationship. A gamma GLM is a better choice 
here. In Whitmore (1986), a different variance function is used, but we do not pursue this 
here as this would not be a GLM.  
 
Figure 7.4 Projected and actual sales 
are shown for 20 products on the left. 
The linear model fit is shown as a solid line and the inverse Gaussian GLM fit is shown with a dotted line. A residual-fitted plot for the inverse Gaussian 
GLM is shown on the right.  
 Other GLMs     157",1,True,0.32767987,True,0.43632048,False,0.17568418,False,0.17904961,True,0.39606142
"7.3 Joint Modeling of the Mean and Dispersion  
All models we have considered so far have modeled the mean response µ=EY where the 
variance takes a known form: var 
 where the dispersion parameter 
 is the 
variance in the Gaussian model, the squared coefficient of variation in the gamma model 
and one in the binomial and Poisson models. We can generalize a little to allowing 
weights by letting 
 when the weights are known. 
In this section, we are interested in examples where 
 varies with the covariates X. 
This is a particular issue that arises in indu strial experiments. We wish to manufacture an 
item with a target mean or optimized response. We set the predictors to produce items as 
close as possible to the target mean or to op timize the mean. This requires a model for the 
mean. We would also prefer that the variance of the response be small at the chosen value 
of the predictors for production. So we need to model the variance as a function of the 
predictors. 
We take, as an example, an experiment to  determine which recipe will most reliably 
produce the best cake. The data comes fro m Box, Bisgaard, and Fung (1988) and is 
shown in Table 7.1. The objective is to bake a cake reliably no matter how incompetent the cook. For this data, we can see, by examin ation, that a response of 6.8 is possible for 
lower flour and shortening content and higher egg content, if the temperature is on the 
high side and the cooking time on the short side. However, we cannot be sure that the consumer will be able to set the temperat ure and cooking time correctly. Perhaps their 
oven is not correctly calibrated or they are just  incompetent. If they happen to bake the 
cake for longer than the set time, they will produce a cake with a 3.5 rating. They will blame the product and not themselves and not bu y that mix again. If on the other hand we 
produce the mix with high flour and eggs an d low shortening, the worst the customer can 
do is a 5.2 and will do better than that for other combinations of time and temperature. 
Here we need a combination of a high mean with  respect to the design factors, flour, eggs 
and shortening, and a low variance with respect  to the environmental factors, temperature 
and time. In this example, the answer is easily seen by inspection, but usually more 
formal model fitting methods will be needed. 
Joint Model Specification:  We use the standard GLM approach for the mean: 
   
Now the dispersion, 
 is no longer considered fixed. Suppose we find an estimate, di, of 
the dispersion and model it using a gamma GLM: 
   
Notice the connection between the two models. The model for the mean produce the 
response for the model for the dispersion, which in turn produces the weights for the 
mean model. In principle, something other than a gamma GLM could be used for the 
dispersion although since we wish to model a strictly positive, continuous and typically Extending the linear model with R     160",1,False,0.16780558,False,0.1893305,True,0.34161788,False,0.107437596,False,0.16807875
"Drying         2.150      0.262    8.19  2.9e-06 
Material      -3.100      0.262  -11.81  5.8e-08 
Preheating    -0.375      0.262   -1.43     0.18 
Residual standard error: 0.525 on 12 degrees of freedom 
Multiple R-Squared: 0.946,Adjusted R-squared: 0.932 
F-statistic: 69.6 on 3 and 12 DF, p-value:  7.39e-08 
Following a suggestion of Smyth, Huele, and Verbyla (2001), we use the squared 
studentized residuals, 
 as the response in the di spersion with a gamma 
GLM using a log-link and weights of 1– hi. Again, we follow the suggestion of some 
previous authors as to which predictors are important for modeling the dispersion: 
> h <- influence(lmod)$hat 
> d <- residuals(lmod)^2/(1-h) 
> gmod <- glm(d ~ Method+Preheating, 
family=Gamma(link=log), 
  weldstrength,weights=1 −h) 
Now feedback the estimated weights to the linear model: 
> w <- 1/fitted(gmod) 
> lmod <- lm(Strength ~ Drying + Material + Preheating, 
  weldstrength, weights=w) 
We now iterate until convergence, where we find that: 
> summary(lmod) 
Coefficients: 
            Estimate Std. Error t value Pr(>|t|) 
(Intercept)   43.825      0.108  406.83 < 2e-16 
Drying         1.869      0.045   41.53 2.5e-14 
Material      -3.234      0.108  -30.03 1.2e-12 
Preheating    -0.239      0.101   -2.35 0.036 
Residual standard error: 1 on 12 degrees of freedom 
Multiple R-Squared: 0.995,Adjusted R-squared: 0.994 
F-statistic: 877 on 3 and 12 DF, p-value: 2.56e-14 
We note that Preheating is now significant in  contrast to the initial mean model fit. The 
output for the dispersion model is: 
> summary(gmod) 
Coefficients: 
           Estimate Std. Error t value  Pr(>|t|) 
(Intercept)  -3.064      0.356   -8.60 0.0000010 
Material     -3.037      0.413   -7.35 0.0000056  
Preheating     2.904      0.413   7.03  0.0000089 
(Dispersion parameter for Gamma family taken to be 
0.50039) 
    Null deviance: 57.919 on 15 degrees of freedom Extending the linear model with R     162",0,False,0.2958042,True,0.36930403,False,0.23571357,False,0.22807658,False,0.27478826
"Residual deviance: 20.943 on 13 degrees of freedom  
The standard errors are not co rrect in this output and further calculation, described in 
Smyth, Huele, and Verbyla (2001), would be  necessary. This would result in somewhat 
larger standard errors (about twice the size), but the two factors would still be significant. 
7.4 Quasi-Likelihood  
Suppose that we are able to specify the link and variance functions of the model for some 
new type of data, but that we do not have a strong idea about the appropriate 
distributional form for the response. For exam ple, suppose that we specify an identity 
link and constant variance. This would be typical in the standard regression setting. We can use least squares to estimate the regressi on parameters. If we want to do some 
inference, then formally we need to assume  a Gaussian distribution for the errors (or 
equivalently, the response). We know that the inference is fairly robust to nonnormality especially as the sample size gets larger. Th e important part of the model specification is 
the link and variance; the outcome is less sensitive to the distribution of the response. 
The same effect holds for other GLMs. Provided we have a larger sample, the results 
are not sensitive to smaller deviations from the distributional assumptions. The link, 
variance and independence assumptions are far more important. Now suppose that we 
were to specify a link and variance function co mbination that does not correspond to any 
of the standard GLMs. An examination of the fitting procedure for GLMs reveals that 
only the link and variance functions are used and no distributional assumptions are necessary. This opens up new modeling possi bilities because one migh t well be able to 
suggest reasonable link and variance functions but not know a suitable distribution. 
Computation of 
and standard errors is often not enough and some form of inference 
is required. To compute a deviance, we need  a likelihood and to compute a likelihood we 
need a distribution. At this point, we need a suitable substitute for a likelihood that can be 
computed without assuming a distribution. 
Let Yi have mean µi and variance 
 We assume that Yi are independent. We 
define a score, Ui: 
   
Now:  
   
   Other GLMs     161",1,False,0.26180583,True,0.396991,False,0.2622716,False,0.14596765,True,0.39672548
"In the first panel of Figure 7.5, we see that the Asian elephant is quite influential and a fit 
without this case should be considered. In the second panel, we see that a pattern of 
constant variation indicating that our choice of variance function was reasonable. We 
used the Pearson residuals because these explicitly normalize the raw residuals using the variance function making the check more tran sparent. Even so, the deviance residuals 
would have served the same purpose.  
Exercises  
1. The relationship between corn yield (bushels per acre) and nitrogen (pounds per acre) 
fertilizer application were studied in Wisconsin. The data may be found in cornnit. 
(a) Using (Gaussian) linear model methods,  represent the relationship between the 
yield response and the nitrogen predictor. You will need to find appropriate 
transformations for the data. Present a qua ntitative interpretation for the effect of 
nitrogen fertilizer on yield. 
(b) Now develop a GLM for the data that does not (explicitly) transform the response. 
Describe quantitatively the relationship between the response and the predictor and 
compare it to the linear model you found in the previous question. 
2. An experiment was conducted as part of an investigation to combat the effects of 
certain toxic agents. The survival time of rats depended on the type of poison used and 
the treatment applied. The data is found in rats. 
(a) Construct a linear model for the data bearing in mind that some transformation of 
the response may be necessary and that the possibility of interactions needs to be 
considered. Interpret the effects of the poisons. 
(b) Build an inverse Gaussian GLM for this  data. Select an appropriate link function 
and perform diagnostics to verify your choices. Interpret the effects of the poisons. 
3. Components are attached to an electronic circuit card assembly by a wave-soldering 
process. The soldering process involves baking and preheating the circuit card and 
then passing it through a solder wave by conveyor. Defects aris e during the process. 
Design is 27−3 with 3 replicates. The data is f ound in wavesolder. Build a pair of 
models for the mean and disper sion in the number of defect s. Investigate which factors 
are significant in the two models. 
4. Data were collected from 39 students in  a University of Chicago MBA class and 
presented in happy. Happiness was measured on a 10 point scale. The response could 
be viewed as ordinal, but a quasi-likelihood approach is also possible. Build a quasi-
GLM selecting appropriate link and variance  functions. Interpret the effect of the 
predictors. 
5. The leafblotch data shows the percentage leaf area affected by leaf blotch on 10 
varieties of barley at nine different sites. The data comes from Wedderburn (1974). 
The data is analyzed in McCullagh and Nelder (1989), which motivates the following 
questions: 
(a) Fit a quasi-GLM with a logit link and a µ(1– µ) variance function. Construct a 
diagnostic plot that shows that this is not a good choice of variance function. Other GLMs     165",1,False,0.20541486,True,0.40107554,True,0.30381364,False,0.26735926,False,0.2674253
"CHAPTER 8  
Random Effects  
Grouped data arise in almost all areas of statistical application. Sometimes the grouping 
structure is simple, where each case belongs  to single group and there is only one 
grouping factor. More complex datasets have a hierarchical or nested structure or include 
longitudinal or spatial elements. All such data  share the common featur e of correlation of 
observations within the same group and so analyses that assume independence of the 
observations will be inappropriate. The use of random effects is one common and convenient way to model such grouping structure. 
A fixed effect  is an unknown constant that we try to estimate from the data. Fixed 
effect parameters are commonly used in linear  and generalized linear models as we have 
presented them earlier in this book. In contrast, a random effect  is a random variable. It 
does not make sense to estimate a random effect; instead, we try to estimate the parameters that describe the distribution of this random effect. 
Consider an experiment to investigate the effect of several drug treatments on a 
sample of patients. Typically, we are intere sted in specific drug treatments and so we 
would treat the drug effects as fixed. However, it makes most sense to treat the patient 
effects as random. It is often reasonable to treat the patients as being randomly selected 
from a larger collection of patients whos e characteristics we would like estimate. 
Furthermore, we are not particularly interested  in these specific patients, but in the whole 
population of patients. A random effects approach to modeling effects is more ambitious in the sense that it attempts to say something about the wider population beyond the 
particular sample. Blocking factors can often be viewed as random effects, because these 
often arise as a random sample of those blocks potentially available. 
There is some judgment required in deciding when to use fixed and when to use 
random effects. Sometimes the choice is clear, but in other cases, reasonable statisticians 
may differ. In some analyses, random eff ects are used simply to induce a certain 
correlation structure in the data and there is sense in which the chosen levels represent a 
sample from a population. Gelman (2005) remarks on the variety of definitions for random effects and proposes a particular straightforward solution to the dilemma of 
whether to use fixed or random effects—he recommends always using random effects. 
A mixed effects  model has both fixed and random effects. A simple example of such a 
model would be a two-way analysis of variance (ANOVA): 
y
ijk=µ+τi+νj+εijk   
where the µ and τi are fixed effects and the error, εijk and the random effects νj are 
independent and identically distributed N(0, σ2) and 
 respectively.  
We would want to estimate the τi and test the hypothesis 
 while we 
would estimate 
 and test 
 Notice the difference: we need to estimate and",2,False,0.03500156,False,0.13202049,True,0.60263443,False,0.17473395,False,0.053151667
"test several fixed effect parameters while we need only estimate and test a single random 
effect parameter. 
In the following sections, we consider estimation and inference for mixed-effect 
models and then illustrate the application to several common designs. 
8.1 Estimation  
This is not as simple as it was for fixed effects models, where least squares is an easily 
applied method with many good properties. Let’s start with the simplest possible random effects model: a one-way ANOVA design with a factor at a levels: 
   
where the αs and εs have mean zero, but variances 
 and 
 respectively. These 
variances are known as the variance components. Notice that this induces a correlation 
between observations at the same level equal to: 
   
This is known as the intraclass correlation coefficient  (ICC). In the limiting case when 
there is no variation between the levels, σα=0 so then ρ=0. Alternatively, when the 
variation between the levels is much larger than that within the levels, the value of ρ will 
approach 1. This illustrates how random  effects generate correlations between 
observations. 
For simplicity, let there be an equal number of observations n per level. We can 
decompose the variation as follows: 
   
or SST=SSE+SSA. SSE is the residual sum of squares, SST is the total sum of squares 
(corrected for the mean) and SSA is the sum of squares due to a. These quantities are 
often displayed in an ANOVA table along with  the degrees of freedom associated with 
each sum of squares. Dividing  through by the respective degrees of freedom, we obtain 
the mean squares, MSE and MSA. Now we find that: 
   
which suggests using the estimators: 
   
This method of estimating variance components can be used for more complex designs. The ANOVA table is constructed, the expected  mean squares calculated and the variance 
components obtained by solving the resulting equations. These estimators are known as Extending the linear model with R     170",4,False,0.052416027,False,0.15913926,True,0.47173852,False,0.076943755,False,0.103467345
"so that the log-likelihood for the data is: 
   
This can be optimized to find maximum likelihood estimates of β, σ2 and D. This is 
straightforward in principle, but there may be difficulties in practice. More complex 
models involving larger numbers of random effects parameters can be difficult to estimate. One particular problem is that the variance cannot be negative so the MLE for 
the variance might be zero. This causes difficulties in the optimization when the 
unrestricted MLE has a maximum that is negative. This forces us to set that variance estimate to be zero where the derivative of the likelihood will not be zero. This 
complicates the numerical calculation. 
Standard errors can obtained using the usual large sample theory for maximum 
likelihood estimates. The variance can be es timated using the inverse of the negative 
second derivative of the log-likelihood evaluated at the MLE. 
MLEs have some drawbacks. One particular  problem is that they are biased. For 
example, consider an i.i.d. sample of normal data x
1,…, xn then the MLE is: 
   
A denominator of  n–1 is needed for an unbiased esti mator. Similar problems occur with 
the estimation of variance components. Given that the number of levels of a factor may 
not be large, the bias of the MLE of the va riance component associat ed with that factor 
may be quite large. Restricted maximum likelihood  (REML) estimators are an attempt to 
get round this problem. The idea is to take a linear combination of the response, k, such 
that kTX=0. We then have: 
kTy~N(0,KTVK)   
We can then proceed to maxi mize the likelihood based on kTy which does not involve any 
of the fixed effect parameters. Once the random  effect parameters have been estimated, it 
is simple enough to obtain the fixed effect parameter estimates. REML generally produces less biased estimates. For balanced data, the REML estimates are usually the 
same as the ANOVA estimates. 
We illustrate the fitting methods using some da ta from an experiment to test the paper 
brightness depending on a shift operator described in Sheldon (1960). We start with a 
fixed effects one-way ANOVA: 
> data(pulp) 
> op <- options (contrasts=c(""contr.sum"", 
""contr.poly"")) 
> lmod <- aov(bright ~ operator, pulp) 
> summary(lmod) 
            Df Sum Sq Mean Sq F value Pr(>F) 
operator     3  1.340   0.447     4.2  0.023 
Residuals   16  1.700   0.106 
> coef(lmod) Extending the linear model with R     172",1,True,0.3553881,False,0.21851493,False,0.29879394,False,0.18008235,False,0.27620235
"(Intercept)   operatorl   operator2   operators 
      60.40       -0.16       -0.34        0.22 
> options(op) 
We have specified sum contrasts here instead of the default treatment contrasts to make 
the later connection to the corresponding rand om effects clearer. The aov function is just 
a wrapper for the standard 1m function that  produces results more appropriate for 
ANOVA models. We see that the opera tor effect is significant with a p-value of 0.023. 
The estimate of σ2 is 0.106 and the estimated overall mean is 60.4. For sum contrasts, 
Σαi=0, so we can calculate the effect for the fourth operator as 0.16+0.34–0.22=0.28. 
Turning to the random effects model, we can compute the variance of the operator 
effects, 
 using the formula above as: 
> (0.447-0.106)/5 
[1] 0.0682 
Now we demonstrate the maximum likelihood estimators. The original R package for 
fitting mixed effects models was nlme as described in Pinheiro and Bates (2000). More 
recently, Bates (2005) introduced  an improved version with the package lme4 which we 
shall use here: 
> library(Ime4) 
> mmod <- lmer(bright ~ 1+(1|operator), pulp) 
> summary(mmod) 
Linear mixed-effects model fit by REML 
Formula: bright ~  1 + (1 | operator) 
   Data: pulp 
    AIC    BIG  logLik deviance REMLdeviance 
24.626 27.613 -9.3131   16.637       18.626 
Random effects: 
  Groups  Name        Variance Std.Dev. 
  operator(Intercept)   0.0681    0.261 
  Residual              0.1062    0.326 
# of obs: 20, groups:   operator,  4 
Fixed effects: 
            Estimate Std. Error DF t value Pr(>|t|) 
(Intercept)   60.400      0.149 19     404   <2e −16 
The model has fixed and random effects components. The fixed effect here is just the 
intercept represented by the first 1 in th e model formula. The random effect is 
represented by (1|operator) indicating that the data is grouped by operator and the 1 
indicating that the random effect is consta nt within each group. The parentheses are 
necessary to ensure that expression  is parsed in the correct order. 
The default fitting method is REML. We see th at this gives identical estimates to the 
ANOVA method 
 For unbalanced 
designs, the REML and ANOVA estimators ar e not necessarily identical. The standard Random effects     173",2,False,0.17866316,False,0.097355165,True,0.38830292,False,0.07166856,False,0.16796052
"deviations are simply the square roots of the variances and not estimates of the 
uncertainty in the variances. 
The maximum likelihood estimates may also be computed: 
> smod <- lmer(bright ~ 1+(1|operator), pulp, 
method=""ML"") 
> summary(smod) 
Linear mixed-effects model fit by maximum likelihood  
Formula: bright ~ 1 + (1 | operator) 
   Data: pulp 
    AIC    BIG  logLik deviance REMLdeviance 
22.512 25.499 -8.2558   16.512       18.738 
Random effects: 
Groups   Name         Variance Std.Dev. 
operator (Intercept)   0.0482   0.219 
Residual               0.1118   0.334 
# of obs: 20, groups:  operator, 4 
Fixed effects: 
            Estimate Std.  Error DF t value Pr(>|t|) 
(Intercept)   60.400       0.129 19     467   <2e −16 
As can be seen, the between-subjects variance,  0.0482, is smaller than with the REML 
method. Because the total variance is partitione d, this means the withinsubjects variance, 
0.1118, is larger than before. 
8.2 Inference  
Using standard likelihood theory, we may derive a test to compare two nested 
hypotheses, H0 and H1, by computing the likelihood ratio test statistic: 
   
where 
 are the MLEs of the parameters under the null hypothesis and 
are the MLEs of the parameters under the alternative hypothesis. 
The null distribution of this test statistic is approximately chi-squared with degrees of 
freedom equal to difference in the dimensions of the two parameters spaces (the 
difference in the number of parameters  when the models are identifiable). 
Unfortunately, this test is only approximate and requires several assumptions—see a 
text such as Cox and Hinkley (1974) for more details. One crucial assumption is that the 
parameters under the null are not on the boundary of the parameter space. Since we are often interested in testing hypotheses about the random effects that take the form 
this a real concern. Furthermore, even if the conditions are met, the χ2 
approximation is sometimes poor. 
Testing the fixed effects:  If you plan to use the likelihood ratio test to compare two 
nested models that differ only in their fixed effects, you cannot use the REML estimation 
method. The reason is that REML estimates the random effects by considering linear Extending the linear model with R     174",2,False,0.23807292,False,0.14608228,True,0.32647026,False,0.12432746,False,0.28755343
"combinations of the data that remove the fixe d effects. If these fixed effects are changed, 
the likelihoods of the two models will not be directly comparable. Use ordinary 
maximum likelihood in this situation if you also wish to use the likelihood ratio test. 
The p-values generated by the likelihood ratio test for fixed effects are approximate 
and unfortunately tend to be too small, thereby sometimes overstating the importance of 
some effects. We may use bootstrap methods to find more accurate p-values for the 
likelihood ratio test. The usual bootstrap approach is nonparametric in that no distribution is assumed. Since we are willing to assume  normality for the er-rors and the random 
effects, we can use a technique called the parametric bootstrap . We generate data under 
the null model using the fitted parameter estimates. We compute the likelihood ratio statistic for this generated data. We repeat this many times and use this to judge the 
significance of the observed test statistic. This approach will be demonstrated below. 
An alternative approach is to condition on the estimated values of the random effect 
parameters and then use standard F- or t-tests. This assumes that the covariance of the 
random part of the model, D, is equal to its estimated value and proceeds as one would 
for generalized least squares. 
Testing the random effects:  In most cases, a test of random effects will involve a 
hypothesis of the form H
0:σ2 = 0. The standard derivation of the asymptotic χ2 
distribution for the likelihood ratio statistic depends on the null hypothesis lying in the 
interior of the parameter space. This assumption is broken when we test if a variance is 
zero. The null distribution in these circumstances is unknown in general and we must resort to numerical methods if we wish for precise testing. If you do use the χ
2 
distribution with the usual degrees of freedom, then the test will tend to be 
conservative—the p-values will tend to be larger than they should be. This means that if 
you observe a significant effect using the χ2 approximation, you can be fairly confident 
that it is actually significant. Small, but not significant, p-values might spur one to use 
more accurate, but time-cons uming, bootstrap methods. 
Expected mean squares:  Another method of hypothesis testing is based on the sums 
of squares found in the ANOVA decompositions. These tests are sometimes more 
powerful than their likelihood ratio test equivalents. However, the correct derivation of these tests usually requires extensive tedious algebra that must be recalculated for each 
type of model. Furthermore, the tests cannot be used (at least without complex and 
unsatisfactory adjustments) when the experiment is unbalanced. 
Now let’s demonstrate these methods on the pulp data. The fixed effect analysis shows 
that the operator effects are statistically significant with a p-value of 0.023. A random 
effects analysis using the expected mean squares approach yields exactly the same F-
statistic for the one-way ANOVA. 
We can also employ the likelihood ratio approach. Because we are testing the random 
effects, we can use either ML or REML. Fo r fixed effects, we must use ML. In this 
example, the only fixed effect is the mean and there is no interest in testing that. We first 
fit the null model: 
> nullmod <- lm (bright ~ 1, pulp) Random effects     175",2,False,0.25873858,False,0.24079266,True,0.392397,False,0.084905714,False,0.27155086
"As there are no random effects in this model, we must use 1m. For models of the same 
class, we could use anova to compute the LRT and its p-value. Here, we need to compute 
this directly: 
> as.numeric(2*(logLik(smod)-logLik(nullmod))) 
[1] 2.5684 
> pchisq(2.5684,1,lower=FALSE) 
[1] 0.10902 
The p-value is now well above the 5% significance level. We have used the MLE here—
using REML produces a slightly different result, but still above 5%. We cannot say that 
this result is necessarily wrong, but the use of the χ2 approximation does cause us to 
doubt the result. 
We can use the parametric bootstrap ap proach to obtain a more accurate p-value. We 
need to estimate the probability, given that the null hypothesis is true, of observing an LRT of 2.5684 or greater. Under the null hypothesis, y~N(µ,σ
2). A simulation approach 
would generate data under this model, fit the null and alternative models and then 
compute the LRT. The process would be re peated a large number of times and the 
proportion of LRTs exceeding th e observed value of 2.5684 would be used to estimate 
the p-value. In practice, we do no t know the true values of µ and a, but we can use the 
estimated values; this distinguishes the parametric bootstrap from the simulation 
approach. We can simulate responses under the null: under the null: 
> y <- simulate(nullmod) 
Now taking the data we generate, we fit both the null and alternative models and then 
compute the LRT. We repeat  the process 1000 times: 
> lrstat <- numeric(1000) 
> for(i in 1:1000){ 
  y <- unlist(simulate(nullmod)) 
  bnull <- 1m(y ~ 1) 
  balt <- lmer(y 1 + (1|operator),pulp,method=""ML"") 
  lrstat[i] <- as.numeric(2*(logLik(bait)-
logLik(bnull))) 
  } 
We may examine the distribution of the bootstrapped LRTs. We compute the proportion 
that are close to zero: 
> mean(lrstat < 0.00001) 
[1] 0.7 
The LRT clearly does not have a χ2 distribution. There is some  discussion of this matter 
in Stram and Lee (1994), who propose a 50:50 mixture of a χ2 and a mass at zero. 
Unfortunately, as we can see, the relative pr oportions of these two components vary from 
case to case. Crainiceanu and Ruppert (2004) gi ve a more complete solution to the one-Extending the linear model with R     176",3,True,0.40281558,False,0.20318282,True,0.3488058,False,0.20740366,True,0.37779188
"way ANOVA problem, but there is no general and exact result for this and more complex 
problems. The parametric bootstrap may be the simplest approach. The method we have 
used above is transparent and could be comput ed much more efficiently if speed is an 
issue. 
Our estimate p-value is: 
> mean(lrstat > 2.5684) 
[1] 0.02 
We should compute the standard error for this estimate by: 
> sqrt(0.02*0.98/1000) 
[1] 0.0044272 
So we can be fairly sure it is under 5%. If in  doubt, do some more replications to make 
sure; this only costs computer time. As it happens, this p-value is close to the fixed 
effects p-value. 
In this example, the random and fixed effect tests gave similar outcomes. However, 
the hypotheses in random and fixed effects are intrinsically different. To generalize 
somewhat, it is easier to conclude there is an  effect in a fixed effects model since the 
conclusion applies only to the levels of the factor used in the experiment, while for 
random effects, the conclusion extends to levels of the factor not considered. Since the 
range of the random effect conclusions is greater, the evidence necessarily has to be 
stronger. 
8.3 Predicting Random Effects  
In a fixed effects model, the effects are repr esented by parameters and it makes sense to 
estimate them. For example, in the one-way ANOVA model: 
yij=µ+αi+εij   
We can calculate 
 We do need to resolve the identifiability problem with the αs and the 
µ, but once we decide on this, the meaning of the 
 is clear enough. We can then 
proceed to make further inference such as multiple comparisons of these levels. 
In a model with random effects, the as are no longer parameters, but random variables. 
Using the standard normal assumption: 
   
It does not makes sense to estimate the α’s because they are random variables. So instead, 
we might think about the expected values. However: 
   
which is clearly not very interesting. If one looks at this from a Bayesian point of view, 
as described in, for example, Gelman, Carlin, Stern, and Rubin (2003), we have a prior Random effects     177",2,False,0.11127591,False,0.10098786,True,0.307927,False,0.052697524,False,0.1837467
"operator, then we can combine this with our fixed effects to produ ce what are known as 
the best linear unbiased predictors  (BLUPs) as follows: 
> fixef(mmod)+ranef(mmod)$operator 
  X. Intercept. 
a        60.278 
b        60.141 
c        60.568 
d        60.613 
This means that we have more than one type of residual depending on what fitted values 
we use. The default predicted values and residuals are from the outermost level of 
grouping. The usual diagnostic plots are still worthwhile: 
> qqnorm(resid(mmod),main="""") 
> 
plot(fitted(mmod),resid(mmod),xlab=""Fitted"",ylab=""Resid
uals"") 
> abline(0,0) 
The plots are shown in Figure 8.1 and indicate no particular problems. Random effects models are particular sensitive to outliers, de pending as they do on variance components, 
which can be substantially inflated by unusual points. The QQ plot is one way to pick out 
outliers. We also need the normality for the testing. The residual-fitted plot is also 
important because we made the assumption that the error variance was constant. 
If we had more than four groups, we could also look at the normality of the group 
level effects and check for constant variance also. With so few groups, it is not possible 
to do this. Also note that there is no point thinking about multiple comparisons. These are 
for comparing selected levels of a factor. Fo r a random effect, the levels were randomly 
selected, so such comparisons have little value.  
 
Figure 8.1 Diagnostic plots for the 
one-way random effects model.  Random effects     179",1,False,0.17313078,True,0.44733703,True,0.36121064,False,0.17790072,False,0.19584382
"8.4 Blocks as Random Effects  
Blocks are properties of the experimental units. The blocks are either clearly defined by 
the conditions of the experiment or they are formed with the judgment of the 
experimenter. Sometimes, blocks represent groups of runs completed in the same period of time. Typically, we are not interested in the block effects specifically, but must 
account for their effect. It is therefore na tural to treat blocks as random effects. 
We illustrate with an experiment to compar e four processes, A, B, C and D, for the 
production of penicillin. These are the treatments. The raw material, corn steep liquor, is 
quite variable and can only be made in blends sufficient for four runs. Thus a randomized 
complete block design is suggested by the nature of the experimental units. The data comes from Box, Hunter, and Hunter (1978). We start with the fixed effects analysis: 
> data(penicillin) 
> summary(penicillin) 
treat    blend      yield 
A:5    Blend1:4   Min. :77 
B:5    Blend2:4 1st Qu.:81 
C:5    Blend3:4 Median :87 
D:5    Blend4:4 Mean   :86 
       Blend5:4 3rd Qu.:89 
                Max.   :97 
> op <- options(contrasts=c(""contr.sum"", ""contr.poly"") 
> lmod <- aov(yield ~ blend + treat, penicillin) 
> summary(lmod) 
            Df Sum Sq Mean Sq F value Pr(>F) 
blend        4  264.0    66.0    3.50  0.041 
treat        3   70.0    23.3    1.24  0.339  
Residuals   12  226.0     18.8 
> coef(lmod) 
(Intercept)      blend1       blend2    blend3    blend
4 
         86           6           -3        -
1         2 
     treat1      treat2       treat3 
         -2          -1            3  
From this we see that there is no signific ant difference between the treatments, but there 
is between the blends. Now let’s fit the data with a mixed model, where we have fixed treatment effects, but random blend effects. This seems natural since the blends we use 
can be viewed as having been selected from some notional population of blends. 
> mmod <- lmer (yield ~ treat + (1|blend), penicillin) 
> summary(mmod) 
Linear mixed-effects model fit by REML 
Formula: yield ~ treat + (1 | blend) 
   Data: penicillin 
    AIC    BIG  logLik deviance REMLdeviance 
118.60 124.58 -53.301   117.28       106.60 Extending the linear model with R     180",2,False,-0.0061231814,False,0.0658691,True,0.36497983,False,0.06721071,False,0.076097935
"Random effects: 
Groups    Name        Variance Std.Dev. 
blend     (Intercept) 11.8     3.43 
Residual              18.8     4.34 
# of obs: 20, groups: blend, 5 
Fixed effects: 
            Estimate Std. Error DF t value Pr(>|t|) 
(Intercept)    86.00       1.82 16   47.34   <2e −16 
treat1         -2.00       1.68 16   -1.19    0.251 
treat2         -1.00       1.68 16   -0.59    0.560 
treat3          3.00       1.68 16    1.78    0.093 
> options(op) 
We notice a few connections. The residual varian ce is the same in both cases: 18.8. This 
is because we have a balanced design an d so REML is equivalent to the ANOVA 
estimator. The treatment effect s are also the same as is the overall mean. The BLUPs for 
the random effects are: 
> ranef(mmod)$blend 
       (Intercept) 
Blendl     4.28788 
Blend2    -2.14394 
Blend3    -0.71465 
Blend4     1.42929 
Blend5    -2.85859 
which, as with the one-way ANOVA, are a shrunken version of the corresponding fixed 
effects. The usual diagnostics show nothing amiss. 
We can test the significance of the fixed effects in two ways. We can use the ANOVA 
method, where we assume that the random effect parameters take their estimated values:  
> anova(mmod) 
Analysis of Variance Table 
      Df Sum Sq Mean Sq Denom F value Pr(>F) 
treat  3   70.0 23.3     16.0    1.24   0.33 
The result is identical with the fixed effects analysis above. 
We can also test for a treatment effect using the maximum-likelihood ratio method: 
> amod <- lmer (yield ~ treat + (1|blend), penicillin, 
method=""ML"") 
> nmod <- lmer (yield ~ 1 + (1|blend), penicillin, 
method=""ML"") 
> anova(amod,nmod) 
Data: penicillin 
Models: 
nmod: yield 1 + (1 | blend) 
amod: yield ~ treat + (1 | blend) 
      Df   AIC   BIG logLik Chisq Chi Df Pr(>Chisq) 
nmod   3 127.3 130.3  -60.7 Random effects     181",2,False,0.14211562,False,0.2388427,True,0.38199037,False,0.08616651,False,0.12419547
"amod   6 129.3 135.3  -58.6  4.05      3       0.26 
Notice that we needed to use the ML meth od because comparison of models with 
different fixed effects is not valid when REML is used. This is because in the REML method, the likelihood of linear combination not involving the fixed effects parameters is 
maximized. When comparing models with different fixed effects, the linear combinations 
will be different and the obtained maximum likelihoods will not be comparable. The qualitative outcome of the test is the same as before, but the test itself is numerically 
different. 
We can improve the accuracy with the parametric bootstrap approach. We can 
generate a response from the null model and use this to compute the LRT. We repeat this 
1000 times, saving the LRT each time: 
> lrstat <- numeric(1000) 
> for(i in 1:1000){ 
  ryield <- unlist(simulate(nmod)) 
  nmodr <- lmer(ryield 1 + (1|blend), penicillin, 
method=""ML"") 
  amodr <- lmer(ryield ~ treat + (1|blend), penicillin, 
method=""ML"") 
  lrstat[i] <- 2*(logLik(amodr)-logLik(nmodr)) 
  } 
Under the standard likelihood theory, the LRT here should have a 
 distribution. We can 
do a QQ plot to check this: 
> 
plot(qchisq((1:1000)/1001,3),sort(lrstat),xlab=expressi
on(chi[3]^2) 
  ylab=""Simulated LRT"") 
> abline(0,l) 
As can be seen in the first panel of Figure 8. 2, the approximation is not particularly good. 
We can compute our estimated p-value as: 
> mean(Irstat > 4.05) 
[1] 0.336 
which is much closer to the F-test result than the 
 based approximation. 
We can also test the significance of the blen ds. As with a fixed effects analysis, we are 
typically not directly interested in size of the blocking effects. Once having  Extending the linear model with R     182",3,True,0.32267183,False,0.19368988,False,0.2060435,False,0.16299285,True,0.30752563
"Figure 8.2 Bootstrapped 
LRTapproximations to the χ2 
distribution. QQ plots of the test 
statistics for the fixed effects are shown on the left and for the random effects 
on the right.  
decided to design the experiment with blocks, we must retain them in the model. 
However, we may wish to examine the blocki ng effects for information useful for the 
design of future experiments. We can fit the model with and without random effects and 
compute the LRT: 
> rmod <- lmer (yield ~ treat + (1|blend), penicillin) 
> nlmod <- lm(yield ~ treat, penicillin) 
> 2* (logLik(rmod)-logLik(nlmod,REML=TRUE)) 
[1]  2.7629 
We need to specify the nondefault REML option for null model to ensure that the LRT is computed correctly. Now we perform the parametric bootstrap much as before: 
> lrstatf <- numeric(1000) 
> for(i in 1:1000){ 
  ryield <- unlist(simulate(nlmod)) 
  nlmodr <- lm(ryield ~ treat, penicillin) 
  rmodr <- lmer(ryield ~ treat + (1|blend), penicillin) 
  lrstatf [i] <- 2*(logLik(rmodr) −logLik(nlmodr, 
REML=TRUE)) 
  } 
Again, the distribution is far from 
 which is clear when we examine the proportion of 
generated LRTs which are close to zero: Random effects     183",3,False,0.22609806,False,0.2615726,True,0.33380136,False,0.15066466,False,0.2848394
"Consider the following example. In an agricultural field trial, the objective was to 
determine the effects of two crop varieties and four different irrigation methods. Eight 
fields were available, but onl y one type of irrigation may be  applied to each field. The 
fields may be divided into two parts with a different variety planted in each half. The 
whole plot factor is the method of irrigation, which should be randomly assigned to the 
fields. Within each field, the variety is rando mly assigned. Here is a summary of the data: 
> data(irrigation) 
> summary(irrigation) 
     field   irrigation   variety      yield 
f1:2   i1:4         v1:8     Min.   :34.8 
f2:2   i2:4         v2:8     1st Qu.:37.6  
f3:2    13:4    Median :40.1 
f4:2    14:4    Mean   :40.2 
f5:2            3rd Qu.:42.7 
f6:2            Max.   :47.6 
(Other):4  
The irrigation and variety are fixed effects, but the field is clearly a random effect. We 
must also consider the interaction between field and variety, which is necessarily also a 
random effect because one of the two compone nts is random. The fullest model that we 
might consider is: 
yijk=µ+ii+νj+(iv) ij+fk+(vf)jk+εijk   
µ, ii, vj, (iv) ij are fixed effects, the rest are random having variances 
 and 
 Note 
that we have no (if)ik term in this model. It would not be possible to estimate such an 
effect since only one type of irrigation is used on a given field; the factors are not 
crossed. 
We may fit this model as follows: 
> lmod <- lmer (yield ~ irrigation * variety + 
(1|field) + (1|field:variety), 
  data=irrigation) 
> logLik(lmod) 
'log Lik.' -22.697 (df=11) 
A simpler model omits the variety by field interaction random effect: 
yijk=µ+ii+νj+(iv) ij+fk+εijk   
> lmodr <- lmer (yield ~ irrigation * variety + 
(1|field),data=irrigation) 
> logLik(lmodr) 
'log Lik.' -22.697 (df=10) 
We see that although the model is simpler, the likelihood is the same. The reason for this 
is that it is not possible to distinguish the variety within field variation from the error Random effects     185",0,False,0.0853224,False,0.0994492,True,0.51686263,False,0.07251678,False,0.13968503
"variation. We would need more  than one observation per vari ety within each field for us 
to separate these two variabilities. Now examine the output of the last model: 
> summary(lmodr) 
Linear mixed-effects model fit by REML 
Formula: yield ~ irrigation * variety + (1 | field) 
   Data: irrigation 
    AIC    BIG  logLik deviance REMLdeviance 
65.395 73.121 -22.697   68.609       45.395 
Random effects: 
Groups   Name        Variance Std.Dev. 
field    (Intercept) 16.20    4.02 
Residual              2.11    1.45 
# of obs: 16, groups: field, 8 
Fixed effects: 
                        Estimate Std. Error DF t 
value  Pr(>|t|) 
(Intercept)                38.50       3.03  8   12.73 
0.0000014  
irrigationi2              1.20   4.28  8  0.28   0.79 
irrigationi3              0.70   4.28  8  0.16   0.87 
irrigationi4              3.50   4.28  8  0.82   0.44 
varietyv2                 0.60   1.45  8  0.41   0.69 
irrigationi2:varietyv2   -0.40   2.05  8 -0.19   0.85 
irrigationiS:varietyv2   -0.20   2.05  8 -0.10   0.92 
irrigationi4:varietyv2    1.20   2.05  8  0.58   0.57 
We can see that the largest variance com ponent is that due to the field effect: 
with 
 We can check the fixed e ffects for significance: 
> anova(lmodr) 
Analysis of Variance Table 
                  Df  Sum Sq Mean Sq Denom F value 
Pr(>F) 
irrigation         3    2.45    0.82  8.00    0.39   0.
76 
variety            1    2.25    2.25  8.00    1.07   0.
33 
irrigation:variety 
3    1.55    0.52  8.00    0.25   0.86 
So we see there is no evidence for a fixed eff ect for either irrigation or variety or their 
interaction. We should check the diagnostic plots to make sure there is nothing amiss: 
> 
plot(fitted(lmodr),resid(lmodr),xlab=""Fitted"",ylab=""Res
iduals"") 
> qqnorm(resid(lmodr),main="""") Extending the linear model with R     186",0,False,0.23802,False,0.25546178,True,0.35782093,False,0.21090464,False,0.24260403
"Figure 8.3 Diagnostic plots for the 
split plot example.  
We can see in Figure 8.3 that there is no problem with the nonconstant variance, but that 
the residuals indicate a short-tailed distribution. This type of divergence from normality 
is unlikely to cause any major problems with the estimation and inference. 
Sometimes analysts ignore the split-plot variable as in: 
> mod <- lm(yield ~ irrigation * variety, 
data=irrigation) 
> anova(mod) 
Analysis of Variance Table  
Response: yield 
                  Df Sum Sq Mean Sq F value Pr(>F) 
irrigation         3   40.2    13.4    0.73   0.56 
variety            1    2.2     2.2    0.12   0.73 
irrigation:variety 3    1.6     0.5    0.03   0.99 
Residuals          8  146.5    18.3  
The results will not be the same. This last model is incorrect because it fails to take into 
account the restrictions on the randomization introduced by the fields and the additional 
variability thereby induced. 
8.6 Nested Effects  
When the levels of one factor vary only within the levels of another factor, that factor is 
said to be nested . For example, when measuring the performance of workers at several 
different job locations, if the workers only work at one location, the workers are nested 
within the locations. If the workers work at more than one location, then the workers are 
crossed  with locations. 
Here is an example to illustrate nesting.  Consistency between laboratory tests is 
important and yet the results may depend on who did the test and where the test was Random effects     187",2,False,0.08777579,True,0.4033673,False,0.28562707,False,0.16760656,False,0.25929564
"Lab (Intercept)                   0.00592  0.0769 
Residual                          0.00720  0.0848 
# of obs: 48, groups: Lab:Technician:Sample, 24; 
          Lab:Technician, 12; Lab, 6 
Fixed effects: 
            Estimate Std. Error DF t value Pr(>|t|) 
(Intercept)    0.388      0.043 47    9.02    8e-12 
So we have 
 and 
 So all four variance 
components are of a similar magnitude. The lack of consistency in measures of fat 
content can be ascribed to variance between labs, variance between technicians, variance 
in measurement due to different labeling and ju st plain measurement error. We can see if 
the model can be simplified by removing the lowest level of the variance components: 
> cmodr <- Imer(Fat ~ 1 + (1|Lab) + (1|Lab:Technician), 
data=eggs) 
> anova(cmod,cmodr) 
Data: eggs 
Models: 
cmodr: Fat ~ 1 + (1 | Lab) + (1 | Lab:Technician) 
cmod: Fat ~ 1 + (1 | Lab) + (1 | Lab:Technician) + 
      (1 | Lab:Technician:Sample) 
      Df   AIC   BIG logLik Chisq Chi Df Pr(>Chisq) 
cmodr 4 -59.1  -51.6   33.5 
cmod  5 -58.7  -49.3   34.4   1.6      1       0.21 
We see that we cannot reject 
 However, we know that this p-value is con-
servative and the true value will be somewhat lower. An examination of the reduced model is interesting: 
> VarCorr(cmodr) 
Groups          Name       Variance Std.Dev. 
Lab:Technician (Intercept) 0.00800  0.0895 
Lab            (Intercept) 0.00592  0.0769 
Residual                   0.00924  0.0961 
The variation due to samples has been absorbed into the other components. 
As before, it is worth checking the accuracy of these p-values. We generate data under 
the null model and com pute the LRT 1000 times: 
> lrstat <- numeric(1000) 
> for(i in 1:1000){ 
  rFat <- unlist(simulate(cmodr)) 
  nmod <- lmer(rFat ~ 1 + (1|Lab) + (11 
Lab:Technician), data=eggs) 
  amod <- lmer(rFat ~ 1 + (1|Lab) + (11 Lab:Technician) 
+ 
  (1|Lab:Technician:Sample), data=eggs) 
  lrstat[i] <- 2*(logLik(amod)-logLik(nmod)) Random effects     189",3,False,0.21164125,False,0.27359658,True,0.31342286,False,0.11368427,False,0.15166464
"> lmod <- aov(wear ~ material + run + position, 
abrasion) 
> summary(lmod) 
            Df Sum Sq Mean Sq F value  Pr(>F) 
material     3   4622    1540   25.15 0.00085 
run          3    986     329    5.37 0.03901 
position     3   1468     489    7.99 0.01617 
Residuals    6    367      61 
All the effects are significant. However, we might regard the run and position as random 
effects. The appropriate model is then: 
> mmod <- lmer(wear ~ material + (1|run) + 
(1|position), abrasion) 
> anova(mmod) 
Analysis of Variance Table 
         Df Sum Sq Mean Sq Denom F value   Pr(>F) 
material  3   4621    1540    12    25.1 0.000018 
> summary(mmod) 
Linear mixed-effects model fit by REML 
Formula: wear ~ material + (1 | run) + (1 | position) 
   Data: abrasion 
    AIC    BIG  logLik MLdeviance REMLdeviance 
114.26 119.66 -50.128     120.43       100.26 
Random effects: 
Groups   Name        Variance Std.Dev. 
run      (Intercept)  66.9     8.18 
position (Intercept) 107.1    10.35 
Residual              61.2     7.83 
# of obs: 16, groups: run, 4; position, 4 
Fixed effects: 
            Estimate Std. Error DF t value Pr(>|t|) 
(Intercept)   265.75       7.67 12   34.66  2.1e-13 
materials     -45.75       5.53 12   -8.27  2.7e-06 
materialC     -24.00       5.53 12   -4.34  0.00097 
materialD     -35.25       5.53 12   -6.37  3.6e-05 
The lmer function is able to  recognize that the run and position effects are crossed and 
fits the model appropriately. The F-test for the fixed effects is almost the same as the 
corresponding fixed effects analysis. The only difference is that the fixed effects analysis 
uses a denominator degrees of freedom of six while the random effects analysis is made 
conditional on the estimated random effects parameters which results in 12 degrees of 
freedom. The difference is not crucial here. 
The significance of the random effects could be tested using the parametric bootstrap 
method. However, since the design of this experiment has already restricted the 
randomization to allow for these effects, there is no motivation to make these tests since we will not modify the analysis of this current experiment. 
The fixed effects analysis was somewhat eas ier to execute, but the random effects 
analysis has the advantage of producing estimat es of the variation in the blocking factors Random effects     191",2,False,0.2727918,False,0.22686236,True,0.4024285,False,0.16126359,False,0.22896759
"which will be more useful in future studies. Fixed effects estimates of the run effect for 
this experiment are only useful for the current study. 
8.8 Multilevel Models  
Multilevel models  is a term used for models for data with hierarchical structure. The term 
is most commonly used in the social scie nces. We can use the methodology we have 
already developed to fit some of these models. 
We take as our example some data from the Junior School Pr oject collected from 
primary (U.S. term is elementary) schools in inner London. The data is described in detail 
in Mortimore, Sammons, Stoll, Lewis, and Ecob (1988) and a subset is analyzed 
extensively in Goldstein (1995). 
The variables in the data are the school, the class within the school (up to four), 
gender, social class of the father (I=1; II=2; III nonmanual=3; III manual=4; IV=5; V=6; 
Long-term unemployed=7; Not currently employed=8; Father absent=9), raven’s test in 
year 1, student id number, english test scor e, mathematics test score and school year 
(coded 0, 1, and 2 for years one, two and three). So there are up to three measures per student. The data was obtained from the Multilevel Models project  at 
http://www.ioe.ac.uk/multilevel/. 
We shall take as our response the math test score result from the final year and try to 
model this as a function of gender, social cl ass and the Raven’s test score from the first 
year which might be taken as a measure of ability when entering the school. We subset 
the data to ignore the math scores from the first two years: 
> data(jsp) 
> jspr <- jsp [jsp$year==2,] 
We start with two plots of the data. Due to the discreteness of the score results, it is 
helpful to  jitter  (add small random perturbations) the scores to avoid overprinting: 
> plot(jitter(math)~jitter(raven),data=jspr,xlab=""Raven 
score"", 
  ylab=""Math score"") 
> boxplot(math ~ social, data=jspr,xlab=""Social 
class"",ylab=""Math score"") 
In Figure 8.4, we can see the positive correlation between the Raven’s test score and the 
final math score. The maximum math score was 40 which reduces the variability at the upper end of the scale. We also see how the math  scores tend to decline with social class. 
One possible approach to analyzing these data  is multiple regression. For example, we 
could fit:  Extending the linear model with R     192",2,False,0.07253519,False,0.09474371,True,0.41287684,False,0.2817154,False,0.14813113
"Figure 8.4 Plots of the Junior School 
Project data.  
> glin <- 1m (math ~ raven*gender*social, jspr) 
> anova(glin) 
Analysis of Variance Table 
Response: math 
                     Df  Sum Sq Mean Sq F value Pr(>F) 
raven                 1   11481   11481  368.06 <2e −16 
gender                1      44      44    1.41 0.2347 
social                8     779      97    3.12 0.0017 
raven:gender          1 0.01145 0.01145 0.00037 0.9847 
raven:social          8     583      73    2.33 0.0175 
gender:social         8     450      56    1.80 0.0727 
raven:gender:social   8     235      29    0.94 0.4824 
Residuals           917   28603      31 
It would seem that gender effects can be removed entirely, giving us: 
> glin <- 1m(math ~ raven*social, jspr) 
> anova(glin) 
Analysis of Variance Table 
Response: math 
              Df Sum Sq Mean Sq F value Pr(>F) 
raven          1  11481   11481  365.72 <2e −16 
social         8    778      97    3.10 0.0019 
raven:social   8    564      71    2.25 0.0222 
Residuals    935  29351      31 
This is a fairly large dataset, so even smal l effects can be signifi cant. Even though the 
raven: social term is significant at the 5% level, we remove it to simplify interpretation:  
> glin <- 1m (math ~ raven+social, jspr) 
> summary(glin) Random effects     193",2,False,0.08071369,True,0.31051037,True,0.32796407,False,0.17857113,False,0.14356928
"> mmod <- lmer(math ~ 
raven*social*gender+(1|school)+(1|school:class), 
  data=jspr)  
> anova(mmod) 
Analysis of Variance Table 
                    Df Sum Sq Mean Sq Denom F value 
Pr(>F) 
raven                1  10218   10218   917  374.40 
<2e−16 
social               8    616      77   917    2.82 
0.0043 
gender               1     22      22   917    0.79 
0.3738 
raven:social         8    577      72   917    2.64 
0.0072 
raven:gender         1      2       2   917    0.09 
0.7639 
social:gender        8    275      34   917    1.26 
0.2605 
raven:social:gender  8    187      23   917    0.86 
0.5524  
Again, it seems that gender is not important and so we simplify to: 
> jspr$craven <- jspr$raven-mean(jspr$raven) 
> mmod <- lmer(math ~ craven*social+(1|school) + 
(1|school:class) , jspr) 
> summary(mmod) 
Linear mixed-effects model fit by REML 
Formula: math ~ craven * social + (1 | school) + (1 | 
school:class) 
   Data: jspr 
    AIC    BIG   logLik deviance REMLdeviance 
5963.2 6065.2  -2960.6   5907.4       5921.2 
Random effects: 
Groups       Name         Variance Std.Dev. 
school:class (Intercept)    1.18    1.08 
school       (Intercept)    3.15    1.77 
Residual                   27.14    5.21 
# of obs: 953, groups: school:class, 90; school, 48 
Fixed effects: 
               Estimate Std. Error  DF t value Pr(>|t|) 
(Intercept)     31.9112     1.1955 935   26.69   <2e −16 
craven           0.6058     0.1885 935    3.21   0.0014 
social2          0.0236     1.2722 935    0.02   0.9852 
social3         -0.6307     1.3089 935   -0.48   0.6300 
social4         -1.9670     1.1971 935   -1.64   0.1007 
social5         -1.3585     1.3002 935   -1.04   0.2964 
social6         -2.2687     1.3737 935   -1.65   0.0990 
social7         -2.5518     1.4055 935   -1.82   0.0698 
social8         -3.3950     1.8014 935   -1.88   0.0598 
social9         -0.8313     1.2535 935   -0.66   0.5074 Random effects     195",2,False,0.2526334,False,0.15357882,True,0.33412167,False,0.16430517,False,0.1510566
"We can also check the assumption of normal ly distributed random effects. We can do 
this at the school and class level: 
> qqnorm(ranef(mmod)$school[[1]],main=""School effects"") 
> qqnorm(ranef(mmod)$""school:class""[[1]],main=""Class 
effects"") 
We see approximate normality in both cases with some evidence of short tails for the school effects. It is interesting to look at the sorted school effects: 
> adjscores <- ranef (mmod) $school [[1]] 
These represent a ranking of the schools adjusted for the quality of the intake and the 
social class of the students. The difference  between the best and the worst is about 5 
points on the math test. Of course, we must recognize that there is variability in these 
estimated effects before making any decisions about the relative strengths of  
 
Figure 8.6 QQ plots of the random 
effects at the school and class levels.  
these schools. Compare this with an unadju sted ranking that simply takes the average 
score achieved by the school, centered by the overall average: 
> rawscores <- coef(1m(math ~ school-1,jspr)) 
> rawscores <- rawscores-mean(rawscores) 
We compare these two measures of school quality in Figure 8.7: 
> plot(rawscores,adjscores) 
> sint <- c(9,14,29) 
> 
text(rawscores[sint],adjscores[sint]+0.2,c(""9"",""15"",""30
"")) Random effects     197",3,False,0.01790188,False,0.12636614,True,0.33218324,False,0.070007004,False,0.091403544
"School 10 is listed but has no students hen ce the need to adjust the labeling. There are 
some interesting differences. School 15 looks best on the raw scores but after adjustment, 
it drops to 15th place. This is a school that apparently performs well, but when the quality 
of the incoming students is considered, its performance is not so impressive. School 30 illustrates the other side of the coin. This school looks average on the raw scores, but is 
doing quite well given the ability of the incoming students. School 9 is actually doing a 
poor job despite raw scores that look quite good. 
It is also worth plotting the residuals and the random effects against the predictors. We 
would be interested in finding any inhomogeneity or signs of structure that might lead to 
an improved model. 
Compositional effects:  Fixed effect predictors in this example so far have been at the 
lowest level, the student, but it is not improbable that factors at the school or class level 
might be important predictors of success in the math test. We can construct some such 
predictors from the individual-level information; such factors are called compositional 
effects . For example, the average entering scor e for a school might be an important 
predictor. The ability of one’s fellow students may have an impact on future 
achievement. We construct this variable: 
> schraven <- lm(raven ~ school, jspr)$fit 
and insert it into our model:  
 
Figure 8.7 Raw and adjusted school-
quality measures. Three selected schools are marked.  Extending the linear model with R     198",3,False,0.072829805,False,0.18711415,True,0.33318764,False,0.1526632,False,0.13108356
"> mmodc <- lmer(math ~ 
craven*social+schraven*social+(1|school) + 
  (1|school:class),jspr) 
> anova(mmodc) 
Analysis of Variance Table 
                Df Sum Sq Mean Sq Denom F value Pr(>F) 
craven           1  10166   10166   926  373.23 <2e-16 
social           8    610      76   926    2.80 0.0045 
schraven         1      5       5   926    0.18 0.6699 
craven:social    8    561      70   926    2.58 0.0088 
social:schraven  8    166      21   926    0.76 0.6353 
We see that this new effect is not significant.  We are not constrained to taking means. We 
might consider various quantiles or measures of spread as potential compositional 
variables. 
Much remains to be investigated with this dataset. We have only used the simplest of 
error structures and we should investigate whether the random effects may also depend 
on some of the other covariates. 
Further Reading:  The classical approach to random  effects can be found in many 
older books such as Snedecor and Cochran (1989) or Scheffeé (1959). More recent books 
such as Searle, Casella, and McCulloch (1 992) also focus on the ANOVA approach. A 
wide range of models are explicitly cons idered in Milliken and Johnson (1992). 
Multilevel models are covered in Goldstein (1995) and Raudenbush and Bryk (2002). 
The predecessor to the Ime4 p ackage was nlme which is described in Pinheiro and Bates 
(2000), but the book still contains much general material of interest. 
Exercises  
1. Use the pulp dataset for this question. 
(a) Analyze the data as a fixed effect s model. Is the operator significant? 
(b) Analyze the data with operator as a random effect. What are the estimated 
variances? 
(c) Compute confidence intervals for these variances. 
(d) Compute the intraclass correlation coefficient. 
(e) Determine the significance of the operator effect using a likelihood ratio test taking 
care to compute the p-value accurately. 
2. The coagulat ion dataset comes from a study of blood coagulation times. Twenty-four 
animals were randomly assigned to four different diets and the samples were taken in 
a random order. 
(a) A new animal is assigned to diet D. Predict the blood coagulation time for this 
animal along with an estimate of the variability in this prediction. 
(b) A new diet is given to a new animal. Predict the blood coagulation time for this 
animal along with an estimate of the variability in this prediction. Random effects     199",2,False,0.057339933,False,0.040400825,True,0.38269073,False,0.061977763,False,0.08899216
"(c) A new diet is given to the first animal in the dataset. Predict the blood coagulation 
time for this animal along with an estimate of the variability in this prediction. You 
may assume that the effects of the initial diet for this animal have washed out. 
3. The eggprod dataset concerns an experiment where six pullets were placed into each of 
12 pens. Four blocks were formed from groups of three pens based on location. Three 
treatments were applied. The number of eggs produced was recorded. 
(a) Fit a model for the number of eggs produced with the treatments as fixed effects 
and the blocks as random effects. Descri be the estimated differences between the 
treatments. 
(b) Test for the significance of the treatment. Compute the p-value using both the χ2 
distribution and resampling methods. 
4. Data on the cutoff times of lawnmowers may be found in the dataset lawn. 3 machines 
were randomly selected from those pr oduced by manufacturers A and B. Each 
machine was tested twice at low speed and high speed. 
(a) Fit a mixed effects model with manufactur er and speed as main effects along with 
their interaction and machine nested in manufacturer as random effects. Write 
down the formula for the model. In the summary output for the model, you will 
find that fixed manufacturer effect has zero degrees of freedom. Explain why this is 
so (check your model formula).  
(b) Show why the manufacturer term may be removed from the fixed effect part of the 
model. 
(c) Determine if the manufacturer term can be removed from the random part of the 
model. 
5. A number of growers supply broccoli to a food processing plant. The plant instructs 
the growers to pack the broccoli into standard -size boxes. There should be 18 clusters 
of broccoli per box and each cluster should weigh between 1.33 and 1.5 pounds. 
Because the growers use different varieties and methods of cultivation, there is some 
variation in the cluster weights. The plant manager selected three growers at random 
and then four boxes at random supplied by these growers. Three clusters were selected 
from each box. The data may be found in th e broccoli dataset. The weight in grams of 
the cluster is given. Estimate and test the variance components. 
6. An experiment was conducted to select the supplier of raw materials for production of 
a component. The breaking strength of the component was the objective of interest. 
Four suppliers were considered. The four operators can only produce one component 
each per day. A latin square design is used  and the data is pr esented in breaking. 
(a) Explain why it would be natural to treat the operators and days as random effects 
but the suppliers as fixed effects. 
(b) Build a model to predict the breaking strength. Describe the variation from 
operator to operator and from day to day. 
(c) Test the significance of the supplier effect. 
(d) Is there a significant difference between the operators? 
(e) For the best choice of supplier, predict what proportion of components produced in 
the future will have a breaking strength exceeding 900. Extending the linear model with R     200",2,False,0.087328434,False,0.18452428,True,0.38652146,False,0.14038222,False,0.18925975
"7. An experiment was conducted to optimize the manufacture of semiconductors. The 
semicond data has the resistance recorded on the wafer as the response. The 
experiment was conducted during four different time periods denoted by ET and three 
different wafers during each period. The positi on on the wafer is a factor with levels 1 
to 4. The Grp variable is a combination of  ET and wafer. Analyze the data as a split 
plot experiment where ET and position are considered as fixed effects. Since the 
wafers are different in experimental time periods, the Grp variable should be regarded 
as the block or group variable. Determine the best model for the data and check all 
appropriate diagnostics. 
8. Redo the Junior Schools Project data analys is in the text with the final year English 
score as the response. Highlight any differen ces from the analysis of the final year 
Math scores. 
9. An experiment was conducted to determine the effect of recipe and baking temperature 
on chocolate cake quality. Fifteen batches of  cake mix for each recipe were prepared. 
Each batch was sufficient for six cakes. Each  of the six cakes was baked at a different 
temperature which was randomly assigned. Several measures of cake quality were 
recorded of which breaking angle was just one. The dataset is presented as choccake. 
Build an appropriate model for the data  and write a report on the analysis. Random effects     201",2,False,0.1278682,False,0.18544883,True,0.34364617,False,0.17204353,False,0.24189314
"CHAPTER 9  
Repeated Measures and Longitudinal Data  
In repeated measures designs, there are seve ral individuals and measurements are taken 
repeatedly on each individual. When these rep eated measurements are taken over time, it 
is called a longitudinal  study or, in some applications, a panel  study. Typically various 
covariates concerning the individual are reco rded and the interest centers on how the 
response depends on the covariates over time. Often it is reasonable to believe that the 
response of each individual has several components: a fixed effect, which is a function of 
the covariates; a random effect, which expresses the variation between individuals; and an error, which is due to meas urement or unrecorded variables. 
Suppose each individual has response y
i, a vector of length ni which is modeled 
conditionally on the random effects γi as: 
   
Notice this is very similar to the model used in the previous chapter with the exception of allowing the errors to have a more general covariance ai. As before, we assume that the 
random effects γi~N(0,σ
2D) so that: 
   
where 
 Now suppose we have M individuals and we can assume 
the errors and random effects between individua ls are uncorrelated, then we can combine 
the data as: 
   
and 
 Z=diag( Z1, Z 2,…, ZM), Σ=diag(Σ1, Σ2,…, ΣM), and 
Λ=diag(Λ1, Λ2,…, ΛM). Now we can write the model simply as 
   
The log-likelihood for the data is then computed as above and estimation, testing, standard errors and confidence intervals all follow using standard likelihood theory as before. In fact, there is no strong distincti on between the methodology used in this and 
the previous chapter. 
Of course, this general structure encompasses a wide range of possible models for 
different types of data. We explore some of these in the following three examples:",1,False,0.05578551,False,0.21276918,True,0.52488685,False,0.12247361,False,0.12776914
"9.1 Longitudinal Data  
The Panel Study of Income Dynamics (PSID), begun in 1968, is a longitudinal study of a 
representative sample of U.S. individual s described in Hill (1992). The study is 
conducted at the Survey Research Center, In stitute for Social Research, University of 
Michigan, and is still continuing. There are currently 8700 households in the study and 
many variables are measured. We chose to  analyze a random subset of this data, 
consisting of 85 heads of household who were aged 25-39 in 1968 and had complete data for at least 11 of the years between 1968 an d 1990. The variables included were annual 
income, gender, years of education and age in 1968: 
> data(psid) 
> head(psid) 
  age educ sex income year person 
1  31   12   M   6000   68      1 
2  31   12   M   5300   69      1 
3  31   12   M   5200   70      1 
4  31   12   M   6900   71      1 
5  31   12   M   7500   72      1 
6  31   12   M   8000   73      1 
Now plot the data: 
> library(lattice) 
> xyplot(income ~ year | person, psid, type=""1"", 
  subset=(person < 21),strip=FALSE) 
The first 20 subjects are shown in Figure 9.1. We see that some individuals have a slowly 
increasing income, typical of someone in steady employment in the same job. Other 
individuals have more erratic incomes. We can also show how the incomes vary by sex. Income is more naturally considered on a log-scale: 
> xyplot(log(income+100) ~ year | sex, psid, type=""1"") 
See Figure 9.2. We added $100 to the income  of each subject to remove the effect of 
some subjects having very low incomes for short periods of time. These cases distorted 
the plots without the adjustment. We see that men’s incomes are generally higher and less 
variable while women’s incomes are more variable, but are perhaps increasing more 
quickly. We could fit a line to each  subject starting with the first: 
> lmod <- 1m(log(income) ~ I(year-78), 
subset=(person==1), psid) 
> coef(lmod) 
  (Intercept) I(year - 78) 
     9.399957     0.084267 Extending the linear model with R     204",3,False,0.072797686,False,0.21614833,True,0.30194917,False,0.2147119,False,0.12459239
"We have centered the predictor at the median value so that the intercept will represent the 
predicted log income in 1978 and not the year 1900 which would be nonsense. We now 
fit a line for all the subjects and plot the results: 
> slopes <- numeric(85);intercepts <- numeric(85) 
> for(i in 1:85){ 
     lmod <- 1m(log(income) ~ I(year-78), 
subset=(person==i), psid) 
     intercepts[i] <- coef(lmod)[1] 
     slopes[i] <- coef(lmod)[2] 
     } 
 
Figure 9.1 The first 20 subjects in the 
PSID data. Income is shown over time.  Repeated measures and longitudinal data     205",3,False,0.21143001,True,0.33711755,True,0.30237654,False,0.27077085,False,0.26858926
"where i indexes the year and j indexes the individual. We have: 
   
The model summary is: 
> summary(mmod) 
Linear mixed-effects model fit by REML 
Formula: log(income) ~ cyear * sex + age + educ + 
(cyear | person) 
   Data: psid 
    AIC    BIG  logLik MLdeviance REMLdeviance 
3839.8 3893.9 -1909.9     3785.6       3819.8 
Random effects: 
Groups   Name        Variance  Std.Dev. Corr 
person   (Intercept) 0.2817    0.531 
          cyear       0.0024    0.049    0.187 
Residual              0.4673    0.684 
# of obs: 1661, groups: person, 85  
Fixed effects: 
            Estimate Std. Error   DF t value  Pr(>|t|) 
(Intercept)   6.6742     0.5433 1655   12.28   < 2e-16 
cyear         0.0853     0.0090 1655    9.48   < 2e-16 
sexM          1.1503     0.1213 1655    9.48   < 2e-16 
age           0.0109     0.0135 1655    0.81     0.419 
educ          0.1042     0.0214 1655    4.86 0.0000013 
cyear:sexM   -0.0263     0.0122 1655   -2.15     0.032  
Let’s start with the fixed effects. We se e that income increas es about 10% for each 
additional year of education. We see that age does not appear to be significant. For 
females, the reference level in this example,  income increases abou t 8.5% a year, while 
for men, it increases about 8.5-2.6=5.9% a year . We see that, for this data, the incomes of 
men are exp(1.15) = 3.16 times higher. 
We know the mean for males and females, but individuals will vary about this. The 
standard deviation for the intercept and slope are 0.28 and 0.0024 
 and 
respectively. These have a correlation of 0.19 ( cor(γ0,γ1)). Finally, there is some 
additional variation in the measurement not  so far accounted for having standard 
deviation of 0.46 ( sd(εijk)). We see that the variation in increase in income is relatively 
small while the variation in overall income between individuals is quite large. 
Furthermore, given the large residual variation,  there is a large year-to-year variation in 
incomes. 
There is a wider range of possible diagnostic plots that can be made with longitudinal 
data than with a standard linear model. In addition to the usual residuals, there are Extending the linear model with R     208",3,True,0.32883984,True,0.3072003,True,0.38506204,True,0.30915564,False,0.22782432
"random effects to be examined. We may wish to  break the residuals down by sex as seen 
in the QQ plots in Figure 9.4: 
> qqmath (~resid(mmod) | sex, psid) 
 
Figure 9.4 QQ plots by sex.  
We see that the residuals are not normally distributed, but have a long tail for the lower 
incomes. We should consider changing the log transformation on the response. 
Furthermore, we see that there is greater va riance in the female incomes. This suggests a 
modification to the model. We can make the same plot broken down by subject although 
there will be rather too many plots to be useful. 
Plots of residuals and fitted values are also valuable. We have broken education into 
three levels: less than high school, high school or more than high school: 
> xyplot(resid(mmod) ~ fitted(mmod) | 
cut(educ,c(0,8.5,12.5,20)), 
  psid, layout=c(3,l),xlab=""Fitted"",ylab=""Residuals"") 
See Figure 9.5. Again, we can see evidence that a different response transformation 
should be considered. 
Plots of the random effects would also be useful here. 
9.2 Repeated Measures  
The acuity of vision for seven subjects was tested. The response is the lag in milliseconds 
between a light flash and a response in the cortex  of the eye. Each ey e is tested at four 
different powers of lens. An object at the distance of the second number appears to be at 
distance of the first number. The data is given in Table 9.1. The data comes from Crowder and Hand (1990) and was also analyzed by Lindsey (1999). 
 Repeated measures and longitudinal data     209",1,False,0.15566182,True,0.47736233,True,0.32735932,False,0.19108751,False,0.22062704
"Figure 9.5 Residuals νs. fitted plots for 
three levels of education: less than high school on the left, high school in the middle and more than high school 
on the right.  
We start by making some plots of the data. We create a numerical variable 
representing the power to complement the ex isting factor so that we can see how the 
acuity changes with increasing power: 
> data(vision) 
> vision$npower <- rep(1:4,14) 
> 
xyplot(acuity~npower|subject,vision,type=""1"",groups=eye
, 
  lty=1:2,layout=c(4,2)) 
See Figure 9.6. There is no apparent trend or difference between right and left eyes.  
feet. 
However, individual #6 appears anomalous w ith a large difference between the eyes. It 
also seems likely that the third measurement on the left eye is in error for this individual. Extending the linear model with R     210",3,False,0.1776442,True,0.38580668,False,0.16388315,False,0.19702904,False,0.19243237
"is any consistent right-left eye difference be tween individuals, we should treat the eye 
factor as nested within subjec ts. We start with this model: 
> mmod <- 
lmer(acuity~power+(1|subject)+(1|subject:eye),vision) 
Note that if we did believe there was a consistent left vs. right eye effect, we would have 
used crossed random effects, putting (1 1 eye) in place of (11 subject: eye). 
We can write this (nested) model as: 
yijk=µ+Pj+si+eik+εijk   
where i= 1,…,7 runs over individuals, j= 1,…, 4 runs over power and k=1, 2 runs over 
eyes. The pj term is a fixed effect, but th e remaining terms are random. Let 
and 
 where we take Σ=I. The summary 
output is: 
> summary(mmod) 
Linear mixed-effects model fit by REML 
Formula: acuity ~ power + (1 | subject) + (1 | 
subject:eye) 
   Data: vision 
    AIC    BIG  logLik MLdeviance REMLdeviance 
342.71 356.89 -164.35     339.22       328.71 
Random effects: 
Groups      Name        Variance Std.Dev. 
subject:eye (Intercept) 10.3     3.21 
subject     (Intercept) 21.5     4.64 
Residual                16.6     4.07 
# of obs: 56, groups: subject:eye, 14; subject, 7 
Fixed effects: 
            Estimate Std. Error DF t value Pr(>|t|) 
(Intercept)  112.643      2.235 52   50.40   <2e −16 
power6/18      0.786      1.540 52    0.51    0.612 
power6/36     -1.000      1.540 52   -0.65    0.519 
power6/60      3.286      1.540 52    2.13    0.038 
We see that the estimated standard deviation for subjects is 4.64 and that for eyes for a 
given subject is 3.21. The residual standard deviation is 4.07. The random effects structure we have used here induces a corr elation between measurements on the same 
subject and another between measurements on the same eye. We can compute these two 
correlations respectively as: 
> 4.64^2/(4.64^2+3.21^2+4.07^2) 
[1] 0.44484 
> (4.64^2+3.21^2)/(4.64^2+3.21^2+4.07^2) 
[1] 0.65774 Extending the linear model with R     212",4,False,0.21553887,False,0.13639547,True,0.36957073,False,0.18308145,False,0.16406941
"We now fit a model for the data that includes all the variables of interest that incorporates 
some of the interactions that we suspect might be present: 
> mjspr$craven <- mjspr$raven-mean(mjspr$raven) 
> mmod <- lmer (score ~ 
subject*gender+craven*subject+social+ 
  (1|school)+(1|school:class)+(1|school:class:id),mjspr
) 
The model being fit for school i, class j, student k in subject l is: 
score ijkl=subject l+gender k+raven k+social k+(subject ×gender) lk+ 
(raven ×subject) lk+school i+class j+student k+εijkl   
where the Raven score has been mean cente red and school, class and student are random 
effects with the other terms, apart from ε, being fixed effects. The test on the fixed effects 
reveals: 
> anova(mmod)  
Analysis of Variance Table 
               Df Sum Sq Mean Sq Denom F value  Pr(>F) 
subject         1     54      54  1892 3953.67 < 2e-16 
gender          1  0.101   0.101  1892    7.46  0.0064 
craven          1      6       6  1892  444.63 < 2e-16 
social          8      1   0.088  1892    6.47 2.5e-08 
subject:gender  1  0.384   0.384  1892   28.23 1.2e-07 
subject:craven  1  0.217   0.217  1892   15.99 6.6e-05  
Both interactions are significant so no simp lifications are indicated for this model. We 
might consider adding additional fixed effects, but we shall attempt to interpret only this 
model for now. The summary output: 
> summary(mmod) 
Linear mixed-effects model fit by REML 
Formula: score ~ subject * gender + craven * subject + 
social + (1 | school) + 
      (1 | school:class) + (1 | school:class :id) 
    Data: mjspr 
      AIC     BIG logLik MLdeviance REMLdeviance 
  -1705.6 -1605.6 870.79    -1846.3      -1741.6 
Random effects: 
Groups          Name        Variance Std.Dev. 
school:class:id (Intercept) 0.010252 0.1013 
school:class    (Intercept) 0.000582 0.0241 
school          (Intercept) 0.002231 0.0472 
Residual                    0.013592 0.1166 
# of obs: 1906, groups: school:class:id, 953; 
school:class, 90; school, 48 
Fixed effects: Extending the linear model with R     216",2,False,0.2658548,False,0.1626791,True,0.4903909,False,0.22347759,False,0.18510073
"> 0.1013^2/(0.1013^2+0.1166^2) 
[1] 0.43013 
giving a moderate positive correlation between the scores. Various diagnostic plots can 
be made. An interesting one is: 
> xyplot(residuals(mmod) ~ 
fitted(mmod)|subject,mjspr,pch=""."", 
  xlab=""Fitted"",ylab=""Residuals"") 
as seen in Figure 9.9. There is somewhat  greater variance in the verbal scores. The 
truncation effect of the maximum score is particularly visible for the math scores. 
 
Figure 9.9 Residuals νs. fitted plot 
broken down by type of test.  
Further Reading:  Longitudinal data analysis is explicitly covered in books by Verbeke 
and Molenberghs (2000), Fitzmaurice, Laird, and Ware (2004), Diggle, Heagerty, Liang, 
and Zeger (2002) and Frees (2004). Books sta ting repeated measures in the title, such as 
Lindsey (1999), cover much the same material.  
Exercises  
1. The ratdrink data consist of five weekly measurements of body weight for 27 rats. The 
first 10 rats are on a control treatment while seven rats have thyroxine added to their 
drinking water. Ten rats have thiouracil added to their water. Build a model for the rat 
weights that shows the ef fect of the treatment. 
2. Data on housing prices in 36 metropolita n statistical areas (MSAs) over nine years 
from 1986-94 were collected and can be f ound in the dataset hprice. Find a good 
model for the data. Explain the effect of the predictors on housing prices. It is not 
necessary to present every part of your anal ysis. Present a compact description of how 
you found your model in five pages or less. Extending the linear model with R     218",1,False,0.10321432,True,0.34198666,False,0.15746559,False,0.100771144,False,0.20221779
"3. The nepali data is a subset from public health study on Nepalese children. Develop a 
model for the weight of the child as he or she ages. You may use mage, lit, died, 
gender and alive (but not ht) as predictors. Show how you developed your model and 
interpret your final model. 
4. The attenu data gives peak accelerations measured at various observation stations for 
23 earthquakes in California. The data has been used by various workers to estimate 
the attenuating affect of di stance on ground acceleration. 
(a) Model the log of the acceleration as a fu nction of the log of the distance while 
taking account of the magnitude of the quake. 
(b) Predict how the acceleration varied for an  earthquake of magnitude 7.5. Express 
quantitatively the uncertainty in this prediction. 
(c) Predict how the acceleration varied for th e first event where only one observation 
was available. 
5. The sleepstudy data found in the Matrix package, which is loaded with Ime4, describes 
the reaction times of subjects who are progressively sleep deprived. Form a model for 
the reaction times and describe the variation between individuals. Repeated measures and longitudinal data     219",4,False,0.18898654,False,0.24475473,True,0.41024,True,0.30923277,False,0.18692623
"CHAPTER 10  
Mixed Effect Models for Nonnormal 
Responses  
10.1 Generalized Linear Mixed Models  
Generalized linear mixed models (GLMM) combine the ideas of  generalized linear 
models with the random effects modeling ideas of the previous two chapters. The 
response is a random variable, Yi, taking observed values, yi, for i=1,…, n, and follows an 
exponential family distribution as defined in Chapter 6: 
   
Let EYi=µi and let this be connected to the linear predictor ηi using the link function g by 
ηi=g(µ i). Suppose for simplicity that we use the canonical link for g so that we may make 
the direct connection that θi=µi. 
Now let the random effects, γ, have distribution h(γ|V) for parameters V. The fixed 
effects are β. Conditional on the random effects, γ: 
   
where xi and zi are the corresponding rows from the design matrices, X and Z, for the 
respective fixed and random effects. Now the likelihood may be written as: 
   
Typically the random effects are assumed normal: γ~N(0,D). However, unless f is also 
normal, the integral remains in the likelihood, which becomes difficult to compute, 
particularly if the random eff ects structure is complicated. 
A variety of approaches are available to ap proximating the likelihood using theoretical 
or numerical methods. A Bayesian approach is also possible. See Sinha (2004) for a 
recent approach that also contains a survey of  past approaches. We investigate the issues 
through an example. 
An experiment was conducted to study the effects of surface and vision on balance. 
The balance of subjects were observed for tw o different surfaces and for restricted and 
unrestricted vision. Balance was assessed qualitatively on an ordinal four-point scale 
based on observation by the experimenter. Forty subjects were studied, twenty males and 
twenty females ranging in age from 18 to 38, with heights given in cm and weights in kg. The subjects were tested while standing on foam or a normal surface and with their eyes 
closed or open or with a do me placed over their head. Each  subject was tested twice in",0,True,0.3488138,False,0.21695358,True,0.35658228,False,0.14608616,False,0.13513891
"However, there may be a significant subject effect. We could try including a fixed subject 
factor: 
> gfs <- glm(stable ~ 
Sex+Age+Height+Weight+Surface+Vision+factor(Subject), 
  binomial,data=ctsib) 
> anova(gf,gfs,test=""Chi”) 
Analysis of Deviance Table 
  Resid. Df Resid. Dev Df Deviance P(>|Chi|) 
1       472        295 
2       437        121  35      174  2.5e-20 
We see strong evidence for a significant subject effect. However, when we examine the 
summary for this model, we see problems with identifiability and separability which 
prompt the use of bias-reduced logistic regression, as described in Section 2.8:  
> library(brlr) 
> modbr <- brlr (stable ~ 
Sex+Age+Height+Weight+Surface+Vision+ 
  factor (Subject), data=ctsib) 
> summary(modbr) 
Coefficients: 
                  Value   Std. Error t value 
(Intercept)       -44.619  50.432     -0.885 
Sexmale             0.538   3.060      0.176 
Age                -0.223   0.137     -1.625 
Height              0.335   0.351      0.953 
Weight             -0.255   0.186     -1.370 
Surfacenorm         6.150   0.734      8.378 
Visiondome          0.635   0.489      1.300 
Visionopen          5.043   0.687      7.344 
We find that the subject-specific variables, sex, age, height and weight, are no longer 
significant. This is because these predictors are constant for a given subject. We cannot completely unconfound these eff ects from the subject effects. 
There are variety of ways of fitting GLMMs in R. We demonstrate the Penalized 
Quasi-Likelihood method implemented in the MASS package: 
> library(MASS) 
> gg <- glmmPQL(stable ~ 
Sex+Age+Height+Weight+Surface+Vision, 
  random=~1 | Subject, family=binomial,data=ctsib) 
> summary(gg) 
Random effects: 
Formula: ""1 | Subject 
        (Intercept) Residual 
StdDev:      3.0608  0.59062 
Variance function: 
Structure: fixed weights 
Formula: ~invwt Mixed effect models for nonnormal responses     223",2,False,0.15558355,False,0.15570515,True,0.31732345,False,0.15666193,True,0.34174854
"Fixed effects: stable ~ Sex + Age + Height + Weight + 
Surface + Vision 
              Value Std.Error DF t-value p-value 
(Intercept) 15.5716   13.4985 437  1.1536  0.2493 
Sexmale      3.3554    1.7526  35  1.9145  0.0638 
Age         -0.0066    0.0820  35 -0.0810  0.9359 
Height      -0.1908    0.0920  35 -2.0736  0.0455 
Weight       0.0695    0.0629  35  1.1051  0.2766 
Surfacenorm  7.7241    0.5736 437 13.4666  0.0000 
Visiondome   0.7265    0.3259 437  2.2289  0.0263 
Visionopen   6.4853    0.5440 437 11.9220  0.0000 
The fit falls somewhere between the two a bove from the point of view of effect 
significance. Notice how there are more degrees  of freedom for the experimental factors 
which do vary within individuals. This is expected. Compared to the fixed effect subject modeling, rather less of the variation is attri buted to the GLMM. Here the SD for subjects 
is 3.06 while the SD of the subject effects from the GLM is: 
> sd(coef(modbr)[9:43]) 
[1] 4.4407 
This model can also be fit using the lmer function from the Ime4 package. Estimation of GLMMs is an active area of research and furt her study of the best methods of estimation 
is necessary. 
10.2 Generalized Estimating Equations  
The advantage of the quasi-lik elihood approach co mpared to GLMs was that we did not 
need to specify the distribution of the response. We only needed to give the link function and the variance. We can adapt this approach  for repeated measures and/or longitudinal 
studies. Let Y
i be a vector of random variables representing the responses on a given 
individual and let EYi=µi which is then linked to the linear predictor η=Xβ in some 
appropriate way. Let: 
var Yi≡var (Yi;β,α)   
where a represents parameters that model th e correlation structure within individuals. The 
parameters, β, may then be estimated setting the (multivariate) score function to zero and 
solving: 
   
These equations can be regarded as the multi variate analogue of those used for the quasi-
likelihood models described in Section 7.4. Since var Y also depends on α, we substitute 
any consistent estimate of α in this equation and still obtain an estimate as asymptotically Extending the linear model with R     224",0,True,0.3128369,False,0.20879717,True,0.33825517,False,0.16965714,False,0.20744045
"(b) In your model, what indicates that a child who already wheezes is likely to 
continue to wheeze? 
(c) What is the predicted probability that a 7 year-old with a smoking mother, 
wheezes? 
(d) Repeat your analysis using a GLM where you assume that the observations are 
independent, that is, each single response value represents a different child. 
Indicate how the conclusions would differ and which results should be preferred. 
(e) Sum the number of times wheezing is recorded for a child  over the four 
measurements and model this as a function of the smoking status of the mother. 
This can be achieved as follows: 
> nohio <- reshape(ohio,idvar =""id"",direction=""wide"", 
  timevar=""age"",v.names=""resp"") 
> nohio <- data.frame(smoke=nohio$smoke, 
  wheeze=apply(nohio[,3:6],1,sum)) 
Now determine the effect of smoking. Compare this result to the previous analyses and 
discuss which is preferable. 
2. The National Youth Survey collected a sample of 11–17 year-olds, 117 boys and 120 
girls, asking questions about marijuana usage. The data is presented in potuse. 
(a) Condense the levels of the response into whether the person did or did not use 
marijuana that year. Build a model for ma rijuana usage over the time period that 
takes account of sex differences. 
(b) In your model, what describes correla tion between marijuana usage one year and 
the next for a particular individual? 
(c) What is the difference between boys and girls? 
(d) Compute the predicted probability of usage by boys over time. 
(e) Can you model the original three-level response in R? 
3. Components are attached to an electronic circuit card assembly by a wave-soldering 
process. The soldering process involves baking and preheating the circuit card and 
then passing it through a solder wave by conveyor. Defects aris e during the process. 
The design is 27−3 with three replicates and the data  is found in wave solder. Assuming 
that the responses for each run are in time  order, analyze the data. Is there any 
evidence of an effect due to the time order? 
4. The nitrofen data in boot package co me from an experiment to measure the 
reproductive toxicity of the pesticide nitrofen on a species of zooplankton called 
Ceriodaphnia dubia . Each animal produced three broods in which the number of live 
offspring was recorded. Fifty animals in total were used and divided into five batches. 
Each batch was treated in a solution with a different concentration of the pesticide. 
Build a model for the number of live offspring produced in the successive broods. 
Your model should describe how this number changes and is related within a 
given animal and how this relates to the concentration of pesticide.  
5. The toenail data comes from a multicenter study comparing two oral treatments for 
toenail infection. Patients were evaluated for the degree of separation of the nail. Extending the linear model with R     230",1,False,0.19643663,False,0.29390347,True,0.37581745,False,0.25901872,False,0.1730935
"The nonparametric approach is more flexible. In modeling new data, one often has 
very little idea of an appropriate form for the model. We do have a number of heuristic 
tools using diagnostic plots to help search for this form, but it would be easier to let the 
modeling approach take care of this search. Another disadvantage of the parametric approach is that one can easily choose the wr ong form for the model and this results in 
bias. The nonparametric approach assumes far less and so is less liable to make bad 
mistakes. The nonparametric approach is particularly useful when little past experience is available 
For our examples we will use three datasets, one real (data on Old Faithful) and two 
simulated, called exa and exb. The data comes from Härdle (1991). The reason we use simulated data is to see how well the estimates match the true function (which cannot 
usually be known for real data). We plot the data in the first three panels of Figure 11.1, 
using a line to mark the true function where known. For exa, the true function is 
f(x)=sin
3(2πx3). For exb, it is cons tant zero, that is, f(x)=0: 
> data(exa) 
> plot (y ~ x, exa,main=""Example A"",pch=""."") 
> lines(m ~ x, exa) 
> data(exb) 
> plot(y ~ x, exb,main=""Example B"",pch=""."") 
> lines(m ~ x, exb) 
> data(faithful) 
> plot(waiting ~ duration, faithful,main=""old 
Faithful"",pch=""."") 
We now examine several widely used nonparametic regression estimators, also known as 
smoothers . 
 
Figure 11.1 Data examples. Example A 
has varying amounts of curvature, two optima and a point of inflexion. Example B has two outliers. The Old Faithful provides the challenges of 
real data.  Nonparametric regression     233",3,False,0.103162736,True,0.3344549,False,0.19331774,False,0.12960243,False,0.18033637
"> lines(exa$x,exa$m) 
> lines(smooth.spline(exa$x,exa$y),lty=2) 
> plot(y ~ x, exb, pch=""."") 
> lines(exb$x,exb$m) 
> lines(smooth.spline(exb$x,exb$y), lty=2) 
 
Figure 11.5 Smoothing spline fits. For 
Examples A and B, the true function is 
shown as solid and the spline fit as 
dotted.  
The fits may be seen in Figure 11.5. The fit for the Old Faithful data looks reasonable. 
The fit for Example A does a good job of tracking the hills and valleys but overfits in the 
smoother region. The default choice of smoothing parameter given by CV is a disaster for 
Example B as the data is just interpolated. This illustrates the danger of blindly relying on 
automatic bandwidth selection methods. 
Regression Splines:  Regression splines differ from smoothing splines in the fol-
lowing way: For regression splines, the knot s of the B-splines used for the basis are 
typically much smaller in number than the sample size. The number of knots chosen controls the amount of smoothing. For smoothing splines, the observed unique x values 
are the knots and λ is used to control the smoothing. It is arguable whether the regression 
spline method is parametric or nonparametric, because once the knots are chosen, a parametric family has been specified with a finite number of parameters. It is the freedom 
to choose the number of knots that makes the method nonparametric. One of the desirable 
characteristics of a nonparametri c regression estimator is that it should be consistent for 
smooth functions. This can be achieved for regression splines if the number of knots is 
allowed to increase at an appropri ate rate with the sample size. 
We demonstrate some regression splines here . We use piecewise linear splines in this 
example, which are constructed and plotted as follows: 
> rhs <- function (x,c) ifelse (x>c, x-c, 0) 
> curve(rhs(x,0.5), 0,1) Extending the linear model with R     240",3,False,0.2136569,True,0.34494627,False,0.16180569,False,0.24772337,False,0.099781275
"where the spline is shown in the first panel of Figure 11.6. Now we define some knots for 
Example A: 
> knots <- 0:9/10 
> knots 
  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
and compute a design matrix of splines with knots at these points for each x: 
> dm <- outer (exa$x,knots,rhs) 
> matplot(exa$x,dm,type=""l"",col=l) 
where the basis functions are shown in the second panel of Figure 11.6. Now we  
 
Figure 11.6 One basis function for 
linear regression splines shown on the 
left and the complete set shown on the 
right.  
compute and display the regression fit: 
> g <- lm(exa$y ~ dm) 
> plot (y ~ x, exa,pch=""."",xlab=""x"",ylab=""y"") 
> lines(exa$x,predict(g)) 
where the plot is shown in the first panel of  Figure 11.7. Because the basis functions are 
piecewise linear, the fit is also piecewise linear . A better fit may be obtained by adjusting 
the knots so that they are denser in regions of greater curvature: Nonparametric regression     241",0,False,0.18963504,True,0.30299637,False,0.10580047,False,0.27207512,False,0.07756858
"default choice takes three quarters of the data  and may not be a good choice as we shall 
see below.  
For the Old Faithful data, the default choice is satisfactory, as seen in the first panel of 
Figure 11.9: 
> plot(waiting ~ duration, faithful,pch=""."") 
> f <- loess(waiting ~ duration, faithful) 
> i <- order(faithful$duration) 
> lines(f$x[i],f$fitted[i]) 
For Example A, the default choice is too larg e. The choice that minimizes the integrated 
squared error between the estimated and true function requires a span (proportion of the range) of 0.22. Both f its are seen in the middle panel of Figure 11.9: 
> plot(y ~ x, exa, pch=""."") 
> lines(exa$x,exa$m,lty=1) 
> f <- loess(y ~ x,exa) 
> lines(f$x,f$fitted,lty=2) 
> f <- loess(y ~ x, exa, span=0.22) 
> lines(f$x,f$fitted,lty=5) 
In practice, the true function is, of course, unknown and we would need to select the span 
ourselves, but this optimal choice does at l east show how well loess can do in the best of 
circumstances. The fit is similar to that for smoothing splines. 
For Example B, the optimal choice of span is one (that is all the data). This is not 
surprising since the true function is a constant and so maximal smoothing is desired. We 
can see that the robust qualities of loess prevent the fit from becoming too distorted by the two outliers even with the default choice of smoothing span: 
> plot(y ~ x, exb, pch=""."") 
> f <- loess(y ~ x, exb) 
> lines(f$x,f$fitted,lty=2) 
> f <- loess(y ~ x, exb,span=1) 
> lines(f$x,f$fitted,lty=5) 
> lines(exb$x,exb$m) 
11.4 Wavelets  
Regression splines are an example of a basis function approach to fitting. We 
approximate the curve by a family of basis functions, 
 so that 
Thus the fit requires estimating the coefficients, ci. The choice of basis functions will 
determine the properties of the fitted curve. The estimation of ci is particularly easy if the 
basis functions are orthogonal. 
Examples of orthogonal bases are orthogonal polynomials and the Fourier basis. The 
disadvantage of both these families is that the basis functions are not compactly 
supported so that the fit of each basis function depends on the whole data. This means Extending the linear model with R     244",1,False,0.23213574,True,0.3926506,False,0.1918644,False,0.22511762,False,0.13248974
"12.1 Additive Models Using the gam Package  
We use data from a study of the relationship between atmospheric ozone concentration, 
O3 and other meteorological variables in the Los Angeles Basin in 1976. To simplify 
matters, we will reduce the predictors to just three: temperature measured at E1 Monte, 
temp, inversion base height at LAX, ibh, and inversion top temperature at LAX, ibt. A 
number of cases with missing variables have been removed for simplicity. The data were 
first presented by Breiman and Friedman (1985 ). First we fit a simple linear model for 
reference purposes: 
> data(ozone) 
> olm <- 1m(03 ~ temp + ibh + ibt, ozone) 
> summary(olm) 
Coefficients: 
             Estimate Std. Error t value  Pr(>|t|) 
(Intercept) -7.727982   1.621662   -4.77 0.0000028 
temp         0.380441   0.040158    9.47   < 2e-16 
ibh         -0.001186   0.000257   -4.62 0.0000055 
ibt         -0.005821   0.010179   -0.57      0.57 
Residual standard error: 4.75 on 326 degrees of freedom 
Multiple R-Squared: 0.652,    Adjusted R-squared: 0.649 
F-statistic:  204 on 3 and 326 DF,  p-value: <2e-16 
Note that ibt is not significant in this model. One task among others in a regression analysis is to find the right transforms on the predictors. Additive models can help here. We fit an additive model using the a Gaussian response as the default. 
> library(gam) 
> amgam <- gam(03 ~ lo(temp) + lo(ibh) + lo(ibt), 
data=ozone) 
> summary(amgam) 
(Dispersion Parameter for gaussian family taken to be 
18.664) 
    Null Deviance: 21115 on 329 degrees of freedom 
Residual Deviance: 5935.1 on 318 degrees of freedom 
AIC: 1916.0 
Number of Local Scoring Iterations: 2 
DF for Terms and F-values for Nonparametric Effects 
             Df Npar Df Npar F    Pr(F) 
(Intercept) 1.0 
lo(temp)    1.0     2.5   7.45  0.00025 
lo(ibh)     1.0     2.9   7.62 0.000082 
lo(ibt)     1.0     2.7   7.84 0.000099 
We have used the loess smoother here by specifying lo in the model formula for all three predictors. Compared to the linear model, the R
2 is: 
> 1-5935.1/21115 
[1] 0.71892 Extending the linear model with R     256",0,False,0.25546145,False,0.29516324,False,0.2421587,True,0.32008928,False,0.14165187
"So the fit is a little better. However, the loess fit does use more degrees of freedom. We 
can compute the equivalent degrees of freedo m by an analogy to linear models. For linear 
smoothers, the relationship between the observed and fitted values may be written as 
ŷ=Py. The trace of P then estimates the effective number of parameters. For example, in 
linear regression, the projection matrix is X(XTX)−1XT whose trace is equal to the rank of 
X or the number of identifiable parameters. Th is notion can be used to obtain the degrees 
of freedom for additive models. 
The gam package uses a score test for the predictors. However, the p-values are only 
approximate at best and should be viewed with some skepticism. It is generally better to 
fit the model without the predictor of interest and then construct the F-test: 
> amgamr <- gam(03 ~ lo(temp) + lo(ibh) , data=ozone) 
> anova(amgamr,amgam,test=""F"") 
Analysis of Deviance Table 
Model 1:03 ~ lo(temp) + lo(ibh) 
Model 2:03 ~ lo(temp) + lo(ibh) + lo(ibt) 
  Resid. Df Resid. Dev     Df Deviance   F Pr(>F) 
1    321.67       6045 
2    318.00       5935   3.66      109 1.6   0.18 
Again the p-value is an approximation, but we can see there is some ev idence that ibt is 
not significant. We now examine the fit: 
> plot(amgam,residuals=TRUE,se=TRUE,pch=""."") 
 
Figure 12.1 Transformations on the 
predictors chosen by the gam fit on the 
ozone data. Partial residuals and approximate 95% pointwise 
confidence bands are shown.  
We see the transformations chosen in Figure 12.1. For ibt, a constant function would fit 
between the confidence bands. This reinforces the conclusion that this predictor is not 
significant. For temperat ure, we can see a change in the slope around 60°, while for ibh, 
there is a clear maximum. The partial residuals allow us to check for outliers or Additive models     257",0,True,0.37059408,False,0.2181955,False,0.20020142,False,0.17927818,False,0.27764094
"2    320.97       6054   2.70      896 17.6  8e-10 
 
Figure 12.2 Transformation functions 
for the model fit by mgcv. Note how the 
same scale has been deliberately used on all three plots. This allows us to easily compare the relative 
contribution of each variable.  
The p-value is only approximate, but it certainly  seems there really is a change in the 
trend. 
You can also do bivariate tr ansformations with mgcv. For example, suppose we 
suspect that there is an interaction between temperature and IBH. We can fit a model with 
this term: 
> amint <- gam(03 ~ s (temp, ibh)+s(ibt), data=ozone) 
> summary(amint) 
Parametric coefficients: 
              Estimate  std. err.    t 
ratio    Pr(>|t|) 
(Intercept)     11.776     0.2409      48.88    <2e-16 
Approximate significance of smooth terms: 
                   edf       chi.sq     p-value 
s(temp,ibh)      6.346       120.46     <2e-16 
     s(ibt)      2.917       36.081     1.57e-07 
R-sq. (adj) = 0.702   Deviance explained =   71% 
GCV score = 19.767   Scale est. = 19.152   n = 330 
We compare this to the previous additive model: 
> anova(ammgcv,amint,test=""F"") 
Analysis of Deviance Table 
Model 1:03 ~ s(temp) + s(ibh) + s(ibt) 
Model 2:03 ~ s(temp, ibh) + s(ibt) Additive models     259",0,False,0.27729905,True,0.3651021,False,0.20574282,False,0.22965518,False,0.19735706
"Resid. Df Resid. Dev      Df Deviance    F Pr(>F) 
1   319.327       5978 
2   319.737       6124  -0.409     -146 19.0 0.0014 
We see that the supposedly more complex model with the bivariate fit actually fits worse 
than the model with univariate functions. This  is because fewer degrees of freedom have 
been used to fit the bivariate function than the two corresponding univariate functions. In 
spite of the output p-value, we suspect that there is no interaction effect, because the 
fitting algorithm is able to fit the bivariate function so simply. We now graphically examine the fit as seen in Figure 12.3:  
> plot(amint) 
> vis.gam(amint,theta=-45,color=""gray”) 
 
Figure 12.3 The bivariate contour plot 
for temperature and ibh is shown in 
the left panel. The middle panel shows the univariate transformation on ibt while the right panel shows a perspective view of the information on 
the left panel.  
Given that the contours appear almost pa rallel and the perspective view looks like it 
could be constructed with piece of paper rippled in one direction, we conclude that there 
is no significant interaction. One interesting side effect is that ibt is now significant. 
One use for additive models is as an exploratory tool for standard parametric 
regression modeling. We can use the fitted functions to help us find suitable simple 
transformations of the predictors. One idea here is to model the temp and ibh effects using piecewise linear regression (also known as “broken stick” or segmented 
regression). We define the right and left “hockey-stick” functions: 
> rhs <- function (x,c) ifelse(x > c, x-c, 0) Extending the linear model with R     260",0,False,0.29888582,True,0.30694914,False,0.21916586,False,0.17438722,True,0.30494684
"> lhs <- function(x, c) ifelse(x < c, c-x, 0) 
and now fit a parametric model using cutpoints of 60 and 1000 for temp and ibh, 
respectively. We pick the cutpoints using the plots: 
> olm2 <- 1m(03 ~ 
rhs(temp,60)+lhs(temp,60)+rhs(ibh,1000)+lhs(ibh, 1000) 
, 
  ozone) 
> summary(olm2) 
Coefficients: 
                Estimate Std. Error t value Pr(>|t|) 
(Intercept)    11.603832   0.622651   18.64  < 2e-16 
rhs (temp, 60)  0.536441   0.033185   16.17  < 2e-16 
lhs (temp, 60) -0.116173   0.037866   -3.07   0.0023 
rhs (ibh, 1000)-0.001486   0.000198   -7.49  6.7e-13 
lhs (ibh, 1000)-0.003554   0.001314   -2.71   0.0072 
Residual standard error: 4.34 on 325 degrees of freedom  
Multiple R-Squared: 0.71,       Adjusted R-squared: 
0.706 
F-statistic:  199 on 4 and 325 degrees of 
freedom,      p-value:     0 
Compare this model to the first linear model we fit to this data. The fit is better and about 
as good as the additive model fit. It is unlikely we could have discovered these 
transformation without the help of the inte rmediate additive models. Furthermore, the 
linear model has the advantage that we can write the prediction formula in a compact 
form. 
We can use additive models for building a linear model as above, but they can be used 
for inference in their own right. For example,  we can predict new values with standard 
error: 
> predict(ammgcv,data.frame(temp=60,ibh=2000,ibt=100), 
se=T) 
$fit 
[1] 11.013 
$se.fit 
[1] 0.97278 
If we try to make predictions for predictor va lues outside the original range of the data, 
we will need to linearly extrapolate the spline  fits. This is dangerous for all the usual 
reasons: 
> predict(ammgcv,data.frame(temp=120,ibh=2000,ibt=100) , 
se=T) 
$fit 
[1] 35.511 
$se.fit 
[1] 5.7261 Additive models     261",0,True,0.32268238,True,0.3929149,False,0.1385718,False,0.29526758,False,0.19996285
"We see that the standard error is much larger although this likely does not fully reflect the 
uncertainty. 
We should also check the usual diagnostics: 
> plot (predict (ammgcv), residuals 
(ammgcv),xlab=""Predicted"",ylab=""Residuals”) 
> qqnorm (residuals (ammgcv), main="""") 
We can see in Figure 12.4 that although the residuals look normal, there is some nonconstant variance. 
Now let’s see the model for the full dataset. We found that the ibh and ibt terms were 
insignificant and so we removed them: 
> amred <- gam(03 ~ 
s(vh)+s(wind)+s(humidity)+s(temp)+s(dpg)+ 
  s(vis)+s(doy),data=ozone) 
> summary(amred) 
Approximate significance of smooth terms: 
                   edf       chi.sq     p-value 
      s(vh)          1       20.497     0.00000852 
    s (wind)         1       6.5571     0.0109 
s (humidity)         1       14.608     0.00016 
    s (temp)     5.769       87.825     7.36e-15 
      s(dpg)     3.312       59.782     1.26e-11 
     s (vis)     2.219       20.731     0.00006 
     s (doy)     4.074       106.69     <2e-16 
 
Figure 12.4 Residuals plots for the 
additive model.  
R-sq. (adj) = 0.793   Deviance explained = 80.5% 
GCV score = 14.113   Scale est. = 13.285    n = 330 Extending the linear model with R     262",0,False,0.17507644,True,0.3045984,False,0.14848498,False,0.15913963,False,0.12764369
"We will compare this to the results of differe nt modeling approaches that we will present 
later. We can see that we achieve a good fit with an R2 of 80.5%, but at the cost of using 
effectively 19.4 parameters including the intercept. 
Also for future reference, here is th e linear model with all insignificant terms 
removed: 
> alm <- 1m(03 ~ vis+doy+ibt+humidity+temp,data=ozone) 
Coefficients: 
             Estimate Std. Error t value Pr(>|t|) 
(Intercept) -10.01786    1.65306   -6.06  3.8e-09 
vis          -0.00820    0.00369   -2.22    0.027 
doy          -0.01020    0.00245   -4.17  3.9e-05 
ibt           0.03491    0.00671    5.21  3.4e-07 
humidity      0.08510    0.01435    5.93  7.7e-09 
temp          0.23281    0.03607    6.45  4.0e-10 
Residual standard error: 4.43 on 324 degrees of freedom 
Multiple R-Squared: 0.699,      Adjusted R-squared: 
0.694 
F-statistic:  150 on 5 and 324 DF,  p-value: <2e-16 
We can see that the fit is substantially worse,  but uses only six parameters. Of course, we 
may be able to improve this fit with some manual data analysis. We could look for good transformations and check for outliers and infl uential points. However, since we want to 
compare different modeling techniques, we want to avoid making subjective 
interventions for the sake of a fair comparison.  
12.3 Generalized Additive Models  
In generalized linear models: 
   
The approach is readily extended to additive models to form generalized additive models 
(GAM). The fitting process is different in the mgcv and gam packages. The mgcv 
package takes a likelihood approach, so the implementation of the fitting algorithm is conceptually straightforward. The gam package uses a backfitting approach as described 
below. 
Recalling the GLM fitting method described in Section 6.2, the iterative reweighted 
least squares (IRWLS) fitting algorith m starts from some reasonable µ
0, forms the 
“adjusted dependent variate” 
 and weights 
It then regresses z on X using weights w using weighted least 
squares to get 
 The process is repeated until convergence. 
In generalized additive models the linear predictor becomes: Additive models     263",0,True,0.30441836,True,0.32486215,False,0.1842866,True,0.31224394,False,0.16424666
"and we just add an iteration step to estimate the fjs. There are two levels of iteration: the 
GLM part where z and w are computed and the additive model part. We need to use a 
smoother that understands weights like loess or splines. 
The ozone data has a response with relatively small integer values. Furthermore, the 
diagnostic plot in Figure 12.4 shows nonconstant variance. This suggests that a Poisson 
response might be suitable. We fit this using the mgcv package: 
> gammgcv <- gam(03 ~ 
s(temp)+s(ibh)+s(ibt),family=poisson, 
  scale=-1,data=ozone) 
> summary(gammgcv) 
Parametric coefficients: 
              Estimate  std. err.    t 
ratio    Pr(>|t|) 
(Intercept)     2.2927    0.02304      99.51    <2e-16 
Approximate significance of smooth terms: 
               edf       chi.sq     p-value 
s(temp)      3.803       79.802     8.19e-15 
s(ibh)      3.779       48.471     2.59e-09 
s(ibt)      1.422      0.94684     0.465 
R-sq. (adj) = 0.712   Deviance explained = 72.9% 
GCV score = 1.5025   Scale est. = 1.4569    n=330 
We have set scale= −1 because negative values for th is parameter indicate that the 
dispersion should be estimated rather than fixed at one. Since we do not truly believe the 
response is Poisson, it seems wise to allow for overdispersion. The default of not specifying scale would fi x the dispersion at one. We see that the estimated dispersion is 
indeed somewhat bigger than one. We see th at IBT is not significa nt. We can check the 
transformations on the predictors as seen in Figure 12.5:  
> plot (gammgcv, residuals=TRUE) 
 
Figure 12.5 Transformation on the 
predictors for the Poisson GAM.  Extending the linear model with R     264",0,False,0.21545999,True,0.3269185,False,0.23389632,False,0.21685952,False,0.18800077
"We see that the selected transformations are quite similar to those observed previously. 
12.4 Alternating Conditional Expectations  
In the additive model: 
   
but in the transform both sides (TBS) model: 
   
For example, 
 cannot be modeled well by additive models, but can if we 
transform both sides: 
 This fits within the transform-both-sides (TBS) 
model framework. A more complicated a lternative approach would be nonlinear 
regression. One particular way of fitting TBS models is alternating conditional 
expectation  (ACE) which is designed to minimize Σi(θ(yi)−Σfj(xij))2. Distractingly, this 
can be trivially minimized by setting θ=fj=0=0 for all j. To avoid this solution, we impose 
the restriction that the variance of θ(y) be one. The fitting pro ceeds using the following 
algorithm: 
1. Initialize: 
   
2. Cycle:  
   
   
Renormalize at the end of each cycle: 
   
We repeat until convergence. ACE is compar able to the additive model, except now we 
allow transformation of the response as well.  In principle, you can use any reasonable 
smoother S, but the original smoother used was the supersmoother. This cannot be easily 
changed in the R software implementation. 
For our example, we start with the same three predictors in the ozone data: Additive models     265",0,False,0.2107723,True,0.30944726,False,0.2196073,False,0.28153476,False,0.06974764
"> x <- ozone[, c(""temp”, “ibh”, “ibt"")] 
> library(acepack) 
> acefit <- ace (x, ozone$03) 
Note that the ace function interface is quite rudimentary as we must give it the X matrix 
explicitly. The function returns the components ty which contains θ(y) and tx which is a 
matrix whose columns contain the fj(xj). We can get a sense of how well these 
transformations work by fitting a linear mode l that uses the transformed variables: 
> summary(lm(acefit$ty ~ acefit$tx)) 
Coefficients: 
                 Estimate Std. Error t value Pr(>|t|) 
(Intercept)       9.2e-18     0.0290 3.2e-16   1.0000 
acefit$txtemp      0.9676     0.0509   19.01  < 2e-16 
acefit$txibh       1.1801     0.1360    8.68  2.2e-16 
acefit$txibt       1.3712     0.5123    2.68   0.0078 
Residual standard error: 0.527 on 326 degrees of 
freedom 
Multiple R-Squared: 0.726,      Adjusted R-squared: 
0.723 
F-statistic:  288 on 3 and 326 degrees of 
freedom,      p-value:    0 
All three transformed predictors  are strongly significant and the fit is superior to the 
original model. The R2 for the comparable additive model was 0.703. So the additional 
transformation of the response did improve the fit. Now we examine the transforms on 
the response and the three predictors: 
> plot (ozone$03,acefit$ty,xlab=""03"", 
  ylab=expression(theta(03))) 
> plot(x[,1],acefit$tx[,1],xlab=""temp"",ylab=""f(temp)"") 
> plot (x[,2],acefit$tx[,2],xlab=""ibh"",ylab=""f(ibh)"") 
> plot(x[,3],acefit$tx[,3],xlab=""ibt"",ylab=""f(ibt)"") 
See Figure 12.6. The transform on the response is close to, but not quite, linear. The transformations on temp and ibh are similar to those found by the additive model. The 
transformation for ibt looks implausibly rough in some parts. 
Now let’s see how we do on the full data: 
> x <- ozone[,-1] 
> acefit <- ace(x,ozone$03) Extending the linear model with R     266",0,False,0.23276503,True,0.34683934,False,0.08572522,True,0.31242517,False,0.12218866
"Figure 12.6 ACE transformations: the 
first panel shows the transformation on 
the response while the remaining three show the transformations on the 
predictors.  
>summary(lm(acefit$ty ~ acefit$tx)] 
Coefficients: 
                   Estimate Std. Error  t value Pr(>|t| 
(Intercept)       -5.8e-17     0.0225 -2.6e-15   1.0000 
acefit$txvh        1.1715      0.3852     3.04   0.0026 
acefit$txwind      1.0739      0.4047     2.65   0.0084 
acefit$txhumidity  0.6515      0.2455     2.65   0.0084 
acefit$txtemp      0.9163      0.1236     7.41  1.le-12 
acefit$txibh       1.3510      0.4370     3.09   0.0022 
acefit$txdpg       1.3217      0.1672     7.91  4.4e-14 
acefit$txibt       0.9256      0.1967     4.70  3.8e-06 
acefit$txvis       1.3864      0.2303     6.02  4.8e-09 
acefit$txdoy       1.2837      0.1097    11.70  < 2e-16 
Residual standard error: 0.409 on 320 degrees of 
freedom 
Multiple R-Squared: 0.838,      Adjusted R-squared: 
0.833 
F-statistic:  184 on 9 and 320 degrees of 
freedom,      p-value:    0 
A very good fit, but we must be cautious. Notice that all the predictors are strongly 
significant. This might be a reflection of reality or it could just be that the ACE model is overfitting the data by using implausible transformations as seen on the ibt variable 
above. 
ACE can be useful in searching for good transformations while building a linear 
model. We might examine the fitted transformations as seen in Figure 12.6 to suggest 
appropriate parametric forms. More caution is n ecessary if the model is to be used in its 
own right, because of the tendency to overfit. 
An alternative view of ACE is to  consider the problem of choosing θ and f
j’s such that 
θ(Y) and Σjfj(Xj) are maximally correlated. ACE solves this problem. For this reason, 
ACE can be viewed as a correlation met hod rather than a regression method. Additive models     267",0,False,0.24491683,True,0.40449905,False,0.20789745,True,0.34149256,False,0.1965902
"Here is how the method of fitting θ works: Suppose Var(Y)≡V(Y) is not constant. We 
transform to constancy by:  
   
We use data to estimate V(y),  then get θ. The purpose of the AVAS method is to obtain 
additivity and variance stabilization and not n ecessarily to produce the best possible fit. 
We demonstrate its application on the ozone data: 
> avasfit <- avas(x,ozone$03) 
Plot the transformations selected: 
> plot (ozone$03, avasfit$ty, 
xlab=""03"",ylab=expression(theta(03))) 
> plot(x[,1],avasfit$tx[,1],xlab=""temp"",ylab=""f(temp)"") 
> plot(x[,2],avasfit$tx[,2],xlab=""ibh"",ylab=""f(ibh)"") 
> plot(x[,3],avasfit$tx[,3],xlab=""ibt"",ylab=""f(ibt)"") 
 
Figure 12.7 AVAS transformations—
the first panel shows the 
transformation on the response while the remaining three show the 
transformations on the predictors.  
See Figure 12.7. It would be convenient if the transformation on the response matched a 
simple functional form. We see if this is possible. We need to sort the response to get the 
line plots to work: 
> i <- order(ozone$03) 
> 
plot(ozone$03[i],avasfit$ty[i],type=""1"",xlab=""03"",ylab=
expression(theta(03)] 
> gs <- lm (avasfit$ty[i] ~ sqrt (ozone$03[i])) 
> lines(ozone$03[i],gs$fit,lty=2) 
> gl <- lm(avasfit$ty[i] ~ log(ozone$03[i])) 
> lines (ozone$03[i], gl$fit, lty=5) Additive models     269",0,False,0.2823866,True,0.30744419,False,0.09365698,False,0.23588108,False,0.02592824
"See the;eft panel of Figure 12.8. We have shown the square-root fit as a dotted line and 
log fit as a dashed line. Neither one fits well across the whole range. Now look at the 
overall fit: 
> lmod <- lm (avasfit$ty avasfit$tx) 
> summary(lmod) 
Coefficients: 
                  Estimate Std Error t value Pr(>|t|) 
(Intercept)       2.87e-07   3.10e-02 9.3e-06    1.000 
avasfit$txtemp    9.02e-01   7.50e-02   12.02  < 2e-16 
avasfit$txibh     7.98e-01   1.09e-01    7.33  1.8e-12 
avasfit$txibt     5.69e-01   2.39e-01    2.38    0.018 
Residual standard error: 0.563 on 326 degrees of 
freedom 
Multiple R-Squared: 0.687,      Adjusted R-squared: 
0.684 
F-statistic:  238 on 3 and 326 degrees of 
freedom,      p-value:    0 
 
Figure 12.8 The left panel checks for 
simple fits to th e AVAS transformation 
on the response given by the solid line. The log fit is given by the dashed line while the square-root fit is given by the 
dotted line. The right  panel shows the 
residuals νs. fitted values plot for the 
AVAS model.  
The fit is not so good, but check the diagnostics: 
> plot (predict (lmod), residuals 
(lmod),xlab=""Fitted"",ylab=""Residuals”) Extending the linear model with R     270",0,True,0.32759297,True,0.33565447,False,0.14567764,False,0.2793888,False,0.24723707
"The plot is shown in the right panel of Figure 12.8. 
AVAS does not optimize the fit; it trades some of the optimality in order to obtain 
constant variance. Whether this is a good trade depends on how much relative value you 
put on the accuracy of point predictions and accu rate estimation of the standard error of 
prediction. In other words, is it more important to try to be right or to know how much 
you are wrong? The choice will depend on the application. 
12.6 Generalized Additive Mixed Models  
The generalized additive mixed model (GAMM) manages to combine the three major 
themes of this book. The response can be nonnormal from the exponential family of 
distributions. The error structure can allow for grouping and hierarchical arrangements in the data. Finally we can allow for smooth transformations of the response. We 
demonstrate this method on the epilepsy data from Section 10.2: 
> data(epilepsy) 
> egamm <- gamm(seizures ~ treat*expind+s(age), 
family=poisson, 
  random=list(id=~1),data=epilepsy,subset=(id!=49)) 
> summary(egamm$gam) 
Parametric coefficients: 
               Estimate std. err.    t 
ratio    Pr(>|t|) 
(Intercept)     3.1607     0.1435     22.02    <2e-16 
       treat  -0.010368     0.2001   -0.05182    0.959 
      expind    -1.2745    0.07574     -16.83    <2e-16 
treat:expind   -0.30238     0.1126     -
2.684    0.00769 
Approximate significance of smooth terms: 
              edf        chi.sq    p-value 
s(age)      1.014       0.30698    0.586 
R-sq. (adj) = 0.328  Scale est. = 2.5754   n = 290 
We see that the age effect is not significan t. Again the interaction effect is significant 
which shows, in this case, a beneficial effect  for the drug. We would like to use an offset 
here for compatibility with the previous analysis. 
12.7 Multivariate Adaptive Regression Splines  
Multivariate adaptive regression splines (MARS)  were introduced by Friedman (1991). 
We wish to find a model of the form: 
   Additive models     271",2,False,0.1651954,True,0.31734592,True,0.30820262,False,0.23411228,False,0.15688156
"a$x10 -0.13890    0.02302   -6.03  4.4e-09 
a$x11 -0.55358    0.17375   -3.19  0.00159 
a$x12  0.02970    0.01074    2.77  0.00602 
Residual standard error: 3.64 on 318 degrees of freedom 
Multiple R-Squared: 0.937,      Adjusted R-squared: 
0.935 
F-statistic:  395 on 12 and 318 degrees of 
freedom,     p-value:    0 
The fit is very good in terms of R2, but the model size is also larger. It is also an additive 
model, so we can reasonably compare it to the additive model presented at the end of 
Section 12.2. That model had an adjusted R2 of 79.3% using 19.4 parameters. 
Let’s reduce the model size to that used for previous models. The parameter nk 
controls the maximum number of model terms: 
> a <- mars(ozone[,-1],ozone[,1],nk=7) 
> summary(lm(ozone[,1] ~ a$x-1)) 
Coefficients: 
      Estimate Std. Error t value Pr (>|t|) 
a$x1 12.663482   0.751400   16.85  < 2e-16 
a$x2  0.483948   0.029735   16.28  < 2e-16 
a$x3 -0.096484   0.043102   -2.24    0.026 
a$x4 -0.001420   0.000199   -7.13  6.8e-12 
a$x5 -0.002100   0.001086   -1.93    0.054 
a$x6 -0.012421   0.002784   -4.46  1.le-05 
a$x7 -0.108042   0.020666   -5.23  3.le-07 
Residual standard error: 4.16 on 323 degrees of freedom 
Multiple R-Squared: 0.916,      Adjusted R-squared: 
0.915 
F-statistic: 506 on 7 and 323 degrees of 
freedom,        p-value:    0 
This fit is worse, but remember we are disallowing any interaction terms. Now let’s allow 
second-order (two-way) inter action terms, nk was chosen to get the same model size as 
before:  
> a <- mars(ozone[,-1],ozone[,1],nk=10,degree=2) 
> summary(lm(ozone[, 1] ~ a$x-1)) 
Coefficients: 
      Estimate Std. Error t value Pr(>|t|) 
a$x1 12.090698   0.647896  18.66  < 2e-16 
a$x2  0.574349   0.031756   18.09  < 2e-16 
a$x3 -0.119057   0.041274   -2.88   0.0042 
a$x4 -0.001149   0.000163   -7.05  1.le-11 
a$x5 -0.008251   0.001380   -5.98  6.0e-09 
a$x6 -0.012828   0.002656   -4.83  2.le-06 
a$x7 -0.102334   0.019730   -5.19  3.8e-07 
Residual standard error: 3.97 on 323 degrees of freedom 
Multiple R-Squared: 0.924,        Adjusted R-squared: 
0.922 Additive models     273",0,True,0.3760725,True,0.32116216,False,0.20081888,False,0.27430588,False,0.22532716
"> 
contour(humidity,temp,zm,xlab=""Humidity"",ylab=""Temperat
ure”) 
> persp (humidity, temp, zm, 
xlab=""Humidity"",ylab=""Temperature”, 
  zlab=""Ozone"",theta=-30) 
Now check the diagnostics: 
> qqnorm (a$res, main="""") 
> plot (a$fit, a$res, xlab=""Fitted”, ylab=""Residuals”) 
These plots show no problem with normality, but some indication of nonconstant 
variance. See the bottom two panels of Figure 12.9. 
It is interesting to compare the MARS approach to the univariate version as 
demonstrated in Figure 11.7. There we used a moderate number of knots in just one 
dimension while MARS gets by with just a few knots in higher dimensions. The key is to choose the right knots. MARS can be favora bly compared to linear regression: it has 
additional flexibility to find nonlinearity in the predictors in higher dimensions. MARS 
can also be favorably compared to the tree method discussed in the next chapter: it allows for continuous fits but still maintains good interpretability. 
Further Reading:  Hastie and Tibshirani (1990) provide the original overview of 
additive modeling, while Wood (2006) gives a more recent introduction. Gu (2002) presents another approach to the problem. Green and Silverman (1993) show the link to 
GLMs. Hastie, Tibshirani, and Fr iedman (2001) discuss additive models as part of larger 
review and compare them to competitive methods. 
Exercises  
1. The fat data gives percentage of body fat, age, weight, height, and 10 body 
circumference measurements, such as the abdomen, are recorded for 252 men. Body 
fat is estimated through an underwater weighing technique, but this is inconvenient to 
use widely. Develop an additive model that allows the estimation of body fat for men 
using only a scale and a measuring tape. Your model should predict %body fat 
according to Siri. You may not use Brozek’s %body fat, density or fat free weight as 
predictors. 
2. Find a good model for volume in terms of girth and height using the trees data. We 
might expect that Volume=c* Height * Girth2 suggesting a logarithmic transformation 
on all variables to achieve a linear mode l. What models do the ACE and AVAS 
procedures suggest for this data? 
3. Refer to the pima dataset described in Question 3 of Chapter 2. First take care to deal 
with the clearly mistaken obse rvations for some variables. Additive models     275",0,False,0.22699645,True,0.31328657,False,0.07253934,False,0.22779554,False,0.115213044
"4.The dvisits data comes from the Australian Health Survey of 1977–78 and consist of 
5190 single adults where young and old have been oversampled. 
(a) Build a generalized additive model with doctorco as the response and sex, age, 
agesq, income, levyplus, freepoor, freerepa, illness, actdays, hscore, chcond1 and 
chcond2 as possible predictor variables. Se lect an appropriate size for your model. 
(b) Check the diagnostics. 
(c) What sort of person would be predicted to visit the doctor the most under your 
selected model? 
(d) For the last person in the dataset, compute the predicted probability distribution for 
their visits to the doctor, i.e., give the probability they visit 0,1, 2 etc. times. 
(e) If you have previously completed the analysis of this data in the exercises for 
Chapter 3, compare the results. 
5. Use the additive model approach to reanalyze the mot or ins data introduced in Section 
7.1. For compatibility with the previous analysis, restrict yourself to data from zone 
one. Try both a Gaussian additive model with a logged response and a gamma GAM 
for the untransformed response. Compare your analysis to the gamma GLM analysis 
in the text. 
6. The ethanol dataset in the lattice package presents data from ethanol fuel burned in a 
single-cylinder engine. The emissions of nitrogen oxides should be considered as the 
response and engine compression and equivalence ratio as the predictors. Study the 
example plots given on the help page for ethanol that reveal the relationship between 
the variables. 
• Apply the additive model approach to the data. 
• Try the MARS approach. 
Did either approach reveal the structure that seems apparent in the plots? Additive models     277",2,False,0.16920732,False,0.22762862,True,0.38130617,False,0.25093397,False,0.2568761
"CHAPTER 13  
Trees  
13.1 Regression Trees  
Regression trees are similar to additive models in that they represent a compromise 
between the linear model and the completely nonparametric approach. Tree methodology 
has roots in both the statistics and computer  science literature. A precursor to current 
methodology was CHAID developed by Morgan and Sonquist (1963) although the book 
by Breiman, Friedman, Olshen, and Stone (1984)  introduced the main ideas to statistics. 
Concurrently, tree methodology was developed in machine learning starting in the 1970s—see Quinlan (1993) for an overview. 
Most statistical work starts from the specifi cation of a model. The model says how we 
believe the data is generated and contains both a systematic and a random component. The model is not completely specified and so we use the data to select a particular model 
by either estimating parameters or perhaps by fitting functions, as in our recent 
nonparametric approaches. Clearly  this strategy has been effective in a wide range of 
situations. However, the insistence on specifyi ng a model, right from the start, does limit 
statistics. It is often difficult to specify a m odel, particularly for larger and more complex 
datasets. Furthermore, it is often impractical to develop inferential methods for more complex statistical models. 
Tukey (1977) advocated exploratory data analysis (EDA) in his book. Graphical and 
descriptive statistics can sometimes make the me ssage of the data very clear or at least 
suggest a suitable form for the model. However, EDA is not a complete solution and 
sometimes we need definite predictions or conclusions. 
Regression trees are an example of a statistical method that is best described by the 
algorithm used in their construction. One can uncover the implicit model underlying 
regression trees, but the algorithm is the tr ue starting point. Any method of analysis 
should ultimately be judged on whether it su ccessfully predicts or explains something. 
Statistical models may achieve this, but algorithmically based methods are also competitive. The distinction between algo rithm based and model-based methods is 
discussed in Breiman (2001b). In the computer science literature, tree methodology has been applied to decision tree problems wher e there is no stochastic structure and we 
simply want to build a rule for making the correct decision. 
We use the recursive partitioning regression algorithm: 
1. Consider all partitions of the region of  the predictors into two regions where the 
division is parallel to one of the axes. In other words, we partition a single predictor by 
choosing a point along the range of that predictor to make the split. It does not matter 
exactly where we make the split between two adjacent points so there will be at most 
(n−1)p partitions to consider.",3,False,0.055981014,False,0.11138949,False,0.24569899,True,0.5452999,False,0.14943185
"> data(ozone) 
> summary(ozone) 
> pairs(ozone,pch=""."") 
The plots (not shown) reveal several nonlinear relationships indicating that a linear 
regression might not be appropriate without the addition of some transformations. Now 
fit a tree: 
> library(rpart) 
> (roz <- rpart(03 ~ .,ozone)) 
n= 330 
node), split, n, deviance, Yval 
     * denotes terminal node 
1) root 330 21115.00 11.7760 
   2) temp< 67.5 214   4114.30 7.4252 
     4) ibh>=3573.5 108   689.63 5.1481 * 
     5) ibh< 3573.5 106  2294.10 9.7453 
      10) dpg< -9.5 35   362.69 6.4571 * 
      11) dpg>= −9.5 71  1366.50 11.3660 
        22) ibt< 159 40   287.90 9.0500 * 
        23) ibt>=159 31   587.10 14.3550 * 
   3) temp>=67.5 116  5478.40 19.8020 
     6) ibt< 226.5 55  1276.80 15.9450 
      12) humidity< 59.5 10   167.60 10.8000 * 
      13) humidity>=59.5 45   785.64 17.0890 * 
     7) ibt>=226.5 61  2646.30 23.2790 
      14) doy>=306.5 8   398.00 16.0000 * 
      15) doy< 306.5 53  1760.50 24.3770 
        30) vis>=55 36  1149.90 22.9440 * 
        31) vis< 55 17   380.12 27.4120 * 
We see that the first split (nodes 2 and 3) is on temperature, 214 observations have 
temperatures less than 67.5 w ith a mean response value of 7.4, whereas 116 observations 
have temperatures greater than 67.5 with a mean response value of 20. The total RSS has 
been reduced from 21,000 to 4100+5500=9600. Although the relevant information can be gleaned from the text-based output, a graphical display is nicer as in Figure 13.1. In the 
first version of the plot, the depth of the branches is proportional to the reduction in error 
due to the split. The disadvantage is that the labels can be hard to read in lower parts of the tree where the reduc tion in error is much smaller. The second version of the plot uses 
a uniform spacing to allow more room for labeling: 
> plot(roz,margin=.10) 
> text(roz) 
> plot(roz,compress=T,uniform=T,branch=0.4,margin=.10) 
> text(roz) 
We see that the first split on temperature produces a large reduction in the RSS. Some of the subsequent splits do not do much. The immediate message is that high  Extending the linear model with R     280",4,False,0.19677141,True,0.3699758,False,0.12814379,True,0.5236163,False,0.15690655
"Figure 13.1 Tree model for the ozone 
data. On the left, the depth of the branches is proportional to the improvement in fit. On the right, the depth is held constant to improve readability. If the logical condition at a node is true, follow the branch to the 
left. 
temperatures are associated with high ozon e levels. A regression tree is a regression 
model, so diagnostics are called for: 
> plot(predict(roz), residuals(roz), 
xlab=""Fitted"",ylab=""Residuals”) 
> qqnorm(residuals(roz)) 
See Figure 13.2. There are no visible problems here. If nonconstant variance is observed, 
one might consider transforming the respons e. Trees are also somewhat sensitive to 
outliers as they are based on local means. Outliers may be observed in the QQ plot, but, 
as with linear models, they ma y conceal themselves and be influential on the fit. Suppose 
we wanted to predict the response for a ne w value—for example the median value in the 
dataset:  
> (x0 <- apply(ozone[,-1],2,median)) 
      vh     wind 
humidity     temp     ibh     dpg     ibt     vis 
   5760.0     5.0     64.0     62.0  2112.5    24.0   1
67.5   120.0 
      doy 
    205.5 
> predict (roz, data.frame(t(x0))) 
     1 
14.355 Trees     281",0,False,0.15924615,True,0.34079844,False,0.17018132,True,0.4421379,False,0.1448954
"You should be able to verify this prediction by following the splits down through the tree 
shown in Figure 13.1. 
 
Figure 13.2 Residuals and fitted values 
for the tree model of the Ozone data 
are shown in the left panel. A QQ plot of the residuals is shown in the right 
panel.  
13.2 Tree Pruning  
The recursive partitioning algorithm describes how to grow the tree, but what is the 
optimal size for the tree? The default form of rpart does restrict the size of the tree, but 
some intervention is probably necessary to select the best tree size. 
One possibility, called a greedy strategy,  is to keep partitioning until the reduction in 
overall cost (RSS for this type of tree) is not reduced by more than ε. However, it is 
difficult to set ε in a sensible way. Furthermore, a greedy strategy may stop too soon. For 
example, consider data laid out in Ta ble 13.1: Neither the horizontal nor  
x2 1 2 
  2 1 
    x1 
Table 13.1 There are four data points arranged in a 
square. The number shows the value of y at that 
point.  
the vertical split will improve the fit at all. Both splits are required to get a better fit. 
However, this drawback is common to most tr ee-growing strategies as looking more than 
one step ahead greatly increases the numb er of splits that must be considered. Extending the linear model with R     282",3,False,0.14182046,False,0.11923055,False,0.103458256,True,0.49023455,False,0.11603519
"Nevertheless, it does illustrate the point that the incremental improvements due to each 
expansion of the tree may not necessarily be always decreasing. 
The observed RSS for a tree will be an underestimate of how well the tree will make 
predictions. This phenomenon is common to most models. One generic method of obtaining a better estimate of predictive ability is cross-validation (CV). 
For a given tree, leave out one observation,  recalculate the tree and use that tree to 
predict the left-out observation. For regression, this criterion would be: 
   
where 
 denotes the predicted value of the tree given the input xj when case j is not 
used in the construction of the tree. For other types of tree, a different criterion would be 
used. For classification problems, it might be the deviance. CV is a more realistic 
estimate of how the tree will perform in pr actice. Leave-out-one cross-validation is 
computationally expensive for trees so usually k-fold cross-validation is used. The data is 
randomly divided into k roughly equal parts and the remainder is used to predict those 
left out. As well as being less expensive computationally than the full leave-out-one 
method, it may even work bette r. One drawback is that th e partition is random so that 
repeating the method will give different numerical results. 
However, there may be very many possible trees if we consider all subsets of a large 
tree; cross-validation would just be too expensive. We need a method to reduce the set of trees to be considered to just those that are worth considering. This is where cost-
complexity pruning is useful. We define a cost-complexity function for trees: 
   
If λ is large, then the tree that minimizes this  cost will be small and vice versa. We can 
determine the best tree of any given size by growing a large tree and then pruning it back. Given a tree of size n, we can determine the best tree of size n−1 by considering all the 
possible ways of combining adjacent nodes. We pick the one that increases the fit 
criterion by the least amount. The strategy is  akin to backward elimination in linear 
regression variable selection except that it can be shown that it generates the optimal 
sequence of trees of a given size. 
We now use cross-validation to select from this sequence of trees. By default, rpart 
selects a tree size that may not be large enough to include all those trees we might want to 
consider. We force it to cons ider a larger tree and then examine the cross-validation 
criterion for all the subtrees. The paramete r cp plays a similar role to the smoothing 
parameter in nonparametric regression and is defined as the ratio of λ to the RSS of the 
root tree (a tree with no branches). When  we call rpart initially, it computes the whole 
sequence of trees and we merely need to use functions like printcp to examine the 
intermediate possibilities: 
> roze <- rpart(03 ~ .,ozone,cp=0.001) 
> printcp(roze) 
        CP nsplit rel error xerror   xstd Trees     283",3,False,0.036296427,False,0.021592356,False,0.09706361,True,0.48396003,False,0.09776139
"You can get some fancier output by: 
> post (roz, filename="""") 
If you do not specify the filename, nothing w ill appear on-screen, but you will find a file 
called roz.ps in the directory from which you started R. See Figure 13.4: Let’s compare 
the result to the earlier linear regression. We achieved an R2 of about 70% using only six 
parameters in the previous chapter. We ca n select a tree with five splits and hence 
effectively six parameters and compare them: 
> rozr <- prune.rpart(roz,0.0154) 
> 1-sum(residuals(rozr)^2)/sum((ozone$03-
mean(ozone$03))^2) 
[1] 0.74603 
We see that the tree model achieved a better fit than the equivalent linear model. Of 
course, it would be a mistake to generalize from this, but it is a good demonstration of the 
value of trees. A tree fit is piecewise consta nt over the regions defined by the partitions, 
so one might not expect a par ticularly good fit. However, we can see from this example 
that it can outperform  linear regression. 
13.3 Classification Trees  
Trees can be used for several different types of response data. For the regression tree, we 
computed the mean within each partition. This is just the null model for a regression. We can extend the tree method to other types of response by fitting an appropriate null model 
on each partition. For example, we can extend  the idea to binomial, multinomial, Poisson 
and survival data by using a deviance,  instead of the RSS, as a criterion. 
Classification trees work similarly to re gression trees except the residual sum of 
squares is no longer a suitable criterion for splitting the nodes. The splits should divide 
the observations within a node so that the class types within a split are mostly of one kind (or failing that, just few kinds). We can measure the purity  of the node with several 
possible measures. Let n
ik be the number of observations of type k within terminal node i 
and pik be the observed proportion of type k within node i. Let Di be the measure for node 
i so that the total measure is ΣDi. There are several choices for Di: 
 Trees     285",3,False,0.19583294,False,0.27562904,False,0.15257785,True,0.5868634,False,0.21257678
"Figure 13.4 Final tree model for the 
ozone data.  
1. Deviance: 
   
 
  Extending the linear model with R     286",3,False,0.1770323,False,0.2756062,False,0.24715547,True,0.45665663,True,0.3079633
"2. Entropy: 
   
3. Gini index: 
   
All these measures share the characteristic th at they are minimized when all members of 
the node are of the same type. The rpart function uses the Gini index by default. 
We illustrate the classification tree method in  a problem involving the identification of 
the sex and species of an historical specimen of kangaroo. We have some training data 
consisting of 148 cases with the following variables: there are three possible species, 
Giganteus, Melanops  and Fuliginosus,  the sex of the animal and 18 skull measurements. 
The data were published in Andrews and Herzberg (1985). The historical specimen is 
from the Rijksmuseum van Natuurlijkee in Leiden which had the following skull 
measurements in the same order as in the data: 
1115 NA 748 182 NA NA 178 311 756 226 NA NA NA 48 1009 
NA 204 593 
We have a choice in how we model the response. One possibility is to form a six-level response representing all possible combinations of sex and species. Another approach is to form separate trees for identifying the sex and the species. We take the latter approach 
below, focusing on the the species. This choi ce is motivated by the belief that different 
features are likely to discriminate the sex and the species so that attempting to model 
them both in the same tree might result in a larger, more complex tree that might be less 
powerful than two smaller trees. Even so, it would be worth trying the first approach 
although we shall not do so here. We start by reading in and specifying the museum case: 
> data(kanga) 
> x0 <- c(1115, NA, 748, 182, NA, NA, 178, 311, 756, 
226, NA, NA, NA, 48, 1009, NA, 204, 593) 
We have missing values for the case to be classified. We have two options. We can build a tree model that will classify if there are missing values in the input or we can build a 
tree model that uses only variables that ar e observed. If we believe that the missing 
values were in some way informative, the first choice would be fine. In this particular 
case, that does not seem plausible, so the latter approach is preferred. However, if we want to build a model that could be used for future unspecified cases, then we would 
have to deal directly with the missing values. For this special purpose situation, where we 
want to classify one particular kangaroo, this is not a concern. 
We exclude all variables that are missing in the test case. We drop sex since we will 
not be modeling it yet. We form a convenient data frame: 
> kanga <- kanga[,c (T, F, ! is.na (x0))] 
> kanga[1:2,] Trees     287",1,False,0.025704747,False,0.07376518,False,0.18343334,True,0.33871928,False,0.12924191
"species basilar.length palate.length palate.width 
squamosal.depth 
1 
giganteus           1312           882           NA    
         180 
2 
giganteus           1439           985           230   
         150 
  lacrymal.width zygomatic.width orbital.width 
foramina.length 
1            394            782            249         
     88 
2            416            824            233         
    100 
mandible.length mandible.depth ramus.height 
           10861           179          591 
           11582           181          643 
We still have missing values in the training set. We have a number of options: 
1. Build a tree model that discretizes the predictors into factors and then treats missing 
values as another level of the factors. This might be appropriate if we think missing 
values are informative in some way. Information would be lost in the discretization. 
For this data, we have no reason to believe that the data is not missing at random and 
furthermore we have already decided to ignore the missing values in the test case. 
2. Fill in or estimate the missing values and then build a tree. We could use missing data 
fill-in methods as used in other regression problems. This is not easy to implement and 
there are concerns about the bias caused by such methods. 
3. The tree-fitting algorithm can handle missing values naturally. If a value for some case 
is not available, then it is simply excluded from the criterion. When we want to 
classify a new case with missing values, we  follow the tree down until we reach a split 
which involves a missing value in our new case  and take the majority verdict in that 
node. A more complicated approach is to allow a second-choice variable for splitting 
at a node called a surrogate split . Information on the surrogate splits may be obtained 
by using the summary command on the tree object. 
4. Leave out the missing cases entirely. 
We first check where the missing values occur:  
> apply(kanga,2,function(x) sum(is.na(x))) 
        species basilar.length palate.length 
palate.width 
              0              1             1           
24 
squamosal.depth lacrymal.width zygomatic.width 
orbital.width 
              1              0               1         
    0 
foramina.length mandible.length mandible.depth 
ramus.height Extending the linear model with R     288",4,False,0.11777204,False,0.15124898,False,0.17162316,True,0.30782086,False,0.15764391
"Figure 13.5 Historical kangaroo tree 
model. The left panel shows the three species, m=melanops, g=giganteus and f=fuliginosus, as they vary with two of the measurements. The right 
panel shows the chosen tree.  
is a factor, classification rather than regressi on is automatically used. Gini’s index is the 
default choice of criterion. Here we specify a smaller value of the complexity parameter 
cp than the default, so that la rger trees are also considered: 
> kt <- rpart(species ~ ., data=newko,cp=0.001) 
> printcp(kt) 
Root node error: 95/144 =0.66 
n= 144 
      CP nsplit rel error xerror   xstd 
1 0.1789      0     1.000  1.105 0.0561 
2 0.1053      1     0.821  0.979 0.0604 
3 0.0500      2     0.716  0.874 0.0624 
4 0.0211      6     0.516  0.800 0.0631 
5 0.0105      7     0.495  0.853 0.0627 
6 0.0010      8     0.484  0.905 0.0620 
The cross-validated error (expressed in rela tive terms in the rel er ror column) reaches a 
minimum for the six-split tree. We select this tree: 
> ktp <- prune(kt,cp=0.0211) 
> ktp 
n= 144 
node), split, n, loss, Yval, (Yprob) 
      * denotes terminal node 
1) root 144 95 fuliginosus (0.340278 0.333333 0.326389) 
  2) zygomatic.w>=923 37 13 fuliginosus (0.648649 
0.162162 0.189189) Extending the linear model with R     290",3,False,0.13224408,False,0.19019896,False,0.09299977,True,0.39969608,False,0.118473396
"pick, female. This cannot be expected to work  particularly well. Th ere is something about 
the relative  dimensions of the skulls that ought be more informative. 
One possibility is to allow splits on linear combinations of variables. This is allowed 
in some classification tree software implementations. An alternative idea is to apply the method to the principal component scores rather than the raw data. Principal components 
(PC) seek out the main directions of variation in the data and might generate more 
effective predictors for cla ssification in this example: 
> pck <- princomp(newko[,-1]) 
> pcdf <- data.frame(species=newko$species,pck$scores) 
> kt <- rpart (species ~ ., pcdf, cp=0.001) 
> printcp(kt) 
Root node error: 95/144 =0.66 
n= 144 
      CP nsplit rel error xerror   xstd 
1 0.4000      0     1.000  1.126 0.0552 
2 0.1789      1     0.600  0.621 0.0621 
3 0.0421      2     0.421  0.558 0.0609 
4 0.0105      3     0.379  0.568 0.0612 
5 0.0010      5     0.358  0.589 0.0616 
We find a significantly smaller relative CV error (0.558). Before we can predict the test 
case, we need to do some work to remove the missing values, unused variables and apply 
the principal component transformation: 
> nx0 <- x0[! is.na(x0)] 
> nx0 <- nx0 [-c(3, 9)] 
> nx0 <- (nx0-pck$center)/pck$scale 
> nx0 %*% pck$loadings 
       Comp.1  Comp.2  Comp.3  Comp.4 Comp.5 
Comp.6  Comp.7 
  [1,] 499.93 -74.834 -37.632 23.169 3.9564 16.584 -
54.017 
        Comp.8  Comp.9 
  [1,] -35.995 -16.705 
Our chosen tree is: 
> ktp <- prune.rpart(kt,0.0421) 
> ktp 
n= 144 
node), split, n, loss, Yval, (Yprob) 
      * denotes terminal node 
1) root 144 95 fuliginosus (0.340278 0.333333 0.326389) 
   2) Comp.2< -15.126 49  8 fuliginosus (0.836735 
0.040816 0.122449) * 
   3) Comp.2>=-15.126 95 49 giganteus (0.084211 
0.484211 0.431579) 
     6) Comp.4>=-9.513 63 24 giganteus (0.111111 
0.619048 0.269841) Extending the linear model with R     292",3,False,0.116191074,False,0.115484044,False,0.06768465,True,0.3504319,False,0.074087664
"2. Refer to the pima dataset described in Question 3 of Chapter 2. First take care to deal 
with the clearly mistaken obse rvations for some variables. 
(a) Fit a tree model with the result of the diabetes test as the response and all the other 
variables as predictors.  
(b) Perform diagnostics on your model, reporting any potential violations. 
(c) Predict the outcome for a woman with predictor values 1, 99, 64, 22, 76, 27, 0.25, 
25 (same order as in dataset). How certain is this prediction? 
(d) If you completed the logistic regression analysis of the data earlier, compare the 
two analyses. 
3. The dataset wbcd is described in Question 2 of Chapter 2. 
(a) Fit a tree model with Class as the response and the other nine variables as 
predictors. 
(b) Use the model to predict the outcome for a new patient with predictor variables 1, 
1, 3, 2, 1, 1,4, 1, 1 (same order as above). 
(c) Suppose that a cancer is classified as benign if p>0.5 and malignant if p<0.5. 
Compute the number of errors of both type s that will be made if this method is 
applied to the current data with the reduced model.  
(d) Suppose we change the cutoff to 0.9 so that p<0.9 is classified as malignant and 
p>0.9 as benign. Compute the number of erro rs in this case. Discuss the issues in 
determining the cutoff. 
(e) It is usually misleading to use the same data to fit a model and test its predictive 
ability. To investigate this, split the data into two parts and assign every third 
observation to a test set and the remaining two thirds of the data to a training set. 
Use the training set to determine the model and the test set to assess its predictive 
performance. Compare the outcome to the previously obtained results. 
(f) If you completed the logistic regression  analysis of the data earlier, compare the 
two analyses. 
4. The dataset uswages is drawn as a sample from the Current Population Survey in 1988. 
(a) Build a tree regression model to predict wage. 
(b) Check the diagnostics of your model. 
(c) Use your model to predict the wage of a subject with predictor characteristics 12, 
33, 0, 1, 0, 0, 0, 1, 0 where the values occur in the same order as in the data frame. 
(d) Conduct a quick linear model analysis and compare the results with the tree model. 
In particular, what do the two models say about the relationship between the 
predictors and the response? 
5. The dvisits data comes from the Australian Health Survey of 1977–78 and con-sist of 
5190 single adults where young and old have been oversampled. 
(a) Build a Poisson tree model with doctorco as the response and sex, age, agesq, 
income, levyplus, freepoor, freerepa, illness, actdays, hscore, chcond1 and chcond2 
as possible predictor variables. Consult the rpart documentation for how to specify 
a Poisson response. 
(b) Check the diagnostics. Extending the linear model with R     294",2,False,0.23530635,False,0.21062154,False,0.2903111,True,0.42781365,True,0.38629985
"(c) What sort of person would be predicted to visit the doctor the most under your 
selected model? 
(d) For the last person in the dataset, compute the predicted probability distribution for 
their visits to the doctor, i.e., give the probability they visit 0, 1, 2 etc. times. 
(e) If you have previously completed the analysis of this data in the exercises for 
Chapter 3, compare the results. Trees     295",2,False,0.20509191,False,0.15501037,True,0.30621785,False,0.23466757,False,0.2083103
"The activation functions for the hidden layer, 
 are almost always logistic. If identity 
functions are used for the hidden layer and for the output, the resulting NN is quite 
similar to the partial least squares approach of Wold, Ruhe, Wold, and Dunn (1984). We 
will set one of our inputs to be constant at one so as to allow for an intercept/bias term. 
The choice of output activation function depends on the nature of  
 
Figure 14.2 NN equivalents of 
multivariate linear regression (shown 
on the top) and polynomial regression 
(shown on the bottom).  
the response. For continuous unrestricted output, an identity function is appropriate while 
for response bounded between zero and one, su ch as a binomial proportion, a logistic 
function should be used. We show the feed-forward NN in Figure 14.3.  Extending the linear model with R     298",0,False,0.19866034,False,0.27189028,False,0.17593497,True,0.33032,False,0.16393586
"The value of the best RSS is somewhat larger  than before. We expect this since weight 
decay sacrifices some fit to the current data to obtain a more stable result. We repeat the 
assessment of the marginal effects as before  and display the results in Figure 14.5: 
> xx <- expand.grid(temp=seq(-3,3,0.1), ibh=0, ibt=0) 
> plot(xx$temp*ozscales[ 'temp']+ozmeans['temp'], 
  predict(bestnn, new=xx)*ozscales['03']+ozmeans['03'], 
xlab=""Temp”, ylab=""03"") 
> xx <- expand.grid(temp=0,ibh=seq(-3,3,0.1), ibt=0) 
> plot(xx$ibh*ozscales['ibh']+ozmeans['ibh'], 
  predict(bestnn, new=xx)*ozscales['03']+ozmeans['03'], 
xlab=""IBH"", ylab=""03"") 
> xx <- expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1)) 
> plot(xx$ibt*ozscales['ibt']+ozmeans['ibt'], 
  predict(bestnn, new=xx)*ozscales['03']+ozmeans[ 
‘03'], xlab=""IBT"",ylab=""03"") 
 
Figure 14.5 Marginal effects of 
predictors for the NN fit with weight 
decay. Other predictors are held fixed 
at their mean values.  
We see that the fits are now plausibly smooth. Note that ibh is strictly positive in practice 
so the strange behavior for negative values is irrelevant. Compare these plots to Figure 
12.2. The shapes are similar fo r temperature and ibh. The ibt plot looks quite different 
although we have no way to assess the significance of any of the terms in the NN fit. 
NNs have interactions built in so one should also look at these. We could produce 
analogous plots to those in Figure 14.5 by varying two predictors at a time. 
Now let’s look at the full dataset. We use four hidden units because there are now 
more inputs. 
> bestrss <- 10000 
> for(i in 1:100){ 
   nnmdl <- nnet (03 ~ ., sx, size=4, linout=T,trace=F) Neural networks     303",0,False,0.13732119,True,0.32846966,False,0.16696101,False,0.1984728,False,0.1421648
"cat (nnmdl$value, ""\n”) 
   if(nnmdl$value < bestrss) 
    bestnn <- nnmdl 
    bestrss <- nnmdl$value 
}} 
> 1-bestnn$value/sum((sx[,1]-mean(sx[,1]))^2) 
[1] 0.85063 
The fit is good and there may be better minimum than we have found and increasing the 
number of hidden units would always improve the fit. The fit can be compared to those in 
previous chapters. The R2 for the linear and tree model fits  was substantially smaller, but 
these approaches place a premium on simplic ity and interpretability. The fit for the 
corresponding additive model was better, but not quite as good as the NN. But the 
additive model also has the interpretability that the NN lacks. Finally, the MARS model 
fit better and was also interpretable. 
Of course, it would be rash to draw firm conclusions from just one dataset. 
Furthermore, the value of the modeling approaches needs to be judged within the context 
of the particular problem. If explanation is the main goal of the data analysis, NNs are not 
a good choice. If prediction is the objective, we cannot judge just by the fit to the data we 
have now. It is more important how the m odel performs on future observations. We do 
not have fresh data here as we have used it a ll to fit the data. Some studies have withheld 
data for use in testing the prediction performance of the models considered. NNs have 
been generally competitive in these studies but by no means dominant. 
14.4 Conclusion  
NNs, as presented here, are a controlled flexib le class of nonlinear regression models. By 
adding more hidden units we can control the complexity of the model in a measured way from relatively simple models up to models suitable for large datasets with complex 
structure. NNs are also attrac tive because they require less expertise to use successfully 
compared to statistical models. Nevertheless the user must still pay attention to basic statistical issues involving transformation and scaling of the data and outliers and 
influential points. See Faraway and Chatfield (1998) for an example of the application of 
neural networks and how they compare with statistical methods. 
NNs are generally good for prediction but bad for understanding. The NN weights are 
almost uninterpretable. Although one can gain some insight from plotting the marginal 
effect of predictors, the NN inevitably intro duces complex interactions that often do not 
reflect reality. Furthermore, without careful  control, the NN can easily overfit the data 
resulting in overoptimistic predictions. 
NNs are quite effective for large complex datasets compared to statistical methods 
where the burden of developing an appropriate sampling model can sometimes slow or 
even block progress. NNs do lack good statis tical theory for inference, diagnostics and 
model selection. Of course, they were not developed with these statistical considerations 
in mind, but experience shows that such issues are often important. Extending the linear model with R     304",3,False,0.2987867,False,0.19900021,False,0.18027027,True,0.3913237,False,0.11529638
"APPENDIX A  
Likelihood Theory  
This appendix is just an overview of the likelihood theory used in this book. For greater 
detail or a more gentle introduction, the read er is advised to consult a book on theoretical 
statistics such as Cox and Hinkley (1974), Bickel and Doksum (1977) or Rice (1998). 
A.1 Maximum Likelihood  
Consider n independent discrete random variables, Y1,…, Y n, with probability distribution 
function f(y|θ) where θ is the, possibly vector-valued, parameter. Suppose we observe 
y=(y 1,…, y n)T, then we define the likelihood as: 
   
So the likelihood is a function of the parameter(s) given the data and is the probability of the observed data given a specified value of the parameter(s). 
For continuous random variables, Y
1,…, Y n with probability density function f(y|θ), we 
recognize that, in prac tice, we can only measure or observe data with limited precision. 
We may record yi, but this effectively indicates an observation in the range 
 so that: 
   
where 
 We can now write the likelihood as: 
   
Now provided that δi is relatively small and does not depend on θ, we may ignore it and 
the likelihood is the same as in the discrete case. 
As an example, suppose that Y is binomially distributed B(n, p) . The likelihood is: 
   
The maximum likelihood  estimate (MLE) is the value of the parameter(s) that gives the 
largest probability to the observed data, or in other words, maximizes the likelihood 
function. The value at which the maximum occurs, 
 is the maximum likelihood",0,True,0.31047654,False,0.16804737,False,0.12979762,False,0.13761932,False,0.1833135
"> nloglik <- function (p,y,n) loglik(p,y,n) - 
loglik(y/n,y,n) 
Now plot the two log-likelihoods, as seen in Figure A.1: 
> pr <- seq(0.05,0.95,by=0.01) 
> 
matplot(pr,cbind(nloglik(pr,10,25),nloglik(pr,20,50)),t
ype=""1"", 
  xlab=""p"",ylab="" log-likelihood”) 
We see that the maximum occurs at p=0.4 in each case at a value of zero because of the 
normalization. For the larger sample, we  see greater curvature and hence more 
information.  
 
Figure A. 1 Normalized binomial log-
likelihood for n =25, y=10 shown with a 
solid line and n =50, y=20 shown with a 
dotted line.  Appendix A     309",0,True,0.31252807,False,0.2135816,False,0.09962162,False,0.11145046,False,0.29840162
"Examples where likelihood can be maximized explicitly are confined to simple cases. 
Typically, numerical optimization is necessary. The Newton-Raphson  method is the most 
well-known technique. Let θ0 be an initial guess at θ, then we update using: 
θ1=θ0−H−1(θ0)u(θ0)   
where H is the Hessian matrix of second derivatives: 
   
We iterate this method, putting θ1 in place of θ0 and so on, until the procedure (hopefully) 
converges. This method works well provided the log-likelihood is smooth and convex around the maximum and that the initial value is reasonably close. In less well-behaved 
cases, several things can go wrong: 
• The likelihood has multiple maxima. The maximum that Newton-Raphson finds will 
depend on the choice of initial estimate. If you are aware that multiple maxima may 
exist, it is advisable to try multiple starting values to search for the overall maximum. 
The number and choice of these starting va lues is problematic. Such problems are 
common in fitting neural networks, but rare for generalized linear models. 
• The maximum likelihood may occur at the boundary of the parameter space. This 
means that perhaps 
 which will confuse the Newton-Raphson method. 
Mixed effect models have several variance parameters. In some cases, these are 
maximized at zero, which causes difficulties in the numerical optimization. 
• The likelihood has a large number of parameters and is quite flat in the neighborhood of 
the maximum. The Newton-Raphson method may take a long time to converge. 
The Fisher scoring method replaces H with −I and sometimes gives superior re-sults. 
This method is used in fitting GLMs and is  equivalent to iteratively reweighted least 
squares. 
A minimization function that uses a Newton-type method is available in R. We 
demonstrate its use for likelihood maximization. Note that we need to minimize  −l 
because nlm minimizes, not maximizes: 
> f <- function (x) -loglik(x,10,25) 
> mm <- nlm (f, 0.5, hessian=T) 
We use a starting value of 0.5 and find the optimum at: 
> mm$estimate 
[1] 0.4 
The inverse of the Hessian at the optimum is equal to the standard estimate of the 
variance: 
> c(1/mm$hessian, 0.4*(1-0.4)/25) 
[1] 0.0096016 0.0096000 Appendix A     310",0,True,0.3449648,False,0.09746615,False,0.013241376,False,0.108794525,False,0.11733353
"Bibliography  
Agresti, A. (1984). Analysis of Ordinal Categorical Data.  New York: Wiley. 
Agresti, A. (2002). Categorical Data Analysis  (2 ed.). New York: John Wiley. 
Allison, T. and D.Cicchetti (1976). Sleep in mammals: Ecological and constitutional correlates. 
Science  194, 732–734. 
Andrews, D. and A.Herzberg (1985). Data: A Collection of Problems from Many Fields for the 
Student and Research Worker.  New York: Springer-Verlag. 
Appleton, D., J.French, and M.Vanderpump ( 1996). Ignoring a covariate: An example of 
Simpson’s paradox. American Statistician  50, 340–341. 
Bates, D. (2005, May). Fittin g linear mixed models in R. R News  5(1), 27–30. 
Becker, R., J.Chambers, and A.Wilks (1998). The New S Language: A Programming Environment 
for Data Analysis and Graphics  (revised ed.). Boca Raton, FL: CRC Press. 
Bellman, R. (1961). Adaptive Control Processes: A Guided Tour.  Princeton, NJ: Princeton 
University Press. 
Bergman, B. and A.Hynen (1997). Dispersion effects from unreplicated designs in the 2k−P series. 
Technometrics  39, 191–198. 
Bickel, P. and K.Doksum (1977). Mathematical Statis tics: Basic Ideas and Selected Topics.  San 
Francisco: Holden Day. 
Bishop, C. (1995). Neural Networks for Pattern Recognition.  Oxford: Clarendon Press. 
Bishop, Y., S.Fienberg, and P.Holland (1975). Discrete Multivariate Analysis: Theory and 
Practice.  Cambridge, MA: MIT Press. 
Blasius, J. and M.Greenacre (1998). Visualization of Categorical Data.  San Diego: Academic 
Press. 
Bliss, C.I. (1935). The calculation of the dose-mortality curve. Annals of Applied Biology  22, 134–
167. 
Bliss, C.I. (1967). Statistics in Biology.  New York: McGraw Hill. 
Bowman, A. and A.Azzalini (1997). Applied Smoothing Techniques fo r Data Analysis: The Kernel 
Approach with S-Plus Illustrations.  Oxford: Oxford University Press. 
Box, G. and R.Meyer (1986). Dispersion effects from fractional designs. Technometrics  28, 19–27. 
Box, G.P., S.Bisgaard, and C.Fung (1988). An expl anation and critique of Taguchi’s contributions 
to quality engineering. Quality and Reli ability Engineerin g International  4, 123–131. 
Box, G.P., W.G.Hunter, and J.S.Hunter (1978). Statistics for Experimenters.  New York: Wiley. 
Breiman, L. (2001a). Random forests. Machine Learning  45, 5–32. 
Breiman, L. (2001b). Sta tistical modeling: The two cultures (w ith comments and a rejoinder by the 
author). Statistical Science  16, 199–231. 
Breiman, L., J.Friedman, R.Olshen, and C.Stone (1984). Classification and Regression Trees.  Boca 
Raton, FL: Chapman & Hall. 
Breiman, L. and J.H.Friedman (1985). Estimating optimal transformations for multiple regression 
and correlation. Journal of the American Statistical Association  80, 580–598. 
Breslow, N. (1982). Covariance adjustment of  relative-risk estimates in matched studies. 
Biometrics  38, 661–672. 
Breslow, N.E. and D.G.Clayton (1993). Approximate inference in generalized linear mixed models. 
Journal of the American Statistical Association  88, 9–25. 
Cameron, A. and P.Trivedi (1998). Regression Analysis of Count Data.  Cambridge: Cambridge 
University Press.",2,False,0.061622247,False,0.13621019,True,0.3091712,False,0.16603437,False,0.09925704
"Sheldon, F. (1960). Statis tical techniques applied to production situations. Indus-trial and 
Engineering Chemistry  52, 507–509. 
Simonoff, J. (1996). Smoothing Methods in Statistics.  New York: Springer. 
Simonoff, J. (2003). Analyzing Categorical Data.  New York: Springer. 
Simpson, E. (1951). The interpretation of interaction in contingency tables. Jour-nal of the Royal 
Statistical Society, Series B  (13), 238–241. 
Sinha, S. (2004). Robust analysis of  generalized linear mixed models. JASA  99, 451–460. 
Smyth, G., F.Huele, and A.Verbyla (2001). Ex act and approximate re ml for heteroscedastic 
regression. Statistical Modelling: An International Journal  1, 161–175. 
Snedecor, G. and W.Cochran (1989). Statistical Methods  (8 ed.). Ames, IA: Iowa State University 
Press. 
Snee, R. (1974). Graphical displa y of two-way contingency tables. American Statistician  28, 9–12. 
Steele, R. (1998). Effect of surface and vision on balance.  Ph. D. thesis, Depart-ment of 
Physiotherapy, University of Queensland. 
Stone, C. (1985). Additive regressi on and other nonparametric models. Annals of Statistics  13, 
689–705. 
Stram, D. and J.Lee (1994) . Variance components te sting in the longitudina l mixed-effects model. 
Biometrics  50, 1171–1179. 
Stuart, A. (1955). A test for homoge neity of the marginal distributi ons in a twoway classification. 
Biometrika  42, 412–416.  
Thall, P.F. and S.C.Vail (1990) . Some covariance models for longitudinal count data with 
overdispersion. Biometrics  46, 657–671. 
Tukey, J. (1977). Exploratory Data Analysis.  New York: Addison Wesley. 
Venables, W. and B.Ripley (2002). Modern Applied Statistics with S  (4 ed.). New York: Springer. 
Verbeke, G. and G.Molenberghs (2000). Linear Mixed Models for Longitudinal Data.  New York: 
Springer. 
Verzani, J. (2004). Using R for Introductory Statistics.  Boca Raton, FL: Chapman & Hall/CRC. 
Wahba, G. (1990). Spline Models for Observational Data.  Philadelphia: SIAM. 
Wand, M. and M.Jones (1995). Kernel Smoothing.  London: Chapman & Hall. 
Wedderburn, R.W.M. (1974). Quasilikelihood functi ons, generalized linear mod-els and the Gauss-
Newton method. Biometrika  61, 439–441.  
Weisberg, S. (2005). Applied Linear Regression  (3 ed.). New York: Wiley. 
Whitmore, G. (1986). Inverse Gaussian ratio estimation. Applied Statistics  35, 8–15. 
Wilkinson, G. and C.Rogers (1973) . Symbolic description of factorial models for the analysis of 
variance. Applied Statistics  22, 392–399. 
Williams, D. (1982). Extra-binomial variation in logistic linear models. Applied Statistics  31, 144–
148. 
Williams, D. (1987). Generalized linear model diagnostics using the deviance and single case 
deletions. Applied Statistics  36, 181–191. 
Wold, S., A.Ruhe, H.Wold, and W.Dunn (1984). Th e collinearity problem in lin-ear regression: 
The partial least squares (pls) approach to generalized inverses. SIAM Journal on Scientific and 
Statistical Computing  5, 735–743. 
Wood, S. (2000). Modelling and sm oothing parameter estimation with multiple quadratic penalties. 
Journal of the Royal Sta tistal Society, Series B  62, 413–428. 
Wood, S. (2006). An Introduction to Generalized  Additive Models with R.  Boca Raton, FL: CRC 
Press. 
Yule, G. (1903). Notes on the theory of a ssociation of attribut es in statistics. Biometrika  2, 121–
134. Bibliography     322",2,False,0.1967052,True,0.3034554,True,0.34712246,True,0.30742925,False,0.18317509
"generalized cross-validation, 215 
generalized estimating equations, 204 
generalized least squares, 155, 159 
generalized linear mixed models, 201 
generalized linear model, 115 
Gini index, 261 
GLM, see generalized linear model 
GLMM, see generalized linear mixed models 
goodness of fit, 29, 121 
greedy strategy, 257 
grouped data, 153  
G-statistic, 58 
 
half-normal plot, 59, 129 
hat matrix, 16, 124 
Hauck-Donner effect, 30 
hazard, 111 
Helmert contrasts, 194 
Hessian matrix, 281 
hierarchical response, 103 
homogeneity, 74 
hypergeometric distribution, 70, 74 
hypothesis tests, 12, 120 
 
independence, 72 indicator variable, see dummy variable 
inertia, 77 
influence, 14, 124 
interaction, 10 
intercept, 6 
interval scale, 69 
intraclass correlation coefficient, 154 
inverse Gaussian distribution, 116 
inverse Gaussian GLM, 142 iteratively reweighted least squares, 117 
 
jacknife residuals, 124 joint independence, 85 
 
kernel estimator, 213 knots, 218 
 
latent variable, 107 latin square, 172 
LD50, 42 
lethal dose, 42 
leverage, 16, 124 
likelihood, 279 
likelihood ratio statistic, 29, 120, 282 
linear model, 1 Index     327",4,False,0.20075744,True,0.30480814,True,0.3118649,False,0.22270483,False,0.2136464
"object-oriented language, 8 
observed information, 285 
odds, 31 
odds ratio, 32, 74, 83 
offset, 63, 139, 207 
one-way ANOVA, 154 
ordinal multinomial response, 106 
ordinal variable, 69, 88 
orthogonal polynomials, 19, 222 
outliers, 4, 5, 129 
overdispersion, 43, 60 
 
packages, 287 
MASS, 18, 31, 34, 43, 65, 108, 139, 203 
SuppDists, 142 
acepack, 242 
brlr, 40, 203 
faraway, 1, 25 
gam, 233 
gee, 204 
lattice, 186 
lme4, 157 
mda, 248 
mgcv, 235 
nnet, 99, 272 
rpart, 255 
sm, 216 
splines, 20, 220 
survival, 51 
wavethresh, 223 
panel data, 185 
parameters, 6 
parametric bootstrap, 159 
partial residual plot, 16, 128 
partial residuals, 232 Pearson residuals, 40, 76, 123 
Pearson’s X
2, 40, 58, 120 
penalized quasi-likelihood, 203 
penalized smoothing splines, 232 
perceptron, 269 
permutation, 30, 132 
piecewise linear, 219 
Poisson deviance, 58 
Poisson distribution, 55, 115 
Poisson regression, 55 
predicted values, 7 
prediction, 41 
principal components, 265 
prior density, 161 
probit, 27, 106 
product multinomial, 74 Index     329",0,False,0.14025897,True,0.32365623,False,0.2895683,False,0.27134988,False,0.25476563
"profile likelihood, 31, 283 
projection matrix, 234 
proportional hazards model, 111 
proportional odds model, 108 
prospective sampling, 34 
 
QQ plot, 14 quasi-binomial, 148 
quasi-deviance, 148 
quasi-independence, 81 
quasi-likelihood, 147, 204 
quasi-Poisson, 148 
quasi-symmetry, 80 
 
R, 287 random effects, 153 
randomization, 167 
randomized block design, 163 
rate model, 61 
recursive partitioning, 253 
regression splines, 218 
regression trees, 253 
relative risk, 32 
REML, see restricted maximum likelihood 
repeated measures, 185 
residual deviance, 29 
residual sum of squares, 8 
residuals, 7, 123 
response feature analysis, 189 
response residuals, 123 
restricted maximum likelihood, 156 
retrospective sampling, 34 
robust smoothing, 218, 221 
roughness penalty, 217 row effects, 91 
R
2, 8 
running median, 226 
 
saturated model, 29, 120 scaled deviance, 120 
score function, 280 
score test, 283 
scores, 89 
shrinkage, 162 
Simpson’s paradox, 82 
simulation, 160 
singular value decomposition, 76 
skip-layer, 271 
smoothing, 212 
smoothing parameter, 213 
smoothing spline, 217 Index     330",0,False,0.20841515,True,0.30089095,True,0.32450828,False,0.26443323,True,0.35517293
"splines, 217 
split-plot design, 167 
square-root transformation, 56 
studentized residuals, 124 
sufficient, 117 
sum contrasts, 157 
symmetry, 79 
 
TBS, see transform both sides 
thin plate splines, 228 
tolerance distribution, 31 
transform both sides, 241 
treatment coding, 10 
tree pruning, 257 
trees, 253 
t-statistic, 12 
t-test, 12 
twicing, 227 
 
unbalanced data, 155 underdispersion, 59 
uniform association, 86 
 
variable bandwidth, 226 variance function, 116 
 
Wald distribution, 142 Wald test, 122, 282 
wavelets, 222 
Weibull distribution, 116 
weight decay, 274 
weights, 116 
working residuals, 124 
 
Yates’ continuity correction, 73 Index     331",4,False,0.0669906,True,0.33412868,False,0.19860342,False,0.29787886,False,0.1839009
