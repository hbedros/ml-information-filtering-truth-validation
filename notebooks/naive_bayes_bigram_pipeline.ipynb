{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Bigram Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'text', 'subject', 'date'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(true_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.4603\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Government News       0.08      0.10      0.09       316\n",
      "    Middle-east       0.12      0.14      0.13       159\n",
      "           News       0.84      0.91      0.87      1821\n",
      "        US_News       0.08      0.08      0.08       160\n",
      "      left-news       0.13      0.12      0.12       897\n",
      "       politics       0.28      0.24      0.26      1344\n",
      "\n",
      "       accuracy                           0.46      4697\n",
      "      macro avg       0.26      0.27      0.26      4697\n",
      "   weighted avg       0.44      0.46      0.45      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Naive Bayes model and dataset\n",
    "from models.naive_bayes_bigram_model import NaiveBayesBigram\n",
    "from utils.utils import fake_df\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb_model = NaiveBayesBigram()\n",
    "\n",
    "# Train the model on the dataset\n",
    "# Assuming `text` is the column with text data and `label` is the column with labels\n",
    "results = nb_model.train(fake_df, text_column='text', label_column='subject')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Naive Bayes Accuracy: {results['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m nb_model \u001b[38;5;241m=\u001b[39m NaiveBayesBigram()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model on the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Assuming `text` is the column with text data and `label` is the column with labels\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mnb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaive Bayes Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/MSDS/Capstone/CODE/ml-models-information-filtering/src/models/naive_bayes_bigram_model.py:27\u001b[0m, in \u001b[0;36mNaiveBayesBigram.train\u001b[0;34m(self, df, text_column, label_column)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m     26\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[text_column])\n\u001b[0;32m---> 27\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Split into training and testing sets\u001b[39;00m\n\u001b[1;32m     30\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from models.naive_bayes_bigram_model import NaiveBayesBigram\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"/Users/haigbedros/Desktop/MSDS/Capstone/CODE/ml-models-information-filtering/notebooks/lda_bert_processed_marr.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb_model = NaiveBayesBigram()\n",
    "\n",
    "# Train the model on the dataset\n",
    "# Assuming `text` is the column with text data and `label` is the column with labels\n",
    "results = nb_model.train(df, text_column='text', label_column='label')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Naive Bayes Accuracy: {results['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Results\n",
    "Class Imbalance:\n",
    "\n",
    "The Classification Report shows that the News class has significantly higher support (1821 samples) compared to other classes like Middle-east (159 samples) or US_News (160 samples).\n",
    "This imbalance likely skews the model's predictions toward the majority class (News), as evidenced by its high precision, recall, and F1-score.\n",
    "Poor Performance on Minority Classes:\n",
    "\n",
    "Classes like Government News, Middle-east, and US_News have very low precision, recall, and F1-scores, indicating that the model struggles to correctly classify these categories.\n",
    "Weighted Average:\n",
    "\n",
    "The weighted average F1-score is 0.45, which reflects the model's overall performance across all classes, weighted by the number of samples in each class.\n",
    "Naive Bayes Limitations:\n",
    "\n",
    "Naive Bayes assumes that features (bigrams in this case) are conditionally independent, which may not hold true for text data.\n",
    "The model may not capture the contextual relationships between words as effectively as more advanced models like BERT.\n",
    "\n",
    "Future: we will use techniques like SMOTE (Synthetic Minority Oversampling Technique) or random oversampling to balance the dataset.\n",
    "\n",
    "\n",
    "**Then will compare with Advanced Models**\n",
    "\n",
    "LDA + BERT Hybrid:\n",
    "Compare the Naive Bayes results with your LDA + BERT pipeline to validate your hypothesis that the hybrid approach will outperform simpler models like Naive Bayes.\n",
    "\n",
    "**Lastly will use Dimensionality Reduction for Visualization**\n",
    "\n",
    "Visualize the bigram features using dimensionality reduction techniques like PCA or t-SNE to understand how well the classes are separated in the feature space.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (notebook)",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
