{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we count tokens\n",
    "\n",
    "import pandas as pd\n",
    "from tokenizer.token_counter import TokenCounter\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize the token counter\n",
    "counter = TokenCounter(\"gpt-3.5-turbo\")\n",
    "\n",
    "def count_tokens_from_file(file_path: str, text_column: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in a file. Supports CSV, TXT, and PDF files.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file.\n",
    "        text_column (str): Column name for text data (only for CSV files).\n",
    "    \n",
    "    Returns:\n",
    "        int: Total token count in the file.\n",
    "    \"\"\"\n",
    "    # Check file extension\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        # Load CSV and extract the specified text column\n",
    "        if text_column is None:\n",
    "            raise ValueError(\"For CSV files, you must specify the 'text_column' argument.\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in the CSV file.\")\n",
    "        texts = df[text_column].dropna().tolist()\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        # Read text file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [line.strip() for line in f if line.strip()]\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        # Read PDF file\n",
    "        reader = PdfReader(file_path)\n",
    "        texts = [page.extract_text() for page in reader.pages if page.extract_text()]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only CSV, TXT, and PDF files are supported.\")\n",
    "    \n",
    "    # Count tokens\n",
    "    total_tokens = counter.total_tokens(texts)\n",
    "    return total_tokens\n",
    "\n",
    "# Example Usage\n",
    "# file_path_csv = \"/path/to/your/file.csv\"\n",
    "# file_path_txt = \"/path/to/your/file.txt\"\n",
    "file_path_pdf = \"/Users/haigbedros/Desktop/MSDS/Capstone/CODE/ml-models-information-filtering/src/Extending the Linear Model with R.pdf\"\n",
    "\n",
    "# For CSV files, specify the text column\n",
    "# csv_token_count = count_tokens_from_file(file_path_csv, text_column=\"content\")  # Replace 'content' with your column name\n",
    "# print(f\"Total tokens in CSV file: {csv_token_count}\")\n",
    "\n",
    "# For TXT files\n",
    "# txt_token_count = count_tokens_from_file(file_path_txt)\n",
    "# print(f\"Total tokens in TXT file: {txt_token_count}\")\n",
    "\n",
    "# For PDF files\n",
    "pdf_token_count = count_tokens_from_file(file_path_pdf)\n",
    "print(f\"Total tokens in PDF file: {pdf_token_count}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
