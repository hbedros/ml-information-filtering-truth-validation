{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in PDF file: 173288\n"
     ]
    }
   ],
   "source": [
    "# here we count tokens\n",
    "\n",
    "import pandas as pd\n",
    "from tokenizer.token_counter import TokenCounter\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize the token counter\n",
    "counter = TokenCounter(\"gpt-3.5-turbo\")\n",
    "\n",
    "def count_tokens_from_file(file_path: str, text_column: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Count tokens in a file. Supports CSV, TXT, and PDF files.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file.\n",
    "        text_column (str): Column name for text data (only for CSV files).\n",
    "    \n",
    "    Returns:\n",
    "        int: Total token count in the file.\n",
    "    \"\"\"\n",
    "    # Check file extension\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        # Load CSV and extract the specified text column\n",
    "        if text_column is None:\n",
    "            raise ValueError(\"For CSV files, you must specify the 'text_column' argument.\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in the CSV file.\")\n",
    "        texts = df[text_column].dropna().tolist()\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        # Read text file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = [line.strip() for line in f if line.strip()]\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        # Read PDF file\n",
    "        reader = PdfReader(file_path)\n",
    "        texts = [page.extract_text() for page in reader.pages if page.extract_text()]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only CSV, TXT, and PDF files are supported.\")\n",
    "    \n",
    "    # Count tokens\n",
    "    total_tokens = counter.total_tokens(texts)\n",
    "    return total_tokens\n",
    "\n",
    "# Example Usage\n",
    "# file_path_csv = \"/path/to/your/file.csv\"\n",
    "# file_path_txt = \"/path/to/your/file.txt\"\n",
    "file_path_pdf = \"/Users/haigbedros/Desktop/MSDS/Capstone/CODE/ml-models-information-filtering/src/Extending the Linear Model with R.pdf\"\n",
    "\n",
    "# For CSV files, specify the text column\n",
    "# csv_token_count = count_tokens_from_file(file_path_csv, text_column=\"content\")  # Replace 'content' with your column name\n",
    "# print(f\"Total tokens in CSV file: {csv_token_count}\")\n",
    "\n",
    "# For TXT files\n",
    "# txt_token_count = count_tokens_from_file(file_path_txt)\n",
    "# print(f\"Total tokens in TXT file: {txt_token_count}\")\n",
    "\n",
    "# For PDF files\n",
    "pdf_token_count = count_tokens_from_file(file_path_pdf)\n",
    "print(f\"Total tokens in PDF file: {pdf_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens in the 'text' column: 284033\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"/Users/haigbedros/Desktop/MSDS/Capstone/CODE/ml-models-information-filtering/notebooks/lda_bert_processed_fake.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Load Hugging Face's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your tokenizer\n",
    "\n",
    "# Count tokens in the 'text' column\n",
    "def count_tokens(text):\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return 0\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "df['token_count'] = df['text'].apply(count_tokens)\n",
    "\n",
    "# Combine token counts (sum them up)\n",
    "total_tokens = df['token_count'].sum()\n",
    "\n",
    "# Print the total token count\n",
    "print(f\"Total Tokens in the 'text' column: {total_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torchmetal)",
   "language": "python",
   "name": "torchmetal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
